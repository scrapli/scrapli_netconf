{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"scrapli_netconf \u00b6 scrapli_netconf is a netconf driver built on top of scrapli . The purpose of scrapli_netconf is to provide a fast, flexible, thoroughly tested, well typed, well documented, simple API that supports both synchronous and asynchronous usage. Working together scrapli and scrapli_netconf aim to provide a consistent (as is practical) look and feel when automating devices over telnet, SSH, or netconf (over SSH). scrapli_netconf aims to be fully RFC compliant at some point, but at the moment does not implement all netconf features/methods.","title":"Scrapli Netconf"},{"location":"#scrapli_netconf","text":"scrapli_netconf is a netconf driver built on top of scrapli . The purpose of scrapli_netconf is to provide a fast, flexible, thoroughly tested, well typed, well documented, simple API that supports both synchronous and asynchronous usage. Working together scrapli and scrapli_netconf aim to provide a consistent (as is practical) look and feel when automating devices over telnet, SSH, or netconf (over SSH). scrapli_netconf aims to be fully RFC compliant at some point, but at the moment does not implement all netconf features/methods.","title":"scrapli_netconf"},{"location":"changelog/","text":"CHANGELOG \u00b6 2022.01.30 \u00b6 Removed deprecated filters argument Removed deprecated NetconfScrape and AsyncNetconfScrape Improved raise_for_status exception messages, see #92 and #90 Dropped Python3.6 support as it is now EOL! Of course, scrapli probably still works just fine with 3.6 (if you install the old 3.6 requirements), but we won't test/support it anymore. 2021.07.30 \u00b6 Force system transport ssh connections to allocate a tty (-tt); fixes issue that would prevent system transport from sending any command > 1024 chars. Added use_compressed_parser argument to the driver constructor -- defaults to True which means we \"squish\" all the whitespace out of any input we get from the user before sending it to the netconf server, generally this is no problem, but some devices (looking at you NX-OS!) lock up and stop reading at some character counts (4096 in NX-OS it seems) causing the connection to timeout and die. By not \"squishing\" whitespace out this does not happen. Fixed some typing issues and pinned to scrapli pre-release to take advantage of updated typing/packaging setup Deprecate filters argument on get_config -- will be supported (by decorator) until 2022.01.30 (and pre-releases). This was done to make the arguments consistent for get , get_config , and rpc . Better handling of multiple filter elements in a filter string Smarter message building -- previously most of the final bytes payload that we send to the servers got built in the base driver class, and then some more (1.1 encoding) got added in the channel base class -- silly! Fixed this, so it is all done in the driver which eliminated a bunch of duplication (yay!). Deprecating comms_ansi -- see also scrapli changelog for this release (2021.07.30) for more details. This was never used here in scrapli_netconf so should be a non issue, but will not be fully deprecated until 2022.01.30. Re-fix #10 ... see #68 -- now there is a test with a comment so I don't break this again :) Added copy_config method, thanks to Roman Dodin for adding this in scrapligo first! Added handling/warning about use_compressed_parser if we catch a timeout exception when looking for prompt after writing inputs -- since I don't know (can't know?) which platforms may require this flag set to False this seems like a reasonable way to let users know and point them in the right direction to get things working! Reswizzled the echo check to be like the scrapligo version -- much simpler/less moving parts, so should be good! 2021.01.30 \u00b6 Big overhaul in line with the scrapli core overhaul... mostly this was about reconciliation of the channel and transport things and putting stuff where it should have been in the first place... see the changelog at scrapli core for much more details FUTURE BREAKING CHANGE -- NetconfScrape and AsyncNetconfScrape have been renamed to NetconfDriver and AsyncNetconfDriver -- there are alias classes so you can continue to use NetconfScrape and AsyncNetconfScrape but there is a warning, and these will be removed at some point in the future! 2021.01.17 \u00b6 Support for future \"vrouter\" setup for testing Flatten all channel inputs (no pretty printed xml) -- seems to behave much more nicely across the board! Updated test to match some recent scrapli core updates (multipl easync transports) 2020.11.15 \u00b6 Support namespaces in hello messages -- primarily to support \"rfc-compliant\" mode in JunOS -- thank you Gary Napier for finding this and coming up with the fix! Another fixup to chunk checker -- think that the itty bitty chunk issues have now been solved :) 2020.10.24 \u00b6 Improve the \"echo\" checker -- and add this for sync as well, because... SSH2 and Paramiko are now supported transports! As part of the \"improved echo checker\" sync channel now also overrides the read_until_input method like the async channel does -- again, for the same reasons. All transports minus system are now optional extras -- this means that asyncssh is no longer an install requirement As expected with above point -- added optional extras install options in setup.py as well as a \"full\" option just like scrapli core MAYBE BREAKING CHANGE: shouldn't be an issue for 99.9999% of people, however, the asyncssh transport is no longer imported and available in the transport package Add error_messages attribute to response object -- initialized as an empty list and the text of any rpc-error/error -message fields are placed into this list if there are any in the response from the server Improve netconf 1.1 chunk matching regex to not ignore/chop off Nokia error messages that contained # symbols 2020.10.10 \u00b6 Handle netconf 1.1 devices that have chunk sizes of 1 Ensure results are \"pretty printed\" Above two items were worked out with thanks to Hugo Tinoco! PS - this has been tested on Nokia devices now too! Hopefully improved asyncssh \"echo checker\" (see _check_echo) method in async_channel for details Update CI to use 3.9 instead of 3.9-dev (and update deprecated set-env) Remove transport session locks 2020.09.23 \u00b6 Strip server capabilities so we don't save capabilities with newlines/whitespace Add validate and delete_config methods 2020.09.18 \u00b6 Fix some pins for dev requirements Add 3.9-dev to actions Fix scrapli-asycnssh not in setup.py install_requires Retest everything! In general, just get this updated/ready for nornir-scrapli ! 2020.07.26 \u00b6 Update to match scrapli core -- moved to updated timeout decorator, fixed a test to match a better exception message 2020.07.12 \u00b6 Minor improvements to response recording (should be a tick faster) Update decorators for async things to use the improved async_operation_timeout in scrapli 2020.07.12 Set strip_namespaces to False for AsyncNetconfScrape for consistency/sanity Update a few dev pins, update required pins to ensure no major lxml updates break things 2020.07.04 \u00b6 First real release??? :) 2020.04.19 \u00b6 Initial pypi release... very beta still","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"CHANGELOG"},{"location":"changelog/#20220130","text":"Removed deprecated filters argument Removed deprecated NetconfScrape and AsyncNetconfScrape Improved raise_for_status exception messages, see #92 and #90 Dropped Python3.6 support as it is now EOL! Of course, scrapli probably still works just fine with 3.6 (if you install the old 3.6 requirements), but we won't test/support it anymore.","title":"2022.01.30"},{"location":"changelog/#20210730","text":"Force system transport ssh connections to allocate a tty (-tt); fixes issue that would prevent system transport from sending any command > 1024 chars. Added use_compressed_parser argument to the driver constructor -- defaults to True which means we \"squish\" all the whitespace out of any input we get from the user before sending it to the netconf server, generally this is no problem, but some devices (looking at you NX-OS!) lock up and stop reading at some character counts (4096 in NX-OS it seems) causing the connection to timeout and die. By not \"squishing\" whitespace out this does not happen. Fixed some typing issues and pinned to scrapli pre-release to take advantage of updated typing/packaging setup Deprecate filters argument on get_config -- will be supported (by decorator) until 2022.01.30 (and pre-releases). This was done to make the arguments consistent for get , get_config , and rpc . Better handling of multiple filter elements in a filter string Smarter message building -- previously most of the final bytes payload that we send to the servers got built in the base driver class, and then some more (1.1 encoding) got added in the channel base class -- silly! Fixed this, so it is all done in the driver which eliminated a bunch of duplication (yay!). Deprecating comms_ansi -- see also scrapli changelog for this release (2021.07.30) for more details. This was never used here in scrapli_netconf so should be a non issue, but will not be fully deprecated until 2022.01.30. Re-fix #10 ... see #68 -- now there is a test with a comment so I don't break this again :) Added copy_config method, thanks to Roman Dodin for adding this in scrapligo first! Added handling/warning about use_compressed_parser if we catch a timeout exception when looking for prompt after writing inputs -- since I don't know (can't know?) which platforms may require this flag set to False this seems like a reasonable way to let users know and point them in the right direction to get things working! Reswizzled the echo check to be like the scrapligo version -- much simpler/less moving parts, so should be good!","title":"2021.07.30"},{"location":"changelog/#20210130","text":"Big overhaul in line with the scrapli core overhaul... mostly this was about reconciliation of the channel and transport things and putting stuff where it should have been in the first place... see the changelog at scrapli core for much more details FUTURE BREAKING CHANGE -- NetconfScrape and AsyncNetconfScrape have been renamed to NetconfDriver and AsyncNetconfDriver -- there are alias classes so you can continue to use NetconfScrape and AsyncNetconfScrape but there is a warning, and these will be removed at some point in the future!","title":"2021.01.30"},{"location":"changelog/#20210117","text":"Support for future \"vrouter\" setup for testing Flatten all channel inputs (no pretty printed xml) -- seems to behave much more nicely across the board! Updated test to match some recent scrapli core updates (multipl easync transports)","title":"2021.01.17"},{"location":"changelog/#20201115","text":"Support namespaces in hello messages -- primarily to support \"rfc-compliant\" mode in JunOS -- thank you Gary Napier for finding this and coming up with the fix! Another fixup to chunk checker -- think that the itty bitty chunk issues have now been solved :)","title":"2020.11.15"},{"location":"changelog/#20201024","text":"Improve the \"echo\" checker -- and add this for sync as well, because... SSH2 and Paramiko are now supported transports! As part of the \"improved echo checker\" sync channel now also overrides the read_until_input method like the async channel does -- again, for the same reasons. All transports minus system are now optional extras -- this means that asyncssh is no longer an install requirement As expected with above point -- added optional extras install options in setup.py as well as a \"full\" option just like scrapli core MAYBE BREAKING CHANGE: shouldn't be an issue for 99.9999% of people, however, the asyncssh transport is no longer imported and available in the transport package Add error_messages attribute to response object -- initialized as an empty list and the text of any rpc-error/error -message fields are placed into this list if there are any in the response from the server Improve netconf 1.1 chunk matching regex to not ignore/chop off Nokia error messages that contained # symbols","title":"2020.10.24"},{"location":"changelog/#20201010","text":"Handle netconf 1.1 devices that have chunk sizes of 1 Ensure results are \"pretty printed\" Above two items were worked out with thanks to Hugo Tinoco! PS - this has been tested on Nokia devices now too! Hopefully improved asyncssh \"echo checker\" (see _check_echo) method in async_channel for details Update CI to use 3.9 instead of 3.9-dev (and update deprecated set-env) Remove transport session locks","title":"2020.10.10"},{"location":"changelog/#20200923","text":"Strip server capabilities so we don't save capabilities with newlines/whitespace Add validate and delete_config methods","title":"2020.09.23"},{"location":"changelog/#20200918","text":"Fix some pins for dev requirements Add 3.9-dev to actions Fix scrapli-asycnssh not in setup.py install_requires Retest everything! In general, just get this updated/ready for nornir-scrapli !","title":"2020.09.18"},{"location":"changelog/#20200726","text":"Update to match scrapli core -- moved to updated timeout decorator, fixed a test to match a better exception message","title":"2020.07.26"},{"location":"changelog/#20200712","text":"Minor improvements to response recording (should be a tick faster) Update decorators for async things to use the improved async_operation_timeout in scrapli 2020.07.12 Set strip_namespaces to False for AsyncNetconfScrape for consistency/sanity Update a few dev pins, update required pins to ensure no major lxml updates break things","title":"2020.07.12"},{"location":"changelog/#20200704","text":"First real release??? :)","title":"2020.07.04"},{"location":"changelog/#20200419","text":"Initial pypi release... very beta still","title":"2020.04.19"},{"location":"about/code_of_conduct/","text":"Code of Conduct \u00b6 Be excellent to each other!","title":"Code of Conduct"},{"location":"about/code_of_conduct/#code-of-conduct","text":"Be excellent to each other!","title":"Code of Conduct"},{"location":"about/contributing/","text":"Contributing \u00b6 Thanks for thinking about contributing! Contributions are not expected, but are quite welcome. Contributions of all kinds are welcomed -- typos, doc updates, adding examples, bug fixes, and feature adds. Some notes on contributing: Please open a GitHub discussion topic for any potential feature adds/changes to discuss them prior to opening a PR, this way everyone has a chance to chime in and make sure we're all on the same page! Please open an issue to discuss any bugs/bug fixes prior to opening a PR. Once we all have discussed any adds/changes, pull requests are very much welcome and appreciated! All PRs should pass tests/CI linting -- checkout the Makefile for some shortcuts for linting and testing. Please include tests! Even simple/basic tests are better than nothing -- it helps make sure changes in the future don't break functionality or make things act in unexpected ways!","title":"Contributing"},{"location":"about/contributing/#contributing","text":"Thanks for thinking about contributing! Contributions are not expected, but are quite welcome. Contributions of all kinds are welcomed -- typos, doc updates, adding examples, bug fixes, and feature adds. Some notes on contributing: Please open a GitHub discussion topic for any potential feature adds/changes to discuss them prior to opening a PR, this way everyone has a chance to chime in and make sure we're all on the same page! Please open an issue to discuss any bugs/bug fixes prior to opening a PR. Once we all have discussed any adds/changes, pull requests are very much welcome and appreciated! All PRs should pass tests/CI linting -- checkout the Makefile for some shortcuts for linting and testing. Please include tests! Even simple/basic tests are better than nothing -- it helps make sure changes in the future don't break functionality or make things act in unexpected ways!","title":"Contributing"},{"location":"api_docs/constants/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.constants \u00b6 scrapli_netconf.constants Expand source code \"\"\"scrapli_netconf.constants\"\"\" from enum import Enum class XmlParserVersion(Enum): COMPRESSED_PARSER = \"flat\" STANDARD_PARSER = \"standard\" class NetconfVersion(Enum): UNKNOWN = \"unknown\" VERSION_1_0 = \"1.0\" VERSION_1_1 = \"1.1\" class NetconfClientCapabilities(Enum): UNKNOWN = \"unknown\" CAPABILITIES_1_0 = \"\"\" urn:ietf:params:netconf:base:1.0 ]]>]]>\"\"\" CAPABILITIES_1_1 = \"\"\" urn:ietf:params:netconf:base:1.1 ]]>]]>\"\"\" Classes \u00b6 NetconfClientCapabilities \u00b6 1 An enumeration. Expand source code class NetconfClientCapabilities(Enum): UNKNOWN = \"unknown\" CAPABILITIES_1_0 = \"\"\" urn:ietf:params:netconf:base:1.0 ]]>]]>\"\"\" CAPABILITIES_1_1 = \"\"\" urn:ietf:params:netconf:base:1.1 ]]>]]>\"\"\" Ancestors (in MRO) \u00b6 enum.Enum Class variables \u00b6 CAPABILITIES_1_0 CAPABILITIES_1_1 UNKNOWN NetconfVersion \u00b6 1 An enumeration. Expand source code class NetconfVersion(Enum): UNKNOWN = \"unknown\" VERSION_1_0 = \"1.0\" VERSION_1_1 = \"1.1\" Ancestors (in MRO) \u00b6 enum.Enum Class variables \u00b6 UNKNOWN VERSION_1_0 VERSION_1_1 XmlParserVersion \u00b6 1 An enumeration. Expand source code class XmlParserVersion(Enum): COMPRESSED_PARSER = \"flat\" STANDARD_PARSER = \"standard\" Ancestors (in MRO) \u00b6 enum.Enum Class variables \u00b6 COMPRESSED_PARSER STANDARD_PARSER","title":"Constants"},{"location":"api_docs/constants/#module-scrapli_netconfconstants","text":"scrapli_netconf.constants Expand source code \"\"\"scrapli_netconf.constants\"\"\" from enum import Enum class XmlParserVersion(Enum): COMPRESSED_PARSER = \"flat\" STANDARD_PARSER = \"standard\" class NetconfVersion(Enum): UNKNOWN = \"unknown\" VERSION_1_0 = \"1.0\" VERSION_1_1 = \"1.1\" class NetconfClientCapabilities(Enum): UNKNOWN = \"unknown\" CAPABILITIES_1_0 = \"\"\" urn:ietf:params:netconf:base:1.0 ]]>]]>\"\"\" CAPABILITIES_1_1 = \"\"\" urn:ietf:params:netconf:base:1.1 ]]>]]>\"\"\"","title":"Module scrapli_netconf.constants"},{"location":"api_docs/constants/#classes","text":"","title":"Classes"},{"location":"api_docs/constants/#netconfclientcapabilities","text":"1 An enumeration. Expand source code class NetconfClientCapabilities(Enum): UNKNOWN = \"unknown\" CAPABILITIES_1_0 = \"\"\" urn:ietf:params:netconf:base:1.0 ]]>]]>\"\"\" CAPABILITIES_1_1 = \"\"\" urn:ietf:params:netconf:base:1.1 ]]>]]>\"\"\"","title":"NetconfClientCapabilities"},{"location":"api_docs/constants/#ancestors-in-mro","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"api_docs/constants/#class-variables","text":"CAPABILITIES_1_0 CAPABILITIES_1_1 UNKNOWN","title":"Class variables"},{"location":"api_docs/constants/#netconfversion","text":"1 An enumeration. Expand source code class NetconfVersion(Enum): UNKNOWN = \"unknown\" VERSION_1_0 = \"1.0\" VERSION_1_1 = \"1.1\"","title":"NetconfVersion"},{"location":"api_docs/constants/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"api_docs/constants/#class-variables_1","text":"UNKNOWN VERSION_1_0 VERSION_1_1","title":"Class variables"},{"location":"api_docs/constants/#xmlparserversion","text":"1 An enumeration. Expand source code class XmlParserVersion(Enum): COMPRESSED_PARSER = \"flat\" STANDARD_PARSER = \"standard\"","title":"XmlParserVersion"},{"location":"api_docs/constants/#ancestors-in-mro_2","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"api_docs/constants/#class-variables_2","text":"COMPRESSED_PARSER STANDARD_PARSER","title":"Class variables"},{"location":"api_docs/exceptions/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.exceptions \u00b6 scrapli_netconf.exceptions Expand source code \"\"\"scrapli_netconf.exceptions\"\"\" from scrapli.exceptions import ScrapliException class CouldNotExchangeCapabilities(ScrapliException): \"\"\"Exception for failure of capabilities exchange\"\"\" class CapabilityNotSupported(ScrapliException): \"\"\"Exception for unsupported capabilities\"\"\" Classes \u00b6 CapabilityNotSupported \u00b6 1 Exception for unsupported capabilities Expand source code class CapabilityNotSupported(ScrapliException): \"\"\"Exception for unsupported capabilities\"\"\" Ancestors (in MRO) \u00b6 scrapli.exceptions.ScrapliException builtins.Exception builtins.BaseException CouldNotExchangeCapabilities \u00b6 1 Exception for failure of capabilities exchange Expand source code class CouldNotExchangeCapabilities(ScrapliException): \"\"\"Exception for failure of capabilities exchange\"\"\" Ancestors (in MRO) \u00b6 scrapli.exceptions.ScrapliException builtins.Exception builtins.BaseException","title":"Exceptions"},{"location":"api_docs/exceptions/#module-scrapli_netconfexceptions","text":"scrapli_netconf.exceptions Expand source code \"\"\"scrapli_netconf.exceptions\"\"\" from scrapli.exceptions import ScrapliException class CouldNotExchangeCapabilities(ScrapliException): \"\"\"Exception for failure of capabilities exchange\"\"\" class CapabilityNotSupported(ScrapliException): \"\"\"Exception for unsupported capabilities\"\"\"","title":"Module scrapli_netconf.exceptions"},{"location":"api_docs/exceptions/#classes","text":"","title":"Classes"},{"location":"api_docs/exceptions/#capabilitynotsupported","text":"1 Exception for unsupported capabilities Expand source code class CapabilityNotSupported(ScrapliException): \"\"\"Exception for unsupported capabilities\"\"\"","title":"CapabilityNotSupported"},{"location":"api_docs/exceptions/#ancestors-in-mro","text":"scrapli.exceptions.ScrapliException builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"api_docs/exceptions/#couldnotexchangecapabilities","text":"1 Exception for failure of capabilities exchange Expand source code class CouldNotExchangeCapabilities(ScrapliException): \"\"\"Exception for failure of capabilities exchange\"\"\"","title":"CouldNotExchangeCapabilities"},{"location":"api_docs/exceptions/#ancestors-in-mro_1","text":"scrapli.exceptions.ScrapliException builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"api_docs/helper/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.helper \u00b6 scrapli_netconf.helper Expand source code \"\"\"scrapli_netconf.helper\"\"\" import re from logging import getLogger from lxml import objectify from lxml.etree import Element LOG = getLogger(\"scrapli_netconf.helper\") def remove_namespaces(tree: Element) -> Element: \"\"\" Remove all namespace tags from Element object Replace element tags like: {http://cisco.com/ns/yang/Cisco-IOS-XR-ipv4-bgp-oper}connection-state With: connection-state Args: tree: lxml Element Returns: Element: lxml Element with namespaces stripped out Raises: N/A \"\"\" for el in tree.getiterator(): if not hasattr(el.tag, \"find\"): continue el.tag = re.sub(r\"^{.*}\", \"\", el.tag) objectify.deannotate(tree, cleanup_namespaces=True) return tree Functions \u00b6 remove_namespaces \u00b6 remove_namespaces(tree: <cyfunction Element at 0x7f9eb0218ad0>) \u2011> <cyfunction Element at 0x7f9eb0218ad0> 1 2 3 4 5 6 7 8 9 10 11 12 13 Remove all namespace tags from Element object Replace element tags like: {http://cisco.com/ns/yang/Cisco-IOS-XR-ipv4-bgp-oper}connection-state With: connection-state Args: tree: lxml Element Returns: Element: lxml Element with namespaces stripped out Raises: N/A","title":"Helper"},{"location":"api_docs/helper/#module-scrapli_netconfhelper","text":"scrapli_netconf.helper Expand source code \"\"\"scrapli_netconf.helper\"\"\" import re from logging import getLogger from lxml import objectify from lxml.etree import Element LOG = getLogger(\"scrapli_netconf.helper\") def remove_namespaces(tree: Element) -> Element: \"\"\" Remove all namespace tags from Element object Replace element tags like: {http://cisco.com/ns/yang/Cisco-IOS-XR-ipv4-bgp-oper}connection-state With: connection-state Args: tree: lxml Element Returns: Element: lxml Element with namespaces stripped out Raises: N/A \"\"\" for el in tree.getiterator(): if not hasattr(el.tag, \"find\"): continue el.tag = re.sub(r\"^{.*}\", \"\", el.tag) objectify.deannotate(tree, cleanup_namespaces=True) return tree","title":"Module scrapli_netconf.helper"},{"location":"api_docs/helper/#functions","text":"","title":"Functions"},{"location":"api_docs/helper/#remove_namespaces","text":"remove_namespaces(tree: <cyfunction Element at 0x7f9eb0218ad0>) \u2011> <cyfunction Element at 0x7f9eb0218ad0> 1 2 3 4 5 6 7 8 9 10 11 12 13 Remove all namespace tags from Element object Replace element tags like: {http://cisco.com/ns/yang/Cisco-IOS-XR-ipv4-bgp-oper}connection-state With: connection-state Args: tree: lxml Element Returns: Element: lxml Element with namespaces stripped out Raises: N/A","title":"remove_namespaces"},{"location":"api_docs/response/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.response \u00b6 scrapli_netconf.response Expand source code \"\"\"scrapli_netconf.response\"\"\" import logging import re from datetime import datetime from typing import Any, Dict, List, Optional, Tuple, Union from lxml import etree from lxml.etree import Element from scrapli.exceptions import ScrapliCommandFailure from scrapli.response import Response from scrapli_netconf.constants import NetconfVersion from scrapli_netconf.helper import remove_namespaces LOG = logging.getLogger(\"response\") # \"chunk match\" matches two groups per section returned from the netconf server, first the length of # the response, and second the response itself. we use the length of the response to validate the # response is in fact X length. this regex is basically \"start at line feed, and match \"#123\" where # \"123\" is obviously any length of digits... then we don't capture zero or more newlines because we # dont care about them. Next we have the main capture group -- this starts with a negative lookahead # that says we want to stop matching as soon as we hit another \"#123\" *or* a \"##\" (end of message), # after that we match anything \".\" and that is the \"body\" of the response CHUNK_MATCH_1_1 = re.compile(pattern=rb\"^#(\\d+)(?:\\n*)(((?!#\\d+\\n+|##).)*)\", flags=re.M | re.S) PARSER = etree.XMLParser(remove_blank_text=True, recover=True) class NetconfResponse(Response): # intentionally overriding base class' list of strings for failed when contains failed_when_contains: List[bytes] # type: ignore[assignment] def __init__( self, netconf_version: NetconfVersion, xml_input: Element, strip_namespaces: bool = True, failed_when_contains: Optional[Union[bytes, List[bytes]]] = None, **kwargs: Any, ): \"\"\" Scrapli Netconf NetconfResponse Store channel_input, resulting output, and start/end/elapsed time information. Attempt to determine if command was successful or not and reflect that in a failed attribute. Args: netconf_version: string of netconf version; `1.0`|`1.1` xml_input: lxml Element of input to be sent to device strip_namespaces: strip out all namespaces if True, otherwise ignore them failed_when_contains: list of bytes that, if present in final output, represent a failed command/interaction -- should generally be left alone for netconf. Note that this differs from the base scrapli Response object as we want to be parsing/checking for these strings in raw byte strings we get back from the device kwargs: kwargs for instantiation of scrapli Response object supertype Returns: N/A # noqa: DAR202 Raises: ValueError: if invalid netconf_version string \"\"\" if netconf_version not in (NetconfVersion.VERSION_1_0, NetconfVersion.VERSION_1_1): raise ValueError(f\"`netconf_version` should be one of 1.0|1.1, got `{netconf_version}`\") self.netconf_version = netconf_version self.xml_input = xml_input self.strip_namespaces = strip_namespaces self.xml_result: Element super().__init__(**kwargs) if failed_when_contains is None: # match on both opening and closing tags too so we never have to think about/compare # things with namespaces (the closing tags wont have namespaces) failed_when_contains = [ b\" \", b\" \", b\" \", b\" \", ] if isinstance(failed_when_contains, bytes): failed_when_contains = [failed_when_contains] self.failed_when_contains = failed_when_contains self.error_messages: List[str] = [] def record_response(self, result: bytes) -> None: \"\"\" Record channel_input results and elapsed time of channel input/reading output Args: result: bytes result of channel_input Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" self.finish_time = datetime.now() self.elapsed_time = (self.finish_time - self.start_time).total_seconds() self.raw_result = result if not self.failed_when_contains: self.failed = False elif not any(err in self.raw_result for err in self.failed_when_contains): self.failed = False if self.netconf_version == NetconfVersion.VERSION_1_0: self._record_response_netconf_1_0() else: self._record_response_netconf_1_1() if self.failed: self._fetch_error_messages() def _record_response_netconf_1_0(self) -> None: \"\"\" Record response for netconf version 1.0 Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" # remove the message end characters and xml document header see: # https://github.com/scrapli/scrapli_netconf/issues/1 self.xml_result = etree.fromstring( self.raw_result.replace(b\"]]>]]>\", b\"\").replace( b' ', b\"\" ), parser=PARSER, ) if self.strip_namespaces: self.xml_result = remove_namespaces(self.xml_result) self.result = etree.tostring(self.xml_result, pretty_print=True).decode() else: self.result = etree.tostring(self.xml_result, pretty_print=True).decode() def _validate_chunk_size_netconf_1_1(self, result: Tuple[str, bytes]) -> None: \"\"\" Validate individual chunk size; handle parsing trailing new lines for chunk sizes It seems that some platforms behave slightly differently than others (looking at you IOSXE) in the way they count chunk sizes with respect to trailing whitespace. Per my reading of the RFC, the response for a netconf 1.1 response should look like this: 1 2 3 ##XYZ <somexml> ## Where \"XYZ\" is an integer number of the count of chars in the following chunk (the chars up to the next \"##\" symbols), then the actual XML response, then a new line(!!!!) and a pair of hash symbols to indicate the chunk is complete. IOSXE seems to *not* want to see the newline between the XML payload and the double hash symbols... instead when it sees that newline it immediately returns the response. This breaks the core behavior of scrapli in that scrapli always writes the input, then reads the written inputs off the channel *before* sending a return character. This ensures that we never have to deal with stripping out the inputs and such because it has already been read. With IOSXE Behaving this way, we have to instead use `send_input` with the `eager` flag set -- this means that we do *not* read the inputs, we simply send a return. We then have to do a little extra parsing to strip out the inputs, but thats no big deal... Where this finally gets to \"spacing\" -- IOSXE seems to include trailing newlines *sometimes* but not other times, whereas IOSXR (for example) *always* counts a single trailing newline (after the XML). SO.... long story long... (the above chunk stuff doesn't necessarily matter for this, but felt like as good a place to document it as any...) this method deals w/ newline counts -- we check the expected chunk length against the actual char count, the char count with all trailing whitespace stripped, and the count of the chunk + a *single* trailing newline character... FIN Args: result: Tuple from re.findall parsing the full response object Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" expected_len = int(result[0]) result_value = result[1] actual_len = len(result_value) rstripped_len = len(result_value.rstrip()) trailing_newline_count = actual_len - rstripped_len if trailing_newline_count > 1: extraneous_trailing_newline_count = trailing_newline_count - 1 else: extraneous_trailing_newline_count = 1 trimmed_newline_len = actual_len - extraneous_trailing_newline_count if rstripped_len == 0: # at least nokia tends to have itty bitty chunks of one element, and/or chunks that have # *only* whitespace and our regex ignores this, so if there was/is nothing in the result # section we can assume it was just whitespace and move on w/our lives actual_len = expected_len if expected_len == actual_len: return if expected_len == rstripped_len: return if expected_len == trimmed_newline_len: return LOG.critical( f\"Return element length invalid, expected {expected_len} got {actual_len} for \" f\"element: {repr(result_value)}\" ) self.failed = True def _record_response_netconf_1_1(self) -> None: \"\"\" Record response for netconf version 1.1 Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" result_sections = re.findall(pattern=CHUNK_MATCH_1_1, string=self.raw_result) # validate all received data for result in result_sections: self._validate_chunk_size_netconf_1_1(result=result) self.xml_result = etree.fromstring( b\"\\n\".join( [ # remove the message end characters and xml document header see: # https://github.com/scrapli/scrapli_netconf/issues/1 result[1].replace(b' ', b\"\") for result in result_sections ] ), parser=PARSER, ) if self.strip_namespaces: self.xml_result = remove_namespaces(self.xml_result) self.result = etree.tostring(self.xml_result, pretty_print=True).decode() else: self.result = etree.tostring(self.xml_result, pretty_print=True).decode() def _fetch_error_messages(self) -> None: \"\"\" Fetch all error messages (if any) RFC states that there MAY be more than one rpc-error so we just xpath for all \"error-message\" tags and pull out the text of those elements. The strip is just to remove leading/trailing white space to make things look a bit nicer. Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" err_messages = self.xml_result.xpath(\"//rpc-error/error-message\") self.error_messages = [err.text.strip() for err in err_messages] def get_xml_elements(self) -> Dict[str, Element]: \"\"\" Parse each section under \"data\" into a dict of {tag: Element} for easy viewing/parsing Args: N/A Returns: xml_elements: dictionary of tag: Element Raises: N/A \"\"\" xml_elements = {} data_element = self.xml_result.find(\"data\", namespaces=self.xml_result.nsmap) # juniper doesn't return data in a \"data\" element for bare rpc calls, guard against that # breaking the iterchildren() if data_element is not None: for child in data_element.iterchildren(): _tag = etree.QName(child.tag).localname xml_elements[_tag] = child return xml_elements def textfsm_parse_output(self, to_dict: bool = True) -> Union[Dict[str, Any], List[Any]]: \"\"\" Override scrapli Response `textfsm_parse_output` method; not applicable for netconf Args: to_dict: ignore, only here to ensure compliance with supertype method Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always \"\"\" raise NotImplementedError(\"No textfsm parsing for netconf output!\") def genie_parse_output(self) -> Union[Dict[str, Any], List[Any]]: \"\"\" Override scrapli Response `genie_parse_output` method; not applicable for netconf Args: N/A Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always \"\"\" raise NotImplementedError(\"No genie parsing for netconf output!\") def raise_for_status(self) -> None: \"\"\" Raise a `ScrapliCommandFailure` if any elements are failed Overrides scrapli core Response.raise_for_status to include rpc error message(s). Args: N/A Returns: None Raises: ScrapliCommandFailure: if any elements are failed \"\"\" if self.failed: raise ScrapliCommandFailure( f\"operation failed, reported rpc errors: {self.error_messages}\" ) Classes \u00b6 NetconfResponse \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Scrapli Netconf NetconfResponse Store channel_input, resulting output, and start/end/elapsed time information. Attempt to determine if command was successful or not and reflect that in a failed attribute. Args: netconf_version: string of netconf version; `1.0`|`1.1` xml_input: lxml Element of input to be sent to device strip_namespaces: strip out all namespaces if True, otherwise ignore them failed_when_contains: list of bytes that, if present in final output, represent a failed command/interaction -- should generally be left alone for netconf. Note that this differs from the base scrapli Response object as we want to be parsing/checking for these strings in raw byte strings we get back from the device kwargs: kwargs for instantiation of scrapli Response object supertype Returns: N/A # noqa: DAR202 Raises: ValueError: if invalid netconf_version string Expand source code class NetconfResponse(Response): # intentionally overriding base class' list of strings for failed when contains failed_when_contains: List[bytes] # type: ignore[assignment] def __init__( self, netconf_version: NetconfVersion, xml_input: Element, strip_namespaces: bool = True, failed_when_contains: Optional[Union[bytes, List[bytes]]] = None, **kwargs: Any, ): \"\"\" Scrapli Netconf NetconfResponse Store channel_input, resulting output, and start/end/elapsed time information. Attempt to determine if command was successful or not and reflect that in a failed attribute. Args: netconf_version: string of netconf version; `1.0`|`1.1` xml_input: lxml Element of input to be sent to device strip_namespaces: strip out all namespaces if True, otherwise ignore them failed_when_contains: list of bytes that, if present in final output, represent a failed command/interaction -- should generally be left alone for netconf. Note that this differs from the base scrapli Response object as we want to be parsing/checking for these strings in raw byte strings we get back from the device kwargs: kwargs for instantiation of scrapli Response object supertype Returns: N/A # noqa: DAR202 Raises: ValueError: if invalid netconf_version string \"\"\" if netconf_version not in (NetconfVersion.VERSION_1_0, NetconfVersion.VERSION_1_1): raise ValueError(f\"`netconf_version` should be one of 1.0|1.1, got `{netconf_version}`\") self.netconf_version = netconf_version self.xml_input = xml_input self.strip_namespaces = strip_namespaces self.xml_result: Element super().__init__(**kwargs) if failed_when_contains is None: # match on both opening and closing tags too so we never have to think about/compare # things with namespaces (the closing tags wont have namespaces) failed_when_contains = [ b\" \", b\" \", b\" \", b\" \", ] if isinstance(failed_when_contains, bytes): failed_when_contains = [failed_when_contains] self.failed_when_contains = failed_when_contains self.error_messages: List[str] = [] def record_response(self, result: bytes) -> None: \"\"\" Record channel_input results and elapsed time of channel input/reading output Args: result: bytes result of channel_input Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" self.finish_time = datetime.now() self.elapsed_time = (self.finish_time - self.start_time).total_seconds() self.raw_result = result if not self.failed_when_contains: self.failed = False elif not any(err in self.raw_result for err in self.failed_when_contains): self.failed = False if self.netconf_version == NetconfVersion.VERSION_1_0: self._record_response_netconf_1_0() else: self._record_response_netconf_1_1() if self.failed: self._fetch_error_messages() def _record_response_netconf_1_0(self) -> None: \"\"\" Record response for netconf version 1.0 Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" # remove the message end characters and xml document header see: # https://github.com/scrapli/scrapli_netconf/issues/1 self.xml_result = etree.fromstring( self.raw_result.replace(b\"]]>]]>\", b\"\").replace( b' ', b\"\" ), parser=PARSER, ) if self.strip_namespaces: self.xml_result = remove_namespaces(self.xml_result) self.result = etree.tostring(self.xml_result, pretty_print=True).decode() else: self.result = etree.tostring(self.xml_result, pretty_print=True).decode() def _validate_chunk_size_netconf_1_1(self, result: Tuple[str, bytes]) -> None: \"\"\" Validate individual chunk size; handle parsing trailing new lines for chunk sizes It seems that some platforms behave slightly differently than others (looking at you IOSXE) in the way they count chunk sizes with respect to trailing whitespace. Per my reading of the RFC, the response for a netconf 1.1 response should look like this: 1 2 3 ##XYZ <somexml> ## Where \"XYZ\" is an integer number of the count of chars in the following chunk (the chars up to the next \"##\" symbols), then the actual XML response, then a new line(!!!!) and a pair of hash symbols to indicate the chunk is complete. IOSXE seems to *not* want to see the newline between the XML payload and the double hash symbols... instead when it sees that newline it immediately returns the response. This breaks the core behavior of scrapli in that scrapli always writes the input, then reads the written inputs off the channel *before* sending a return character. This ensures that we never have to deal with stripping out the inputs and such because it has already been read. With IOSXE Behaving this way, we have to instead use `send_input` with the `eager` flag set -- this means that we do *not* read the inputs, we simply send a return. We then have to do a little extra parsing to strip out the inputs, but thats no big deal... Where this finally gets to \"spacing\" -- IOSXE seems to include trailing newlines *sometimes* but not other times, whereas IOSXR (for example) *always* counts a single trailing newline (after the XML). SO.... long story long... (the above chunk stuff doesn't necessarily matter for this, but felt like as good a place to document it as any...) this method deals w/ newline counts -- we check the expected chunk length against the actual char count, the char count with all trailing whitespace stripped, and the count of the chunk + a *single* trailing newline character... FIN Args: result: Tuple from re.findall parsing the full response object Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" expected_len = int(result[0]) result_value = result[1] actual_len = len(result_value) rstripped_len = len(result_value.rstrip()) trailing_newline_count = actual_len - rstripped_len if trailing_newline_count > 1: extraneous_trailing_newline_count = trailing_newline_count - 1 else: extraneous_trailing_newline_count = 1 trimmed_newline_len = actual_len - extraneous_trailing_newline_count if rstripped_len == 0: # at least nokia tends to have itty bitty chunks of one element, and/or chunks that have # *only* whitespace and our regex ignores this, so if there was/is nothing in the result # section we can assume it was just whitespace and move on w/our lives actual_len = expected_len if expected_len == actual_len: return if expected_len == rstripped_len: return if expected_len == trimmed_newline_len: return LOG.critical( f\"Return element length invalid, expected {expected_len} got {actual_len} for \" f\"element: {repr(result_value)}\" ) self.failed = True def _record_response_netconf_1_1(self) -> None: \"\"\" Record response for netconf version 1.1 Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" result_sections = re.findall(pattern=CHUNK_MATCH_1_1, string=self.raw_result) # validate all received data for result in result_sections: self._validate_chunk_size_netconf_1_1(result=result) self.xml_result = etree.fromstring( b\"\\n\".join( [ # remove the message end characters and xml document header see: # https://github.com/scrapli/scrapli_netconf/issues/1 result[1].replace(b' ', b\"\") for result in result_sections ] ), parser=PARSER, ) if self.strip_namespaces: self.xml_result = remove_namespaces(self.xml_result) self.result = etree.tostring(self.xml_result, pretty_print=True).decode() else: self.result = etree.tostring(self.xml_result, pretty_print=True).decode() def _fetch_error_messages(self) -> None: \"\"\" Fetch all error messages (if any) RFC states that there MAY be more than one rpc-error so we just xpath for all \"error-message\" tags and pull out the text of those elements. The strip is just to remove leading/trailing white space to make things look a bit nicer. Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" err_messages = self.xml_result.xpath(\"//rpc-error/error-message\") self.error_messages = [err.text.strip() for err in err_messages] def get_xml_elements(self) -> Dict[str, Element]: \"\"\" Parse each section under \"data\" into a dict of {tag: Element} for easy viewing/parsing Args: N/A Returns: xml_elements: dictionary of tag: Element Raises: N/A \"\"\" xml_elements = {} data_element = self.xml_result.find(\"data\", namespaces=self.xml_result.nsmap) # juniper doesn't return data in a \"data\" element for bare rpc calls, guard against that # breaking the iterchildren() if data_element is not None: for child in data_element.iterchildren(): _tag = etree.QName(child.tag).localname xml_elements[_tag] = child return xml_elements def textfsm_parse_output(self, to_dict: bool = True) -> Union[Dict[str, Any], List[Any]]: \"\"\" Override scrapli Response `textfsm_parse_output` method; not applicable for netconf Args: to_dict: ignore, only here to ensure compliance with supertype method Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always \"\"\" raise NotImplementedError(\"No textfsm parsing for netconf output!\") def genie_parse_output(self) -> Union[Dict[str, Any], List[Any]]: \"\"\" Override scrapli Response `genie_parse_output` method; not applicable for netconf Args: N/A Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always \"\"\" raise NotImplementedError(\"No genie parsing for netconf output!\") def raise_for_status(self) -> None: \"\"\" Raise a `ScrapliCommandFailure` if any elements are failed Overrides scrapli core Response.raise_for_status to include rpc error message(s). Args: N/A Returns: None Raises: ScrapliCommandFailure: if any elements are failed \"\"\" if self.failed: raise ScrapliCommandFailure( f\"operation failed, reported rpc errors: {self.error_messages}\" ) Ancestors (in MRO) \u00b6 scrapli.response.Response Class variables \u00b6 failed_when_contains: List[bytes] Methods \u00b6 genie_parse_output \u00b6 genie_parse_output(self) \u2011> Union[Dict[str, Any], List[Any]] 1 2 3 4 5 6 7 8 9 10 Override scrapli Response `genie_parse_output` method; not applicable for netconf Args: N/A Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always get_xml_elements \u00b6 get_xml_elements(self) \u2011> Dict[str, <cyfunction Element at 0x7f9eb0218ad0>] 1 2 3 4 5 6 7 8 9 10 Parse each section under \"data\" into a dict of {tag: Element} for easy viewing/parsing Args: N/A Returns: xml_elements: dictionary of tag: Element Raises: N/A raise_for_status \u00b6 raise_for_status(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 Raise a `ScrapliCommandFailure` if any elements are failed Overrides scrapli core Response.raise_for_status to include rpc error message(s). Args: N/A Returns: None Raises: ScrapliCommandFailure: if any elements are failed record_response \u00b6 record_response(self, result: bytes) \u2011> None 1 2 3 4 5 6 7 8 9 10 Record channel_input results and elapsed time of channel input/reading output Args: result: bytes result of channel_input Returns: N/A # noqa: DAR202 Raises: N/A textfsm_parse_output \u00b6 textfsm_parse_output(self, to_dict: bool = True) \u2011> Union[Dict[str, Any], List[Any]] 1 2 3 4 5 6 7 8 9 10 Override scrapli Response `textfsm_parse_output` method; not applicable for netconf Args: to_dict: ignore, only here to ensure compliance with supertype method Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always","title":"Response"},{"location":"api_docs/response/#module-scrapli_netconfresponse","text":"scrapli_netconf.response Expand source code \"\"\"scrapli_netconf.response\"\"\" import logging import re from datetime import datetime from typing import Any, Dict, List, Optional, Tuple, Union from lxml import etree from lxml.etree import Element from scrapli.exceptions import ScrapliCommandFailure from scrapli.response import Response from scrapli_netconf.constants import NetconfVersion from scrapli_netconf.helper import remove_namespaces LOG = logging.getLogger(\"response\") # \"chunk match\" matches two groups per section returned from the netconf server, first the length of # the response, and second the response itself. we use the length of the response to validate the # response is in fact X length. this regex is basically \"start at line feed, and match \"#123\" where # \"123\" is obviously any length of digits... then we don't capture zero or more newlines because we # dont care about them. Next we have the main capture group -- this starts with a negative lookahead # that says we want to stop matching as soon as we hit another \"#123\" *or* a \"##\" (end of message), # after that we match anything \".\" and that is the \"body\" of the response CHUNK_MATCH_1_1 = re.compile(pattern=rb\"^#(\\d+)(?:\\n*)(((?!#\\d+\\n+|##).)*)\", flags=re.M | re.S) PARSER = etree.XMLParser(remove_blank_text=True, recover=True) class NetconfResponse(Response): # intentionally overriding base class' list of strings for failed when contains failed_when_contains: List[bytes] # type: ignore[assignment] def __init__( self, netconf_version: NetconfVersion, xml_input: Element, strip_namespaces: bool = True, failed_when_contains: Optional[Union[bytes, List[bytes]]] = None, **kwargs: Any, ): \"\"\" Scrapli Netconf NetconfResponse Store channel_input, resulting output, and start/end/elapsed time information. Attempt to determine if command was successful or not and reflect that in a failed attribute. Args: netconf_version: string of netconf version; `1.0`|`1.1` xml_input: lxml Element of input to be sent to device strip_namespaces: strip out all namespaces if True, otherwise ignore them failed_when_contains: list of bytes that, if present in final output, represent a failed command/interaction -- should generally be left alone for netconf. Note that this differs from the base scrapli Response object as we want to be parsing/checking for these strings in raw byte strings we get back from the device kwargs: kwargs for instantiation of scrapli Response object supertype Returns: N/A # noqa: DAR202 Raises: ValueError: if invalid netconf_version string \"\"\" if netconf_version not in (NetconfVersion.VERSION_1_0, NetconfVersion.VERSION_1_1): raise ValueError(f\"`netconf_version` should be one of 1.0|1.1, got `{netconf_version}`\") self.netconf_version = netconf_version self.xml_input = xml_input self.strip_namespaces = strip_namespaces self.xml_result: Element super().__init__(**kwargs) if failed_when_contains is None: # match on both opening and closing tags too so we never have to think about/compare # things with namespaces (the closing tags wont have namespaces) failed_when_contains = [ b\" \", b\" \", b\" \", b\" \", ] if isinstance(failed_when_contains, bytes): failed_when_contains = [failed_when_contains] self.failed_when_contains = failed_when_contains self.error_messages: List[str] = [] def record_response(self, result: bytes) -> None: \"\"\" Record channel_input results and elapsed time of channel input/reading output Args: result: bytes result of channel_input Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" self.finish_time = datetime.now() self.elapsed_time = (self.finish_time - self.start_time).total_seconds() self.raw_result = result if not self.failed_when_contains: self.failed = False elif not any(err in self.raw_result for err in self.failed_when_contains): self.failed = False if self.netconf_version == NetconfVersion.VERSION_1_0: self._record_response_netconf_1_0() else: self._record_response_netconf_1_1() if self.failed: self._fetch_error_messages() def _record_response_netconf_1_0(self) -> None: \"\"\" Record response for netconf version 1.0 Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" # remove the message end characters and xml document header see: # https://github.com/scrapli/scrapli_netconf/issues/1 self.xml_result = etree.fromstring( self.raw_result.replace(b\"]]>]]>\", b\"\").replace( b' ', b\"\" ), parser=PARSER, ) if self.strip_namespaces: self.xml_result = remove_namespaces(self.xml_result) self.result = etree.tostring(self.xml_result, pretty_print=True).decode() else: self.result = etree.tostring(self.xml_result, pretty_print=True).decode() def _validate_chunk_size_netconf_1_1(self, result: Tuple[str, bytes]) -> None: \"\"\" Validate individual chunk size; handle parsing trailing new lines for chunk sizes It seems that some platforms behave slightly differently than others (looking at you IOSXE) in the way they count chunk sizes with respect to trailing whitespace. Per my reading of the RFC, the response for a netconf 1.1 response should look like this: 1 2 3 ##XYZ <somexml> ## Where \"XYZ\" is an integer number of the count of chars in the following chunk (the chars up to the next \"##\" symbols), then the actual XML response, then a new line(!!!!) and a pair of hash symbols to indicate the chunk is complete. IOSXE seems to *not* want to see the newline between the XML payload and the double hash symbols... instead when it sees that newline it immediately returns the response. This breaks the core behavior of scrapli in that scrapli always writes the input, then reads the written inputs off the channel *before* sending a return character. This ensures that we never have to deal with stripping out the inputs and such because it has already been read. With IOSXE Behaving this way, we have to instead use `send_input` with the `eager` flag set -- this means that we do *not* read the inputs, we simply send a return. We then have to do a little extra parsing to strip out the inputs, but thats no big deal... Where this finally gets to \"spacing\" -- IOSXE seems to include trailing newlines *sometimes* but not other times, whereas IOSXR (for example) *always* counts a single trailing newline (after the XML). SO.... long story long... (the above chunk stuff doesn't necessarily matter for this, but felt like as good a place to document it as any...) this method deals w/ newline counts -- we check the expected chunk length against the actual char count, the char count with all trailing whitespace stripped, and the count of the chunk + a *single* trailing newline character... FIN Args: result: Tuple from re.findall parsing the full response object Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" expected_len = int(result[0]) result_value = result[1] actual_len = len(result_value) rstripped_len = len(result_value.rstrip()) trailing_newline_count = actual_len - rstripped_len if trailing_newline_count > 1: extraneous_trailing_newline_count = trailing_newline_count - 1 else: extraneous_trailing_newline_count = 1 trimmed_newline_len = actual_len - extraneous_trailing_newline_count if rstripped_len == 0: # at least nokia tends to have itty bitty chunks of one element, and/or chunks that have # *only* whitespace and our regex ignores this, so if there was/is nothing in the result # section we can assume it was just whitespace and move on w/our lives actual_len = expected_len if expected_len == actual_len: return if expected_len == rstripped_len: return if expected_len == trimmed_newline_len: return LOG.critical( f\"Return element length invalid, expected {expected_len} got {actual_len} for \" f\"element: {repr(result_value)}\" ) self.failed = True def _record_response_netconf_1_1(self) -> None: \"\"\" Record response for netconf version 1.1 Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" result_sections = re.findall(pattern=CHUNK_MATCH_1_1, string=self.raw_result) # validate all received data for result in result_sections: self._validate_chunk_size_netconf_1_1(result=result) self.xml_result = etree.fromstring( b\"\\n\".join( [ # remove the message end characters and xml document header see: # https://github.com/scrapli/scrapli_netconf/issues/1 result[1].replace(b' ', b\"\") for result in result_sections ] ), parser=PARSER, ) if self.strip_namespaces: self.xml_result = remove_namespaces(self.xml_result) self.result = etree.tostring(self.xml_result, pretty_print=True).decode() else: self.result = etree.tostring(self.xml_result, pretty_print=True).decode() def _fetch_error_messages(self) -> None: \"\"\" Fetch all error messages (if any) RFC states that there MAY be more than one rpc-error so we just xpath for all \"error-message\" tags and pull out the text of those elements. The strip is just to remove leading/trailing white space to make things look a bit nicer. Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" err_messages = self.xml_result.xpath(\"//rpc-error/error-message\") self.error_messages = [err.text.strip() for err in err_messages] def get_xml_elements(self) -> Dict[str, Element]: \"\"\" Parse each section under \"data\" into a dict of {tag: Element} for easy viewing/parsing Args: N/A Returns: xml_elements: dictionary of tag: Element Raises: N/A \"\"\" xml_elements = {} data_element = self.xml_result.find(\"data\", namespaces=self.xml_result.nsmap) # juniper doesn't return data in a \"data\" element for bare rpc calls, guard against that # breaking the iterchildren() if data_element is not None: for child in data_element.iterchildren(): _tag = etree.QName(child.tag).localname xml_elements[_tag] = child return xml_elements def textfsm_parse_output(self, to_dict: bool = True) -> Union[Dict[str, Any], List[Any]]: \"\"\" Override scrapli Response `textfsm_parse_output` method; not applicable for netconf Args: to_dict: ignore, only here to ensure compliance with supertype method Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always \"\"\" raise NotImplementedError(\"No textfsm parsing for netconf output!\") def genie_parse_output(self) -> Union[Dict[str, Any], List[Any]]: \"\"\" Override scrapli Response `genie_parse_output` method; not applicable for netconf Args: N/A Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always \"\"\" raise NotImplementedError(\"No genie parsing for netconf output!\") def raise_for_status(self) -> None: \"\"\" Raise a `ScrapliCommandFailure` if any elements are failed Overrides scrapli core Response.raise_for_status to include rpc error message(s). Args: N/A Returns: None Raises: ScrapliCommandFailure: if any elements are failed \"\"\" if self.failed: raise ScrapliCommandFailure( f\"operation failed, reported rpc errors: {self.error_messages}\" )","title":"Module scrapli_netconf.response"},{"location":"api_docs/response/#classes","text":"","title":"Classes"},{"location":"api_docs/response/#netconfresponse","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Scrapli Netconf NetconfResponse Store channel_input, resulting output, and start/end/elapsed time information. Attempt to determine if command was successful or not and reflect that in a failed attribute. Args: netconf_version: string of netconf version; `1.0`|`1.1` xml_input: lxml Element of input to be sent to device strip_namespaces: strip out all namespaces if True, otherwise ignore them failed_when_contains: list of bytes that, if present in final output, represent a failed command/interaction -- should generally be left alone for netconf. Note that this differs from the base scrapli Response object as we want to be parsing/checking for these strings in raw byte strings we get back from the device kwargs: kwargs for instantiation of scrapli Response object supertype Returns: N/A # noqa: DAR202 Raises: ValueError: if invalid netconf_version string Expand source code class NetconfResponse(Response): # intentionally overriding base class' list of strings for failed when contains failed_when_contains: List[bytes] # type: ignore[assignment] def __init__( self, netconf_version: NetconfVersion, xml_input: Element, strip_namespaces: bool = True, failed_when_contains: Optional[Union[bytes, List[bytes]]] = None, **kwargs: Any, ): \"\"\" Scrapli Netconf NetconfResponse Store channel_input, resulting output, and start/end/elapsed time information. Attempt to determine if command was successful or not and reflect that in a failed attribute. Args: netconf_version: string of netconf version; `1.0`|`1.1` xml_input: lxml Element of input to be sent to device strip_namespaces: strip out all namespaces if True, otherwise ignore them failed_when_contains: list of bytes that, if present in final output, represent a failed command/interaction -- should generally be left alone for netconf. Note that this differs from the base scrapli Response object as we want to be parsing/checking for these strings in raw byte strings we get back from the device kwargs: kwargs for instantiation of scrapli Response object supertype Returns: N/A # noqa: DAR202 Raises: ValueError: if invalid netconf_version string \"\"\" if netconf_version not in (NetconfVersion.VERSION_1_0, NetconfVersion.VERSION_1_1): raise ValueError(f\"`netconf_version` should be one of 1.0|1.1, got `{netconf_version}`\") self.netconf_version = netconf_version self.xml_input = xml_input self.strip_namespaces = strip_namespaces self.xml_result: Element super().__init__(**kwargs) if failed_when_contains is None: # match on both opening and closing tags too so we never have to think about/compare # things with namespaces (the closing tags wont have namespaces) failed_when_contains = [ b\" \", b\" \", b\" \", b\" \", ] if isinstance(failed_when_contains, bytes): failed_when_contains = [failed_when_contains] self.failed_when_contains = failed_when_contains self.error_messages: List[str] = [] def record_response(self, result: bytes) -> None: \"\"\" Record channel_input results and elapsed time of channel input/reading output Args: result: bytes result of channel_input Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" self.finish_time = datetime.now() self.elapsed_time = (self.finish_time - self.start_time).total_seconds() self.raw_result = result if not self.failed_when_contains: self.failed = False elif not any(err in self.raw_result for err in self.failed_when_contains): self.failed = False if self.netconf_version == NetconfVersion.VERSION_1_0: self._record_response_netconf_1_0() else: self._record_response_netconf_1_1() if self.failed: self._fetch_error_messages() def _record_response_netconf_1_0(self) -> None: \"\"\" Record response for netconf version 1.0 Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" # remove the message end characters and xml document header see: # https://github.com/scrapli/scrapli_netconf/issues/1 self.xml_result = etree.fromstring( self.raw_result.replace(b\"]]>]]>\", b\"\").replace( b' ', b\"\" ), parser=PARSER, ) if self.strip_namespaces: self.xml_result = remove_namespaces(self.xml_result) self.result = etree.tostring(self.xml_result, pretty_print=True).decode() else: self.result = etree.tostring(self.xml_result, pretty_print=True).decode() def _validate_chunk_size_netconf_1_1(self, result: Tuple[str, bytes]) -> None: \"\"\" Validate individual chunk size; handle parsing trailing new lines for chunk sizes It seems that some platforms behave slightly differently than others (looking at you IOSXE) in the way they count chunk sizes with respect to trailing whitespace. Per my reading of the RFC, the response for a netconf 1.1 response should look like this: 1 2 3 ##XYZ <somexml> ## Where \"XYZ\" is an integer number of the count of chars in the following chunk (the chars up to the next \"##\" symbols), then the actual XML response, then a new line(!!!!) and a pair of hash symbols to indicate the chunk is complete. IOSXE seems to *not* want to see the newline between the XML payload and the double hash symbols... instead when it sees that newline it immediately returns the response. This breaks the core behavior of scrapli in that scrapli always writes the input, then reads the written inputs off the channel *before* sending a return character. This ensures that we never have to deal with stripping out the inputs and such because it has already been read. With IOSXE Behaving this way, we have to instead use `send_input` with the `eager` flag set -- this means that we do *not* read the inputs, we simply send a return. We then have to do a little extra parsing to strip out the inputs, but thats no big deal... Where this finally gets to \"spacing\" -- IOSXE seems to include trailing newlines *sometimes* but not other times, whereas IOSXR (for example) *always* counts a single trailing newline (after the XML). SO.... long story long... (the above chunk stuff doesn't necessarily matter for this, but felt like as good a place to document it as any...) this method deals w/ newline counts -- we check the expected chunk length against the actual char count, the char count with all trailing whitespace stripped, and the count of the chunk + a *single* trailing newline character... FIN Args: result: Tuple from re.findall parsing the full response object Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" expected_len = int(result[0]) result_value = result[1] actual_len = len(result_value) rstripped_len = len(result_value.rstrip()) trailing_newline_count = actual_len - rstripped_len if trailing_newline_count > 1: extraneous_trailing_newline_count = trailing_newline_count - 1 else: extraneous_trailing_newline_count = 1 trimmed_newline_len = actual_len - extraneous_trailing_newline_count if rstripped_len == 0: # at least nokia tends to have itty bitty chunks of one element, and/or chunks that have # *only* whitespace and our regex ignores this, so if there was/is nothing in the result # section we can assume it was just whitespace and move on w/our lives actual_len = expected_len if expected_len == actual_len: return if expected_len == rstripped_len: return if expected_len == trimmed_newline_len: return LOG.critical( f\"Return element length invalid, expected {expected_len} got {actual_len} for \" f\"element: {repr(result_value)}\" ) self.failed = True def _record_response_netconf_1_1(self) -> None: \"\"\" Record response for netconf version 1.1 Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" result_sections = re.findall(pattern=CHUNK_MATCH_1_1, string=self.raw_result) # validate all received data for result in result_sections: self._validate_chunk_size_netconf_1_1(result=result) self.xml_result = etree.fromstring( b\"\\n\".join( [ # remove the message end characters and xml document header see: # https://github.com/scrapli/scrapli_netconf/issues/1 result[1].replace(b' ', b\"\") for result in result_sections ] ), parser=PARSER, ) if self.strip_namespaces: self.xml_result = remove_namespaces(self.xml_result) self.result = etree.tostring(self.xml_result, pretty_print=True).decode() else: self.result = etree.tostring(self.xml_result, pretty_print=True).decode() def _fetch_error_messages(self) -> None: \"\"\" Fetch all error messages (if any) RFC states that there MAY be more than one rpc-error so we just xpath for all \"error-message\" tags and pull out the text of those elements. The strip is just to remove leading/trailing white space to make things look a bit nicer. Args: N/A Returns: N/A # noqa: DAR202 Raises: N/A \"\"\" err_messages = self.xml_result.xpath(\"//rpc-error/error-message\") self.error_messages = [err.text.strip() for err in err_messages] def get_xml_elements(self) -> Dict[str, Element]: \"\"\" Parse each section under \"data\" into a dict of {tag: Element} for easy viewing/parsing Args: N/A Returns: xml_elements: dictionary of tag: Element Raises: N/A \"\"\" xml_elements = {} data_element = self.xml_result.find(\"data\", namespaces=self.xml_result.nsmap) # juniper doesn't return data in a \"data\" element for bare rpc calls, guard against that # breaking the iterchildren() if data_element is not None: for child in data_element.iterchildren(): _tag = etree.QName(child.tag).localname xml_elements[_tag] = child return xml_elements def textfsm_parse_output(self, to_dict: bool = True) -> Union[Dict[str, Any], List[Any]]: \"\"\" Override scrapli Response `textfsm_parse_output` method; not applicable for netconf Args: to_dict: ignore, only here to ensure compliance with supertype method Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always \"\"\" raise NotImplementedError(\"No textfsm parsing for netconf output!\") def genie_parse_output(self) -> Union[Dict[str, Any], List[Any]]: \"\"\" Override scrapli Response `genie_parse_output` method; not applicable for netconf Args: N/A Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always \"\"\" raise NotImplementedError(\"No genie parsing for netconf output!\") def raise_for_status(self) -> None: \"\"\" Raise a `ScrapliCommandFailure` if any elements are failed Overrides scrapli core Response.raise_for_status to include rpc error message(s). Args: N/A Returns: None Raises: ScrapliCommandFailure: if any elements are failed \"\"\" if self.failed: raise ScrapliCommandFailure( f\"operation failed, reported rpc errors: {self.error_messages}\" )","title":"NetconfResponse"},{"location":"api_docs/response/#ancestors-in-mro","text":"scrapli.response.Response","title":"Ancestors (in MRO)"},{"location":"api_docs/response/#class-variables","text":"failed_when_contains: List[bytes]","title":"Class variables"},{"location":"api_docs/response/#methods","text":"","title":"Methods"},{"location":"api_docs/response/#genie_parse_output","text":"genie_parse_output(self) \u2011> Union[Dict[str, Any], List[Any]] 1 2 3 4 5 6 7 8 9 10 Override scrapli Response `genie_parse_output` method; not applicable for netconf Args: N/A Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always","title":"genie_parse_output"},{"location":"api_docs/response/#get_xml_elements","text":"get_xml_elements(self) \u2011> Dict[str, <cyfunction Element at 0x7f9eb0218ad0>] 1 2 3 4 5 6 7 8 9 10 Parse each section under \"data\" into a dict of {tag: Element} for easy viewing/parsing Args: N/A Returns: xml_elements: dictionary of tag: Element Raises: N/A","title":"get_xml_elements"},{"location":"api_docs/response/#raise_for_status","text":"raise_for_status(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 Raise a `ScrapliCommandFailure` if any elements are failed Overrides scrapli core Response.raise_for_status to include rpc error message(s). Args: N/A Returns: None Raises: ScrapliCommandFailure: if any elements are failed","title":"raise_for_status"},{"location":"api_docs/response/#record_response","text":"record_response(self, result: bytes) \u2011> None 1 2 3 4 5 6 7 8 9 10 Record channel_input results and elapsed time of channel input/reading output Args: result: bytes result of channel_input Returns: N/A # noqa: DAR202 Raises: N/A","title":"record_response"},{"location":"api_docs/response/#textfsm_parse_output","text":"textfsm_parse_output(self, to_dict: bool = True) \u2011> Union[Dict[str, Any], List[Any]] 1 2 3 4 5 6 7 8 9 10 Override scrapli Response `textfsm_parse_output` method; not applicable for netconf Args: to_dict: ignore, only here to ensure compliance with supertype method Returns: N/A # noqa: DAR202 Raises: NotImplementedError: always","title":"textfsm_parse_output"},{"location":"api_docs/channel/async_channel/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.channel.async_channel \u00b6 scrapli_netconf.channel.async_channel Expand source code \"\"\"scrapli_netconf.channel.async_channel\"\"\" from scrapli.channel import AsyncChannel from scrapli.channel.base_channel import BaseChannelArgs from scrapli.decorators import ChannelTimeout from scrapli.transport.base.async_transport import AsyncTransport from scrapli_netconf.channel.base_channel import BaseNetconfChannel, NetconfBaseChannelArgs from scrapli_netconf.constants import NetconfVersion class AsyncNetconfChannel(AsyncChannel, BaseNetconfChannel): def __init__( self, transport: AsyncTransport, base_channel_args: BaseChannelArgs, netconf_base_channel_args: NetconfBaseChannelArgs, ): super().__init__(transport=transport, base_channel_args=base_channel_args) self._netconf_base_channel_args = netconf_base_channel_args # always use `]]>]]>` as the initial prompt to match self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._server_echo = False self._capabilities_buf = b\"\" async def open_netconf(self) -> None: \"\"\" Open the netconf channel Args: N/A Returns: None Raises: N/A \"\"\" # open in scrapli core is where we open channel log (if applicable), do that self.open() raw_server_capabilities = await self._get_server_capabilities() self._process_capabilities_exchange(raw_server_capabilities=raw_server_capabilities) await self._send_client_capabilities() @ChannelTimeout( \"timed out determining if session is authenticated/getting server capabilities\", ) async def _get_server_capabilities(self) -> bytes: \"\"\" Read until all server capabilities have been sent by server Args: N/A Returns: bytes: raw bytes containing server capabilities Raises: N/A \"\"\" capabilities_buf = self._capabilities_buf # reset this to empty to avoid any confusion now that we are moving on self._capabilities_buf = b\"\" async with self._channel_lock(): while b\"]]>]]>\" not in capabilities_buf: capabilities_buf += await self.read() self.logger.debug(f\"received raw server capabilities: {repr(capabilities_buf)}\") return capabilities_buf @ChannelTimeout(\"timed out sending client capabilities\") async def _send_client_capabilities( self, ) -> None: \"\"\" Send client capabilities to the netconf server Args: N/A Returns: None Raises: N/A \"\"\" async with self._channel_lock(): _ = self._pre_send_client_capabilities( client_capabilities=self._netconf_base_channel_args.client_capabilities ) self.send_return() async def _read_until_input(self, channel_input: bytes) -> bytes: \"\"\" Async read until all input has been entered. Args: channel_input: string to write to channel Returns: bytes: output read from channel Raises: N/A \"\"\" output = b\"\" if self._server_echo is None or self._server_echo is False: # if server_echo is `None` we dont know if the server echoes yet, so just return nothing # if its False we know it doesnt echo and we can return empty byte string anyway return output if not channel_input: self.logger.info(f\"Read: {repr(output)}\") return output while True: output += await self.read() # if we have all the input *or* we see the closing rpc tag we know we are done here if channel_input in output or b\"rpc>\" in output: break self.logger.info(f\"Read: {repr(output)}\") return output async def send_input_netconf(self, channel_input: str) -> bytes: \"\"\" Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: N/A \"\"\" bytes_final_channel_input = channel_input.encode() buf: bytes buf, _ = await super().send_input( channel_input=channel_input, strip_prompt=False, eager=True ) if bytes_final_channel_input in buf: buf = buf.split(bytes_final_channel_input)[1] buf = await self._read_until_prompt(buf=buf) if self._server_echo is None: # At least per early drafts of the netconf over ssh rfcs the netconf servers MUST NOT # echo the input commands back to the client. In the case of \"normal\" scrapli netconf # with the system transport this happens anyway because we combine the stdin and stdout # fds into a single pty, however for other transports we have an actual stdin and # stdout fd to read/write. It seems that at the very least IOSXE with NETCONF 1.1 seems # to want to echo inputs back onto to the stdout for the channel. This is totally ok # and we can deal with it, we just need to *know* that it is happening, so while the # _server_echo attribute is still `None`, we can go ahead and see if the input we sent # is in the output we read off the channel. If it is *not* we know the server does *not* # echo and we can move on. If it *is* in the output, we know the server echoes, and we # also have one additional step in that we need to read \"until prompt\" again in order to # capture the reply to our rpc. # # See: https://tools.ietf.org/html/draft-ietf-netconf-ssh-02 (search for \"echo\") self.logger.debug(\"server echo is unset, determining if server echoes inputs now\") if bytes_final_channel_input in buf: self.logger.debug(\"server echoes inputs, setting _server_echo to 'true'\") self._server_echo = True # since echo is True and we only read until our input (because our inputs always end # with a \"prompt\" that we read until) we need to once again read until prompt, this # read will read all the way up through the *reply* to the prompt at end of the # reply message buf = await self._read_until_prompt(buf=b\"\") else: self.logger.debug(\"server does *not* echo inputs, setting _server_echo to 'false'\") self._server_echo = False if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: # netconf 1.1 with \"chunking\" style message format needs an extra return char here self.send_return() return buf Classes \u00b6 AsyncNetconfChannel \u00b6 1 2 3 4 5 6 7 8 9 10 11 BaseChannel Object -- provides convenience methods to both sync and async Channels Args: transport: initialized scrapli Transport/AsyncTransport object base_channel_args: BaseChannelArgs object Returns: None Raises: N/A Expand source code class AsyncNetconfChannel(AsyncChannel, BaseNetconfChannel): def __init__( self, transport: AsyncTransport, base_channel_args: BaseChannelArgs, netconf_base_channel_args: NetconfBaseChannelArgs, ): super().__init__(transport=transport, base_channel_args=base_channel_args) self._netconf_base_channel_args = netconf_base_channel_args # always use `]]>]]>` as the initial prompt to match self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._server_echo = False self._capabilities_buf = b\"\" async def open_netconf(self) -> None: \"\"\" Open the netconf channel Args: N/A Returns: None Raises: N/A \"\"\" # open in scrapli core is where we open channel log (if applicable), do that self.open() raw_server_capabilities = await self._get_server_capabilities() self._process_capabilities_exchange(raw_server_capabilities=raw_server_capabilities) await self._send_client_capabilities() @ChannelTimeout( \"timed out determining if session is authenticated/getting server capabilities\", ) async def _get_server_capabilities(self) -> bytes: \"\"\" Read until all server capabilities have been sent by server Args: N/A Returns: bytes: raw bytes containing server capabilities Raises: N/A \"\"\" capabilities_buf = self._capabilities_buf # reset this to empty to avoid any confusion now that we are moving on self._capabilities_buf = b\"\" async with self._channel_lock(): while b\"]]>]]>\" not in capabilities_buf: capabilities_buf += await self.read() self.logger.debug(f\"received raw server capabilities: {repr(capabilities_buf)}\") return capabilities_buf @ChannelTimeout(\"timed out sending client capabilities\") async def _send_client_capabilities( self, ) -> None: \"\"\" Send client capabilities to the netconf server Args: N/A Returns: None Raises: N/A \"\"\" async with self._channel_lock(): _ = self._pre_send_client_capabilities( client_capabilities=self._netconf_base_channel_args.client_capabilities ) self.send_return() async def _read_until_input(self, channel_input: bytes) -> bytes: \"\"\" Async read until all input has been entered. Args: channel_input: string to write to channel Returns: bytes: output read from channel Raises: N/A \"\"\" output = b\"\" if self._server_echo is None or self._server_echo is False: # if server_echo is `None` we dont know if the server echoes yet, so just return nothing # if its False we know it doesnt echo and we can return empty byte string anyway return output if not channel_input: self.logger.info(f\"Read: {repr(output)}\") return output while True: output += await self.read() # if we have all the input *or* we see the closing rpc tag we know we are done here if channel_input in output or b\"rpc>\" in output: break self.logger.info(f\"Read: {repr(output)}\") return output async def send_input_netconf(self, channel_input: str) -> bytes: \"\"\" Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: N/A \"\"\" bytes_final_channel_input = channel_input.encode() buf: bytes buf, _ = await super().send_input( channel_input=channel_input, strip_prompt=False, eager=True ) if bytes_final_channel_input in buf: buf = buf.split(bytes_final_channel_input)[1] buf = await self._read_until_prompt(buf=buf) if self._server_echo is None: # At least per early drafts of the netconf over ssh rfcs the netconf servers MUST NOT # echo the input commands back to the client. In the case of \"normal\" scrapli netconf # with the system transport this happens anyway because we combine the stdin and stdout # fds into a single pty, however for other transports we have an actual stdin and # stdout fd to read/write. It seems that at the very least IOSXE with NETCONF 1.1 seems # to want to echo inputs back onto to the stdout for the channel. This is totally ok # and we can deal with it, we just need to *know* that it is happening, so while the # _server_echo attribute is still `None`, we can go ahead and see if the input we sent # is in the output we read off the channel. If it is *not* we know the server does *not* # echo and we can move on. If it *is* in the output, we know the server echoes, and we # also have one additional step in that we need to read \"until prompt\" again in order to # capture the reply to our rpc. # # See: https://tools.ietf.org/html/draft-ietf-netconf-ssh-02 (search for \"echo\") self.logger.debug(\"server echo is unset, determining if server echoes inputs now\") if bytes_final_channel_input in buf: self.logger.debug(\"server echoes inputs, setting _server_echo to 'true'\") self._server_echo = True # since echo is True and we only read until our input (because our inputs always end # with a \"prompt\" that we read until) we need to once again read until prompt, this # read will read all the way up through the *reply* to the prompt at end of the # reply message buf = await self._read_until_prompt(buf=b\"\") else: self.logger.debug(\"server does *not* echo inputs, setting _server_echo to 'false'\") self._server_echo = False if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: # netconf 1.1 with \"chunking\" style message format needs an extra return char here self.send_return() return buf Ancestors (in MRO) \u00b6 scrapli.channel.async_channel.AsyncChannel scrapli_netconf.channel.base_channel.BaseNetconfChannel scrapli.channel.base_channel.BaseChannel Methods \u00b6 open_netconf \u00b6 open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 Open the netconf channel Args: N/A Returns: None Raises: N/A send_input_netconf \u00b6 send_input_netconf(self, channel_input: str) \u2011> bytes 1 2 3 4 5 6 7 8 9 10 Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: N/A","title":"Async Channel"},{"location":"api_docs/channel/async_channel/#module-scrapli_netconfchannelasync_channel","text":"scrapli_netconf.channel.async_channel Expand source code \"\"\"scrapli_netconf.channel.async_channel\"\"\" from scrapli.channel import AsyncChannel from scrapli.channel.base_channel import BaseChannelArgs from scrapli.decorators import ChannelTimeout from scrapli.transport.base.async_transport import AsyncTransport from scrapli_netconf.channel.base_channel import BaseNetconfChannel, NetconfBaseChannelArgs from scrapli_netconf.constants import NetconfVersion class AsyncNetconfChannel(AsyncChannel, BaseNetconfChannel): def __init__( self, transport: AsyncTransport, base_channel_args: BaseChannelArgs, netconf_base_channel_args: NetconfBaseChannelArgs, ): super().__init__(transport=transport, base_channel_args=base_channel_args) self._netconf_base_channel_args = netconf_base_channel_args # always use `]]>]]>` as the initial prompt to match self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._server_echo = False self._capabilities_buf = b\"\" async def open_netconf(self) -> None: \"\"\" Open the netconf channel Args: N/A Returns: None Raises: N/A \"\"\" # open in scrapli core is where we open channel log (if applicable), do that self.open() raw_server_capabilities = await self._get_server_capabilities() self._process_capabilities_exchange(raw_server_capabilities=raw_server_capabilities) await self._send_client_capabilities() @ChannelTimeout( \"timed out determining if session is authenticated/getting server capabilities\", ) async def _get_server_capabilities(self) -> bytes: \"\"\" Read until all server capabilities have been sent by server Args: N/A Returns: bytes: raw bytes containing server capabilities Raises: N/A \"\"\" capabilities_buf = self._capabilities_buf # reset this to empty to avoid any confusion now that we are moving on self._capabilities_buf = b\"\" async with self._channel_lock(): while b\"]]>]]>\" not in capabilities_buf: capabilities_buf += await self.read() self.logger.debug(f\"received raw server capabilities: {repr(capabilities_buf)}\") return capabilities_buf @ChannelTimeout(\"timed out sending client capabilities\") async def _send_client_capabilities( self, ) -> None: \"\"\" Send client capabilities to the netconf server Args: N/A Returns: None Raises: N/A \"\"\" async with self._channel_lock(): _ = self._pre_send_client_capabilities( client_capabilities=self._netconf_base_channel_args.client_capabilities ) self.send_return() async def _read_until_input(self, channel_input: bytes) -> bytes: \"\"\" Async read until all input has been entered. Args: channel_input: string to write to channel Returns: bytes: output read from channel Raises: N/A \"\"\" output = b\"\" if self._server_echo is None or self._server_echo is False: # if server_echo is `None` we dont know if the server echoes yet, so just return nothing # if its False we know it doesnt echo and we can return empty byte string anyway return output if not channel_input: self.logger.info(f\"Read: {repr(output)}\") return output while True: output += await self.read() # if we have all the input *or* we see the closing rpc tag we know we are done here if channel_input in output or b\"rpc>\" in output: break self.logger.info(f\"Read: {repr(output)}\") return output async def send_input_netconf(self, channel_input: str) -> bytes: \"\"\" Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: N/A \"\"\" bytes_final_channel_input = channel_input.encode() buf: bytes buf, _ = await super().send_input( channel_input=channel_input, strip_prompt=False, eager=True ) if bytes_final_channel_input in buf: buf = buf.split(bytes_final_channel_input)[1] buf = await self._read_until_prompt(buf=buf) if self._server_echo is None: # At least per early drafts of the netconf over ssh rfcs the netconf servers MUST NOT # echo the input commands back to the client. In the case of \"normal\" scrapli netconf # with the system transport this happens anyway because we combine the stdin and stdout # fds into a single pty, however for other transports we have an actual stdin and # stdout fd to read/write. It seems that at the very least IOSXE with NETCONF 1.1 seems # to want to echo inputs back onto to the stdout for the channel. This is totally ok # and we can deal with it, we just need to *know* that it is happening, so while the # _server_echo attribute is still `None`, we can go ahead and see if the input we sent # is in the output we read off the channel. If it is *not* we know the server does *not* # echo and we can move on. If it *is* in the output, we know the server echoes, and we # also have one additional step in that we need to read \"until prompt\" again in order to # capture the reply to our rpc. # # See: https://tools.ietf.org/html/draft-ietf-netconf-ssh-02 (search for \"echo\") self.logger.debug(\"server echo is unset, determining if server echoes inputs now\") if bytes_final_channel_input in buf: self.logger.debug(\"server echoes inputs, setting _server_echo to 'true'\") self._server_echo = True # since echo is True and we only read until our input (because our inputs always end # with a \"prompt\" that we read until) we need to once again read until prompt, this # read will read all the way up through the *reply* to the prompt at end of the # reply message buf = await self._read_until_prompt(buf=b\"\") else: self.logger.debug(\"server does *not* echo inputs, setting _server_echo to 'false'\") self._server_echo = False if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: # netconf 1.1 with \"chunking\" style message format needs an extra return char here self.send_return() return buf","title":"Module scrapli_netconf.channel.async_channel"},{"location":"api_docs/channel/async_channel/#classes","text":"","title":"Classes"},{"location":"api_docs/channel/async_channel/#asyncnetconfchannel","text":"1 2 3 4 5 6 7 8 9 10 11 BaseChannel Object -- provides convenience methods to both sync and async Channels Args: transport: initialized scrapli Transport/AsyncTransport object base_channel_args: BaseChannelArgs object Returns: None Raises: N/A Expand source code class AsyncNetconfChannel(AsyncChannel, BaseNetconfChannel): def __init__( self, transport: AsyncTransport, base_channel_args: BaseChannelArgs, netconf_base_channel_args: NetconfBaseChannelArgs, ): super().__init__(transport=transport, base_channel_args=base_channel_args) self._netconf_base_channel_args = netconf_base_channel_args # always use `]]>]]>` as the initial prompt to match self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._server_echo = False self._capabilities_buf = b\"\" async def open_netconf(self) -> None: \"\"\" Open the netconf channel Args: N/A Returns: None Raises: N/A \"\"\" # open in scrapli core is where we open channel log (if applicable), do that self.open() raw_server_capabilities = await self._get_server_capabilities() self._process_capabilities_exchange(raw_server_capabilities=raw_server_capabilities) await self._send_client_capabilities() @ChannelTimeout( \"timed out determining if session is authenticated/getting server capabilities\", ) async def _get_server_capabilities(self) -> bytes: \"\"\" Read until all server capabilities have been sent by server Args: N/A Returns: bytes: raw bytes containing server capabilities Raises: N/A \"\"\" capabilities_buf = self._capabilities_buf # reset this to empty to avoid any confusion now that we are moving on self._capabilities_buf = b\"\" async with self._channel_lock(): while b\"]]>]]>\" not in capabilities_buf: capabilities_buf += await self.read() self.logger.debug(f\"received raw server capabilities: {repr(capabilities_buf)}\") return capabilities_buf @ChannelTimeout(\"timed out sending client capabilities\") async def _send_client_capabilities( self, ) -> None: \"\"\" Send client capabilities to the netconf server Args: N/A Returns: None Raises: N/A \"\"\" async with self._channel_lock(): _ = self._pre_send_client_capabilities( client_capabilities=self._netconf_base_channel_args.client_capabilities ) self.send_return() async def _read_until_input(self, channel_input: bytes) -> bytes: \"\"\" Async read until all input has been entered. Args: channel_input: string to write to channel Returns: bytes: output read from channel Raises: N/A \"\"\" output = b\"\" if self._server_echo is None or self._server_echo is False: # if server_echo is `None` we dont know if the server echoes yet, so just return nothing # if its False we know it doesnt echo and we can return empty byte string anyway return output if not channel_input: self.logger.info(f\"Read: {repr(output)}\") return output while True: output += await self.read() # if we have all the input *or* we see the closing rpc tag we know we are done here if channel_input in output or b\"rpc>\" in output: break self.logger.info(f\"Read: {repr(output)}\") return output async def send_input_netconf(self, channel_input: str) -> bytes: \"\"\" Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: N/A \"\"\" bytes_final_channel_input = channel_input.encode() buf: bytes buf, _ = await super().send_input( channel_input=channel_input, strip_prompt=False, eager=True ) if bytes_final_channel_input in buf: buf = buf.split(bytes_final_channel_input)[1] buf = await self._read_until_prompt(buf=buf) if self._server_echo is None: # At least per early drafts of the netconf over ssh rfcs the netconf servers MUST NOT # echo the input commands back to the client. In the case of \"normal\" scrapli netconf # with the system transport this happens anyway because we combine the stdin and stdout # fds into a single pty, however for other transports we have an actual stdin and # stdout fd to read/write. It seems that at the very least IOSXE with NETCONF 1.1 seems # to want to echo inputs back onto to the stdout for the channel. This is totally ok # and we can deal with it, we just need to *know* that it is happening, so while the # _server_echo attribute is still `None`, we can go ahead and see if the input we sent # is in the output we read off the channel. If it is *not* we know the server does *not* # echo and we can move on. If it *is* in the output, we know the server echoes, and we # also have one additional step in that we need to read \"until prompt\" again in order to # capture the reply to our rpc. # # See: https://tools.ietf.org/html/draft-ietf-netconf-ssh-02 (search for \"echo\") self.logger.debug(\"server echo is unset, determining if server echoes inputs now\") if bytes_final_channel_input in buf: self.logger.debug(\"server echoes inputs, setting _server_echo to 'true'\") self._server_echo = True # since echo is True and we only read until our input (because our inputs always end # with a \"prompt\" that we read until) we need to once again read until prompt, this # read will read all the way up through the *reply* to the prompt at end of the # reply message buf = await self._read_until_prompt(buf=b\"\") else: self.logger.debug(\"server does *not* echo inputs, setting _server_echo to 'false'\") self._server_echo = False if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: # netconf 1.1 with \"chunking\" style message format needs an extra return char here self.send_return() return buf","title":"AsyncNetconfChannel"},{"location":"api_docs/channel/async_channel/#ancestors-in-mro","text":"scrapli.channel.async_channel.AsyncChannel scrapli_netconf.channel.base_channel.BaseNetconfChannel scrapli.channel.base_channel.BaseChannel","title":"Ancestors (in MRO)"},{"location":"api_docs/channel/async_channel/#methods","text":"","title":"Methods"},{"location":"api_docs/channel/async_channel/#open_netconf","text":"open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 Open the netconf channel Args: N/A Returns: None Raises: N/A","title":"open_netconf"},{"location":"api_docs/channel/async_channel/#send_input_netconf","text":"send_input_netconf(self, channel_input: str) \u2011> bytes 1 2 3 4 5 6 7 8 9 10 Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: N/A","title":"send_input_netconf"},{"location":"api_docs/channel/base_channel/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.channel.base_channel \u00b6 scrapli_netconf.channel.base_channel Expand source code \"\"\"scrapli_netconf.channel.base_channel\"\"\" import re from dataclasses import dataclass from typing import List, Optional from lxml import etree from scrapli.channel.base_channel import BaseChannel from scrapli_netconf.constants import NetconfClientCapabilities, NetconfVersion, XmlParserVersion from scrapli_netconf.exceptions import CapabilityNotSupported, CouldNotExchangeCapabilities @dataclass() class NetconfBaseChannelArgs: netconf_version: NetconfVersion server_capabilities: Optional[List[str]] = None client_capabilities: NetconfClientCapabilities = NetconfClientCapabilities.UNKNOWN xml_parser: XmlParserVersion = XmlParserVersion.COMPRESSED_PARSER class BaseNetconfChannel(BaseChannel): _netconf_base_channel_args: NetconfBaseChannelArgs def _process_capabilities_exchange(self, raw_server_capabilities: bytes) -> None: \"\"\" Process received capabilities; return client capabilities Args: raw_server_capabilities: raw bytes containing server capabilities Returns: None Raises: CapabilityNotSupported: if user has provided a preferred netconf version but it is not available in servers offered capabilities \"\"\" server_capabilities = self._parse_server_capabilities( raw_server_capabilities=raw_server_capabilities ) self._netconf_base_channel_args.server_capabilities = server_capabilities if \"urn:ietf:params:netconf:base:1.1\" in server_capabilities: final_channel_version = NetconfVersion.VERSION_1_1 else: final_channel_version = NetconfVersion.VERSION_1_0 if self._netconf_base_channel_args.netconf_version != NetconfVersion.UNKNOWN: if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_0: if \"urn:ietf:params:netconf:base:1.0\" not in server_capabilities: raise CapabilityNotSupported( \"user requested netconf version 1.0 but capability not offered\" ) final_channel_version = NetconfVersion.VERSION_1_0 elif self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: if \"urn:ietf:params:netconf:base:1.1\" not in server_capabilities: raise CapabilityNotSupported( \"user requested netconf version 1.1 but capability not offered\" ) final_channel_version = NetconfVersion.VERSION_1_1 if final_channel_version == NetconfVersion.VERSION_1_0: self._netconf_base_channel_args.netconf_version = NetconfVersion.VERSION_1_0 self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._netconf_base_channel_args.client_capabilities = ( NetconfClientCapabilities.CAPABILITIES_1_0 ) else: self._netconf_base_channel_args.netconf_version = NetconfVersion.VERSION_1_1 self._base_channel_args.comms_prompt_pattern = r\"^##$\" self._netconf_base_channel_args.client_capabilities = ( NetconfClientCapabilities.CAPABILITIES_1_1 ) def _parse_server_capabilities(self, raw_server_capabilities: bytes) -> List[str]: \"\"\" Parse netconf server capabilities Args: raw_server_capabilities: raw bytes containing server capabilities Returns: N/A # noqa: DAR202 Raises: CouldNotExchangeCapabilities: if server capabilities cannot be parsed \"\"\" server_capabilities = [] # matches hello with or without namespace filtered_raw_server_capabilities = re.search( pattern=rb\"( < (\\w+\\:){0,1}hello.* < \\/(\\w+\\:){0,1}hello>)\", string=raw_server_capabilities, flags=re.I | re.S, ) if filtered_raw_server_capabilities is None: msg = \"failed to parse server capabilities\" raise CouldNotExchangeCapabilities(msg) server_capabilities_xml = etree.fromstring(filtered_raw_server_capabilities.groups()[0]) for elem in server_capabilities_xml.iter(): if \"capability\" not in elem.tag: continue server_capabilities.append(elem.text.strip()) self.logger.info(f\"server capabilities received and parsed: {server_capabilities}\") return server_capabilities def _process_output(self, buf: bytes, strip_prompt: bool) -> bytes: \"\"\" Override scrapli _process_output as this is unnecessary for scrapli_netconf Args: buf: bytes output from the device strip_prompt: ignored in this base class; for LSP reasons for subclasses Returns: bytes: output of joined output lines optionally with prompt removed Raises: N/A \"\"\" _ = strip_prompt return buf def _pre_send_client_capabilities( self, client_capabilities: NetconfClientCapabilities ) -> bytes: \"\"\" Handle pre \"_send_client_capabilities\" tasks for consistency between sync/async versions Args: client_capabilities: string of client netconf capabilities to send to server Returns: bytes: bytes of client capabilities to send to the channel Raises: N/A \"\"\" self.logger.info(\"sending client capabilities\") bytes_client_capabilities: bytes = client_capabilities.value.encode().strip() self.logger.debug(f\"attempting to send capabilities: {client_capabilities}\") self.write(client_capabilities.value) return bytes_client_capabilities Classes \u00b6 BaseNetconfChannel \u00b6 1 2 3 4 5 6 7 8 9 10 11 BaseChannel Object -- provides convenience methods to both sync and async Channels Args: transport: initialized scrapli Transport/AsyncTransport object base_channel_args: BaseChannelArgs object Returns: None Raises: N/A Expand source code class BaseNetconfChannel(BaseChannel): _netconf_base_channel_args: NetconfBaseChannelArgs def _process_capabilities_exchange(self, raw_server_capabilities: bytes) -> None: \"\"\" Process received capabilities; return client capabilities Args: raw_server_capabilities: raw bytes containing server capabilities Returns: None Raises: CapabilityNotSupported: if user has provided a preferred netconf version but it is not available in servers offered capabilities \"\"\" server_capabilities = self._parse_server_capabilities( raw_server_capabilities=raw_server_capabilities ) self._netconf_base_channel_args.server_capabilities = server_capabilities if \"urn:ietf:params:netconf:base:1.1\" in server_capabilities: final_channel_version = NetconfVersion.VERSION_1_1 else: final_channel_version = NetconfVersion.VERSION_1_0 if self._netconf_base_channel_args.netconf_version != NetconfVersion.UNKNOWN: if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_0: if \"urn:ietf:params:netconf:base:1.0\" not in server_capabilities: raise CapabilityNotSupported( \"user requested netconf version 1.0 but capability not offered\" ) final_channel_version = NetconfVersion.VERSION_1_0 elif self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: if \"urn:ietf:params:netconf:base:1.1\" not in server_capabilities: raise CapabilityNotSupported( \"user requested netconf version 1.1 but capability not offered\" ) final_channel_version = NetconfVersion.VERSION_1_1 if final_channel_version == NetconfVersion.VERSION_1_0: self._netconf_base_channel_args.netconf_version = NetconfVersion.VERSION_1_0 self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._netconf_base_channel_args.client_capabilities = ( NetconfClientCapabilities.CAPABILITIES_1_0 ) else: self._netconf_base_channel_args.netconf_version = NetconfVersion.VERSION_1_1 self._base_channel_args.comms_prompt_pattern = r\"^##$\" self._netconf_base_channel_args.client_capabilities = ( NetconfClientCapabilities.CAPABILITIES_1_1 ) def _parse_server_capabilities(self, raw_server_capabilities: bytes) -> List[str]: \"\"\" Parse netconf server capabilities Args: raw_server_capabilities: raw bytes containing server capabilities Returns: N/A # noqa: DAR202 Raises: CouldNotExchangeCapabilities: if server capabilities cannot be parsed \"\"\" server_capabilities = [] # matches hello with or without namespace filtered_raw_server_capabilities = re.search( pattern=rb\"( < (\\w+\\:){0,1}hello.* < \\/(\\w+\\:){0,1}hello>)\", string=raw_server_capabilities, flags=re.I | re.S, ) if filtered_raw_server_capabilities is None: msg = \"failed to parse server capabilities\" raise CouldNotExchangeCapabilities(msg) server_capabilities_xml = etree.fromstring(filtered_raw_server_capabilities.groups()[0]) for elem in server_capabilities_xml.iter(): if \"capability\" not in elem.tag: continue server_capabilities.append(elem.text.strip()) self.logger.info(f\"server capabilities received and parsed: {server_capabilities}\") return server_capabilities def _process_output(self, buf: bytes, strip_prompt: bool) -> bytes: \"\"\" Override scrapli _process_output as this is unnecessary for scrapli_netconf Args: buf: bytes output from the device strip_prompt: ignored in this base class; for LSP reasons for subclasses Returns: bytes: output of joined output lines optionally with prompt removed Raises: N/A \"\"\" _ = strip_prompt return buf def _pre_send_client_capabilities( self, client_capabilities: NetconfClientCapabilities ) -> bytes: \"\"\" Handle pre \"_send_client_capabilities\" tasks for consistency between sync/async versions Args: client_capabilities: string of client netconf capabilities to send to server Returns: bytes: bytes of client capabilities to send to the channel Raises: N/A \"\"\" self.logger.info(\"sending client capabilities\") bytes_client_capabilities: bytes = client_capabilities.value.encode().strip() self.logger.debug(f\"attempting to send capabilities: {client_capabilities}\") self.write(client_capabilities.value) return bytes_client_capabilities Ancestors (in MRO) \u00b6 scrapli.channel.base_channel.BaseChannel Descendants \u00b6 scrapli_netconf.channel.async_channel.AsyncNetconfChannel scrapli_netconf.channel.sync_channel.NetconfChannel NetconfBaseChannelArgs \u00b6 1 NetconfBaseChannelArgs(netconf_version: scrapli_netconf.constants.NetconfVersion, server_capabilities: Optional[List[str]] = None, client_capabilities: scrapli_netconf.constants.NetconfClientCapabilities = <NetconfClientCapabilities.UNKNOWN: 'unknown'>, xml_parser: scrapli_netconf.constants.XmlParserVersion = <XmlParserVersion.COMPRESSED_PARSER: 'flat'>) Expand source code @dataclass() class NetconfBaseChannelArgs: netconf_version: NetconfVersion server_capabilities: Optional[List[str]] = None client_capabilities: NetconfClientCapabilities = NetconfClientCapabilities.UNKNOWN xml_parser: XmlParserVersion = XmlParserVersion.COMPRESSED_PARSER Class variables \u00b6 client_capabilities: scrapli_netconf.constants.NetconfClientCapabilities netconf_version: scrapli_netconf.constants.NetconfVersion server_capabilities: Optional[List[str]] xml_parser: scrapli_netconf.constants.XmlParserVersion","title":"Base Channel"},{"location":"api_docs/channel/base_channel/#module-scrapli_netconfchannelbase_channel","text":"scrapli_netconf.channel.base_channel Expand source code \"\"\"scrapli_netconf.channel.base_channel\"\"\" import re from dataclasses import dataclass from typing import List, Optional from lxml import etree from scrapli.channel.base_channel import BaseChannel from scrapli_netconf.constants import NetconfClientCapabilities, NetconfVersion, XmlParserVersion from scrapli_netconf.exceptions import CapabilityNotSupported, CouldNotExchangeCapabilities @dataclass() class NetconfBaseChannelArgs: netconf_version: NetconfVersion server_capabilities: Optional[List[str]] = None client_capabilities: NetconfClientCapabilities = NetconfClientCapabilities.UNKNOWN xml_parser: XmlParserVersion = XmlParserVersion.COMPRESSED_PARSER class BaseNetconfChannel(BaseChannel): _netconf_base_channel_args: NetconfBaseChannelArgs def _process_capabilities_exchange(self, raw_server_capabilities: bytes) -> None: \"\"\" Process received capabilities; return client capabilities Args: raw_server_capabilities: raw bytes containing server capabilities Returns: None Raises: CapabilityNotSupported: if user has provided a preferred netconf version but it is not available in servers offered capabilities \"\"\" server_capabilities = self._parse_server_capabilities( raw_server_capabilities=raw_server_capabilities ) self._netconf_base_channel_args.server_capabilities = server_capabilities if \"urn:ietf:params:netconf:base:1.1\" in server_capabilities: final_channel_version = NetconfVersion.VERSION_1_1 else: final_channel_version = NetconfVersion.VERSION_1_0 if self._netconf_base_channel_args.netconf_version != NetconfVersion.UNKNOWN: if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_0: if \"urn:ietf:params:netconf:base:1.0\" not in server_capabilities: raise CapabilityNotSupported( \"user requested netconf version 1.0 but capability not offered\" ) final_channel_version = NetconfVersion.VERSION_1_0 elif self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: if \"urn:ietf:params:netconf:base:1.1\" not in server_capabilities: raise CapabilityNotSupported( \"user requested netconf version 1.1 but capability not offered\" ) final_channel_version = NetconfVersion.VERSION_1_1 if final_channel_version == NetconfVersion.VERSION_1_0: self._netconf_base_channel_args.netconf_version = NetconfVersion.VERSION_1_0 self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._netconf_base_channel_args.client_capabilities = ( NetconfClientCapabilities.CAPABILITIES_1_0 ) else: self._netconf_base_channel_args.netconf_version = NetconfVersion.VERSION_1_1 self._base_channel_args.comms_prompt_pattern = r\"^##$\" self._netconf_base_channel_args.client_capabilities = ( NetconfClientCapabilities.CAPABILITIES_1_1 ) def _parse_server_capabilities(self, raw_server_capabilities: bytes) -> List[str]: \"\"\" Parse netconf server capabilities Args: raw_server_capabilities: raw bytes containing server capabilities Returns: N/A # noqa: DAR202 Raises: CouldNotExchangeCapabilities: if server capabilities cannot be parsed \"\"\" server_capabilities = [] # matches hello with or without namespace filtered_raw_server_capabilities = re.search( pattern=rb\"( < (\\w+\\:){0,1}hello.* < \\/(\\w+\\:){0,1}hello>)\", string=raw_server_capabilities, flags=re.I | re.S, ) if filtered_raw_server_capabilities is None: msg = \"failed to parse server capabilities\" raise CouldNotExchangeCapabilities(msg) server_capabilities_xml = etree.fromstring(filtered_raw_server_capabilities.groups()[0]) for elem in server_capabilities_xml.iter(): if \"capability\" not in elem.tag: continue server_capabilities.append(elem.text.strip()) self.logger.info(f\"server capabilities received and parsed: {server_capabilities}\") return server_capabilities def _process_output(self, buf: bytes, strip_prompt: bool) -> bytes: \"\"\" Override scrapli _process_output as this is unnecessary for scrapli_netconf Args: buf: bytes output from the device strip_prompt: ignored in this base class; for LSP reasons for subclasses Returns: bytes: output of joined output lines optionally with prompt removed Raises: N/A \"\"\" _ = strip_prompt return buf def _pre_send_client_capabilities( self, client_capabilities: NetconfClientCapabilities ) -> bytes: \"\"\" Handle pre \"_send_client_capabilities\" tasks for consistency between sync/async versions Args: client_capabilities: string of client netconf capabilities to send to server Returns: bytes: bytes of client capabilities to send to the channel Raises: N/A \"\"\" self.logger.info(\"sending client capabilities\") bytes_client_capabilities: bytes = client_capabilities.value.encode().strip() self.logger.debug(f\"attempting to send capabilities: {client_capabilities}\") self.write(client_capabilities.value) return bytes_client_capabilities","title":"Module scrapli_netconf.channel.base_channel"},{"location":"api_docs/channel/base_channel/#classes","text":"","title":"Classes"},{"location":"api_docs/channel/base_channel/#basenetconfchannel","text":"1 2 3 4 5 6 7 8 9 10 11 BaseChannel Object -- provides convenience methods to both sync and async Channels Args: transport: initialized scrapli Transport/AsyncTransport object base_channel_args: BaseChannelArgs object Returns: None Raises: N/A Expand source code class BaseNetconfChannel(BaseChannel): _netconf_base_channel_args: NetconfBaseChannelArgs def _process_capabilities_exchange(self, raw_server_capabilities: bytes) -> None: \"\"\" Process received capabilities; return client capabilities Args: raw_server_capabilities: raw bytes containing server capabilities Returns: None Raises: CapabilityNotSupported: if user has provided a preferred netconf version but it is not available in servers offered capabilities \"\"\" server_capabilities = self._parse_server_capabilities( raw_server_capabilities=raw_server_capabilities ) self._netconf_base_channel_args.server_capabilities = server_capabilities if \"urn:ietf:params:netconf:base:1.1\" in server_capabilities: final_channel_version = NetconfVersion.VERSION_1_1 else: final_channel_version = NetconfVersion.VERSION_1_0 if self._netconf_base_channel_args.netconf_version != NetconfVersion.UNKNOWN: if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_0: if \"urn:ietf:params:netconf:base:1.0\" not in server_capabilities: raise CapabilityNotSupported( \"user requested netconf version 1.0 but capability not offered\" ) final_channel_version = NetconfVersion.VERSION_1_0 elif self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: if \"urn:ietf:params:netconf:base:1.1\" not in server_capabilities: raise CapabilityNotSupported( \"user requested netconf version 1.1 but capability not offered\" ) final_channel_version = NetconfVersion.VERSION_1_1 if final_channel_version == NetconfVersion.VERSION_1_0: self._netconf_base_channel_args.netconf_version = NetconfVersion.VERSION_1_0 self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._netconf_base_channel_args.client_capabilities = ( NetconfClientCapabilities.CAPABILITIES_1_0 ) else: self._netconf_base_channel_args.netconf_version = NetconfVersion.VERSION_1_1 self._base_channel_args.comms_prompt_pattern = r\"^##$\" self._netconf_base_channel_args.client_capabilities = ( NetconfClientCapabilities.CAPABILITIES_1_1 ) def _parse_server_capabilities(self, raw_server_capabilities: bytes) -> List[str]: \"\"\" Parse netconf server capabilities Args: raw_server_capabilities: raw bytes containing server capabilities Returns: N/A # noqa: DAR202 Raises: CouldNotExchangeCapabilities: if server capabilities cannot be parsed \"\"\" server_capabilities = [] # matches hello with or without namespace filtered_raw_server_capabilities = re.search( pattern=rb\"( < (\\w+\\:){0,1}hello.* < \\/(\\w+\\:){0,1}hello>)\", string=raw_server_capabilities, flags=re.I | re.S, ) if filtered_raw_server_capabilities is None: msg = \"failed to parse server capabilities\" raise CouldNotExchangeCapabilities(msg) server_capabilities_xml = etree.fromstring(filtered_raw_server_capabilities.groups()[0]) for elem in server_capabilities_xml.iter(): if \"capability\" not in elem.tag: continue server_capabilities.append(elem.text.strip()) self.logger.info(f\"server capabilities received and parsed: {server_capabilities}\") return server_capabilities def _process_output(self, buf: bytes, strip_prompt: bool) -> bytes: \"\"\" Override scrapli _process_output as this is unnecessary for scrapli_netconf Args: buf: bytes output from the device strip_prompt: ignored in this base class; for LSP reasons for subclasses Returns: bytes: output of joined output lines optionally with prompt removed Raises: N/A \"\"\" _ = strip_prompt return buf def _pre_send_client_capabilities( self, client_capabilities: NetconfClientCapabilities ) -> bytes: \"\"\" Handle pre \"_send_client_capabilities\" tasks for consistency between sync/async versions Args: client_capabilities: string of client netconf capabilities to send to server Returns: bytes: bytes of client capabilities to send to the channel Raises: N/A \"\"\" self.logger.info(\"sending client capabilities\") bytes_client_capabilities: bytes = client_capabilities.value.encode().strip() self.logger.debug(f\"attempting to send capabilities: {client_capabilities}\") self.write(client_capabilities.value) return bytes_client_capabilities","title":"BaseNetconfChannel"},{"location":"api_docs/channel/base_channel/#ancestors-in-mro","text":"scrapli.channel.base_channel.BaseChannel","title":"Ancestors (in MRO)"},{"location":"api_docs/channel/base_channel/#descendants","text":"scrapli_netconf.channel.async_channel.AsyncNetconfChannel scrapli_netconf.channel.sync_channel.NetconfChannel","title":"Descendants"},{"location":"api_docs/channel/base_channel/#netconfbasechannelargs","text":"1 NetconfBaseChannelArgs(netconf_version: scrapli_netconf.constants.NetconfVersion, server_capabilities: Optional[List[str]] = None, client_capabilities: scrapli_netconf.constants.NetconfClientCapabilities = <NetconfClientCapabilities.UNKNOWN: 'unknown'>, xml_parser: scrapli_netconf.constants.XmlParserVersion = <XmlParserVersion.COMPRESSED_PARSER: 'flat'>) Expand source code @dataclass() class NetconfBaseChannelArgs: netconf_version: NetconfVersion server_capabilities: Optional[List[str]] = None client_capabilities: NetconfClientCapabilities = NetconfClientCapabilities.UNKNOWN xml_parser: XmlParserVersion = XmlParserVersion.COMPRESSED_PARSER","title":"NetconfBaseChannelArgs"},{"location":"api_docs/channel/base_channel/#class-variables","text":"client_capabilities: scrapli_netconf.constants.NetconfClientCapabilities netconf_version: scrapli_netconf.constants.NetconfVersion server_capabilities: Optional[List[str]] xml_parser: scrapli_netconf.constants.XmlParserVersion","title":"Class variables"},{"location":"api_docs/channel/sync_channel/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.channel.sync_channel \u00b6 scrapli_netconf.channel.sync_channel Expand source code \"\"\"scrapli_netconf.channel.sync_channel\"\"\" import re from typing import Optional from scrapli.channel import Channel from scrapli.channel.base_channel import BaseChannelArgs from scrapli.decorators import ChannelTimeout from scrapli.exceptions import ScrapliAuthenticationFailed, ScrapliTimeout from scrapli.transport.base import Transport from scrapli_netconf.channel.base_channel import BaseNetconfChannel, NetconfBaseChannelArgs from scrapli_netconf.constants import NetconfVersion HELLO_MATCH = re.compile(pattern=rb\" < (\\w+\\:){0,1}hello\", flags=re.I) class NetconfChannel(Channel, BaseNetconfChannel): def __init__( self, transport: Transport, base_channel_args: BaseChannelArgs, netconf_base_channel_args: NetconfBaseChannelArgs, ): super().__init__(transport=transport, base_channel_args=base_channel_args) self._netconf_base_channel_args = netconf_base_channel_args # always use `]]>]]>` as the initial prompt to match self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._server_echo: Optional[bool] = None self._capabilities_buf = b\"\" def open_netconf(self) -> None: \"\"\" Open the netconf channel Args: N/A Returns: None Raises: N/A \"\"\" # open in scrapli core is where we open channel log (if applicable), do that self.open() raw_server_capabilities = self._get_server_capabilities() self._process_capabilities_exchange(raw_server_capabilities=raw_server_capabilities) self._send_client_capabilities() @staticmethod def _authenticate_check_hello(buf: bytes) -> bool: \"\"\" Check if \"hello\" message is in output Args: buf: bytes output from the channel Returns: bool: true if hello message is seen, otherwise false Raises: N/A \"\"\" hello_match = re.search(pattern=HELLO_MATCH, string=buf) if hello_match: return True return False @ChannelTimeout(\"timed out during in channel netconf authentication\") def channel_authenticate_netconf( self, auth_password: str, auth_private_key_passphrase: str ) -> None: \"\"\" Handle SSH Authentication for transports that only operate \"in the channel\" (i.e. system) Args: auth_password: password to authenticate with auth_private_key_passphrase: passphrase for ssh key if necessary Returns: None Raises: ScrapliAuthenticationFailed: if password prompt seen more than twice ScrapliAuthenticationFailed: if passphrase prompt seen more than twice \"\"\" self.logger.debug(\"attempting in channel netconf authentication\") password_count = 0 passphrase_count = 0 authenticate_buf = b\"\" with self._channel_lock(): while True: buf = self.read() authenticate_buf += buf.lower() self._capabilities_buf += buf self._ssh_message_handler(output=authenticate_buf) if b\"password\" in authenticate_buf: # clear the authentication buffer so we don't re-read the password prompt authenticate_buf = b\"\" password_count += 1 if password_count > 2: msg = \"password prompt seen more than once, assuming auth failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) self.write(channel_input=auth_password, redacted=True) self.send_return() if b\"enter passphrase for key\" in authenticate_buf: # clear the authentication buffer so we don't re-read the passphrase prompt authenticate_buf = b\"\" passphrase_count += 1 if passphrase_count > 2: msg = \"passphrase prompt seen more than once, assuming auth failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) self.write(channel_input=auth_private_key_passphrase, redacted=True) self.send_return() if self._authenticate_check_hello(buf=authenticate_buf): self.logger.info( \"found start of server capabilities, authentication successful\" ) return @ChannelTimeout( \"timed out determining if session is authenticated/getting server capabilities\", ) def _get_server_capabilities(self) -> bytes: \"\"\" Read until all server capabilities have been sent by server Args: N/A Returns: bytes: raw bytes containing server capabilities Raises: N/A \"\"\" capabilities_buf = self._capabilities_buf # reset this to empty to avoid any confusion now that we are moving on self._capabilities_buf = b\"\" with self._channel_lock(): while b\"]]>]]>\" not in capabilities_buf: capabilities_buf += self.read() self.logger.debug(f\"received raw server capabilities: {repr(capabilities_buf)}\") return capabilities_buf @ChannelTimeout(\"timed out sending client capabilities\") def _send_client_capabilities( self, ) -> None: \"\"\" Send client capabilities to the netconf server Args: N/A Returns: None Raises: N/A \"\"\" with self._channel_lock(): bytes_client_capabilities = self._pre_send_client_capabilities( client_capabilities=self._netconf_base_channel_args.client_capabilities ) self._read_until_input(channel_input=bytes_client_capabilities) self.send_return() def _read_until_input(self, channel_input: bytes) -> bytes: \"\"\" Async read until all input has been entered. Args: channel_input: string to write to channel Returns: bytes: output read from channel Raises: N/A \"\"\" output = b\"\" if self._server_echo is None or self._server_echo is False: # if server_echo is `None` we dont know if the server echoes yet, so just return nothing # if its False we know it doesnt echo and we can return empty byte string anyway return output if not channel_input: self.logger.info(f\"Read: {repr(output)}\") return output while True: output += self.read() # if we have all the input *or* we see the closing rpc tag we know we are done here if channel_input in output or b\"rpc>\" in output: break self.logger.info(f\"Read: {repr(output)}\") return output def send_input_netconf(self, channel_input: str) -> bytes: \"\"\" Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: ScrapliTimeout: re-raises channel timeouts with additional message if channel input may be big enough to require setting `use_compressed_parser` to false -- note that this has only been seen as an issue with NXOS so far. \"\"\" bytes_final_channel_input = channel_input.encode() buf: bytes buf, _ = super().send_input(channel_input=channel_input, strip_prompt=False, eager=True) if bytes_final_channel_input in buf: # if we got the input AND the rpc-reply we can strip out our inputs so we just have the # reply remaining buf = buf.split(bytes_final_channel_input)[1] try: buf = self._read_until_prompt(buf=buf) except ScrapliTimeout as exc: if len(channel_input) >= 4096: msg = ( \"timed out finding prompt after sending input, input is greater than 4096 \" \"chars, try setting 'use_compressed_parser' to False\" ) self.logger.info(msg) raise ScrapliTimeout(msg) from exc raise ScrapliTimeout from exc if self._server_echo is None: # At least per early drafts of the netconf over ssh rfcs the netconf servers MUST NOT # echo the input commands back to the client. In the case of \"normal\" scrapli netconf # with the system transport this happens anyway because we combine the stdin and stdout # fds into a single pty, however for other transports we have an actual stdin and # stdout fd to read/write. It seems that at the very least IOSXE with NETCONF 1.1 seems # to want to echo inputs back onto to the stdout for the channel. This is totally ok # and we can deal with it, we just need to *know* that it is happening, so while the # _server_echo attribute is still `None`, we can go ahead and see if the input we sent # is in the output we read off the channel. If it is *not* we know the server does *not* # echo and we can move on. If it *is* in the output, we know the server echoes, and we # also have one additional step in that we need to read \"until prompt\" again in order to # capture the reply to our rpc. # # See: https://tools.ietf.org/html/draft-ietf-netconf-ssh-02 (search for \"echo\") self.logger.debug(\"server echo is unset, determining if server echoes inputs now\") if bytes_final_channel_input in buf: self.logger.debug(\"server echoes inputs, setting _server_echo to 'true'\") self._server_echo = True # since echo is True and we only read until our input (because our inputs always end # with a \"prompt\" that we read until) we need to once again read until prompt, this # read will read all the way up through the *reply* to the prompt at end of the # reply message buf = self._read_until_prompt(buf=b\"\") else: self.logger.debug(\"server does *not* echo inputs, setting _server_echo to 'false'\") self._server_echo = False if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: # netconf 1.1 with \"chunking\" style message format needs an extra return char here self.send_return() return buf Classes \u00b6 NetconfChannel \u00b6 1 2 3 4 5 6 7 8 9 10 11 BaseChannel Object -- provides convenience methods to both sync and async Channels Args: transport: initialized scrapli Transport/AsyncTransport object base_channel_args: BaseChannelArgs object Returns: None Raises: N/A Expand source code class NetconfChannel(Channel, BaseNetconfChannel): def __init__( self, transport: Transport, base_channel_args: BaseChannelArgs, netconf_base_channel_args: NetconfBaseChannelArgs, ): super().__init__(transport=transport, base_channel_args=base_channel_args) self._netconf_base_channel_args = netconf_base_channel_args # always use `]]>]]>` as the initial prompt to match self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._server_echo: Optional[bool] = None self._capabilities_buf = b\"\" def open_netconf(self) -> None: \"\"\" Open the netconf channel Args: N/A Returns: None Raises: N/A \"\"\" # open in scrapli core is where we open channel log (if applicable), do that self.open() raw_server_capabilities = self._get_server_capabilities() self._process_capabilities_exchange(raw_server_capabilities=raw_server_capabilities) self._send_client_capabilities() @staticmethod def _authenticate_check_hello(buf: bytes) -> bool: \"\"\" Check if \"hello\" message is in output Args: buf: bytes output from the channel Returns: bool: true if hello message is seen, otherwise false Raises: N/A \"\"\" hello_match = re.search(pattern=HELLO_MATCH, string=buf) if hello_match: return True return False @ChannelTimeout(\"timed out during in channel netconf authentication\") def channel_authenticate_netconf( self, auth_password: str, auth_private_key_passphrase: str ) -> None: \"\"\" Handle SSH Authentication for transports that only operate \"in the channel\" (i.e. system) Args: auth_password: password to authenticate with auth_private_key_passphrase: passphrase for ssh key if necessary Returns: None Raises: ScrapliAuthenticationFailed: if password prompt seen more than twice ScrapliAuthenticationFailed: if passphrase prompt seen more than twice \"\"\" self.logger.debug(\"attempting in channel netconf authentication\") password_count = 0 passphrase_count = 0 authenticate_buf = b\"\" with self._channel_lock(): while True: buf = self.read() authenticate_buf += buf.lower() self._capabilities_buf += buf self._ssh_message_handler(output=authenticate_buf) if b\"password\" in authenticate_buf: # clear the authentication buffer so we don't re-read the password prompt authenticate_buf = b\"\" password_count += 1 if password_count > 2: msg = \"password prompt seen more than once, assuming auth failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) self.write(channel_input=auth_password, redacted=True) self.send_return() if b\"enter passphrase for key\" in authenticate_buf: # clear the authentication buffer so we don't re-read the passphrase prompt authenticate_buf = b\"\" passphrase_count += 1 if passphrase_count > 2: msg = \"passphrase prompt seen more than once, assuming auth failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) self.write(channel_input=auth_private_key_passphrase, redacted=True) self.send_return() if self._authenticate_check_hello(buf=authenticate_buf): self.logger.info( \"found start of server capabilities, authentication successful\" ) return @ChannelTimeout( \"timed out determining if session is authenticated/getting server capabilities\", ) def _get_server_capabilities(self) -> bytes: \"\"\" Read until all server capabilities have been sent by server Args: N/A Returns: bytes: raw bytes containing server capabilities Raises: N/A \"\"\" capabilities_buf = self._capabilities_buf # reset this to empty to avoid any confusion now that we are moving on self._capabilities_buf = b\"\" with self._channel_lock(): while b\"]]>]]>\" not in capabilities_buf: capabilities_buf += self.read() self.logger.debug(f\"received raw server capabilities: {repr(capabilities_buf)}\") return capabilities_buf @ChannelTimeout(\"timed out sending client capabilities\") def _send_client_capabilities( self, ) -> None: \"\"\" Send client capabilities to the netconf server Args: N/A Returns: None Raises: N/A \"\"\" with self._channel_lock(): bytes_client_capabilities = self._pre_send_client_capabilities( client_capabilities=self._netconf_base_channel_args.client_capabilities ) self._read_until_input(channel_input=bytes_client_capabilities) self.send_return() def _read_until_input(self, channel_input: bytes) -> bytes: \"\"\" Async read until all input has been entered. Args: channel_input: string to write to channel Returns: bytes: output read from channel Raises: N/A \"\"\" output = b\"\" if self._server_echo is None or self._server_echo is False: # if server_echo is `None` we dont know if the server echoes yet, so just return nothing # if its False we know it doesnt echo and we can return empty byte string anyway return output if not channel_input: self.logger.info(f\"Read: {repr(output)}\") return output while True: output += self.read() # if we have all the input *or* we see the closing rpc tag we know we are done here if channel_input in output or b\"rpc>\" in output: break self.logger.info(f\"Read: {repr(output)}\") return output def send_input_netconf(self, channel_input: str) -> bytes: \"\"\" Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: ScrapliTimeout: re-raises channel timeouts with additional message if channel input may be big enough to require setting `use_compressed_parser` to false -- note that this has only been seen as an issue with NXOS so far. \"\"\" bytes_final_channel_input = channel_input.encode() buf: bytes buf, _ = super().send_input(channel_input=channel_input, strip_prompt=False, eager=True) if bytes_final_channel_input in buf: # if we got the input AND the rpc-reply we can strip out our inputs so we just have the # reply remaining buf = buf.split(bytes_final_channel_input)[1] try: buf = self._read_until_prompt(buf=buf) except ScrapliTimeout as exc: if len(channel_input) >= 4096: msg = ( \"timed out finding prompt after sending input, input is greater than 4096 \" \"chars, try setting 'use_compressed_parser' to False\" ) self.logger.info(msg) raise ScrapliTimeout(msg) from exc raise ScrapliTimeout from exc if self._server_echo is None: # At least per early drafts of the netconf over ssh rfcs the netconf servers MUST NOT # echo the input commands back to the client. In the case of \"normal\" scrapli netconf # with the system transport this happens anyway because we combine the stdin and stdout # fds into a single pty, however for other transports we have an actual stdin and # stdout fd to read/write. It seems that at the very least IOSXE with NETCONF 1.1 seems # to want to echo inputs back onto to the stdout for the channel. This is totally ok # and we can deal with it, we just need to *know* that it is happening, so while the # _server_echo attribute is still `None`, we can go ahead and see if the input we sent # is in the output we read off the channel. If it is *not* we know the server does *not* # echo and we can move on. If it *is* in the output, we know the server echoes, and we # also have one additional step in that we need to read \"until prompt\" again in order to # capture the reply to our rpc. # # See: https://tools.ietf.org/html/draft-ietf-netconf-ssh-02 (search for \"echo\") self.logger.debug(\"server echo is unset, determining if server echoes inputs now\") if bytes_final_channel_input in buf: self.logger.debug(\"server echoes inputs, setting _server_echo to 'true'\") self._server_echo = True # since echo is True and we only read until our input (because our inputs always end # with a \"prompt\" that we read until) we need to once again read until prompt, this # read will read all the way up through the *reply* to the prompt at end of the # reply message buf = self._read_until_prompt(buf=b\"\") else: self.logger.debug(\"server does *not* echo inputs, setting _server_echo to 'false'\") self._server_echo = False if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: # netconf 1.1 with \"chunking\" style message format needs an extra return char here self.send_return() return buf Ancestors (in MRO) \u00b6 scrapli.channel.sync_channel.Channel scrapli_netconf.channel.base_channel.BaseNetconfChannel scrapli.channel.base_channel.BaseChannel Methods \u00b6 channel_authenticate_netconf \u00b6 channel_authenticate_netconf(self, auth_password: str, auth_private_key_passphrase: str) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 Handle SSH Authentication for transports that only operate \"in the channel\" (i.e. system) Args: auth_password: password to authenticate with auth_private_key_passphrase: passphrase for ssh key if necessary Returns: None Raises: ScrapliAuthenticationFailed: if password prompt seen more than twice ScrapliAuthenticationFailed: if passphrase prompt seen more than twice open_netconf \u00b6 open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 Open the netconf channel Args: N/A Returns: None Raises: N/A send_input_netconf \u00b6 send_input_netconf(self, channel_input: str) \u2011> bytes 1 2 3 4 5 6 7 8 9 10 11 12 Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: ScrapliTimeout: re-raises channel timeouts with additional message if channel input may be big enough to require setting `use_compressed_parser` to false -- note that this has only been seen as an issue with NXOS so far.","title":"Sync Channel"},{"location":"api_docs/channel/sync_channel/#module-scrapli_netconfchannelsync_channel","text":"scrapli_netconf.channel.sync_channel Expand source code \"\"\"scrapli_netconf.channel.sync_channel\"\"\" import re from typing import Optional from scrapli.channel import Channel from scrapli.channel.base_channel import BaseChannelArgs from scrapli.decorators import ChannelTimeout from scrapli.exceptions import ScrapliAuthenticationFailed, ScrapliTimeout from scrapli.transport.base import Transport from scrapli_netconf.channel.base_channel import BaseNetconfChannel, NetconfBaseChannelArgs from scrapli_netconf.constants import NetconfVersion HELLO_MATCH = re.compile(pattern=rb\" < (\\w+\\:){0,1}hello\", flags=re.I) class NetconfChannel(Channel, BaseNetconfChannel): def __init__( self, transport: Transport, base_channel_args: BaseChannelArgs, netconf_base_channel_args: NetconfBaseChannelArgs, ): super().__init__(transport=transport, base_channel_args=base_channel_args) self._netconf_base_channel_args = netconf_base_channel_args # always use `]]>]]>` as the initial prompt to match self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._server_echo: Optional[bool] = None self._capabilities_buf = b\"\" def open_netconf(self) -> None: \"\"\" Open the netconf channel Args: N/A Returns: None Raises: N/A \"\"\" # open in scrapli core is where we open channel log (if applicable), do that self.open() raw_server_capabilities = self._get_server_capabilities() self._process_capabilities_exchange(raw_server_capabilities=raw_server_capabilities) self._send_client_capabilities() @staticmethod def _authenticate_check_hello(buf: bytes) -> bool: \"\"\" Check if \"hello\" message is in output Args: buf: bytes output from the channel Returns: bool: true if hello message is seen, otherwise false Raises: N/A \"\"\" hello_match = re.search(pattern=HELLO_MATCH, string=buf) if hello_match: return True return False @ChannelTimeout(\"timed out during in channel netconf authentication\") def channel_authenticate_netconf( self, auth_password: str, auth_private_key_passphrase: str ) -> None: \"\"\" Handle SSH Authentication for transports that only operate \"in the channel\" (i.e. system) Args: auth_password: password to authenticate with auth_private_key_passphrase: passphrase for ssh key if necessary Returns: None Raises: ScrapliAuthenticationFailed: if password prompt seen more than twice ScrapliAuthenticationFailed: if passphrase prompt seen more than twice \"\"\" self.logger.debug(\"attempting in channel netconf authentication\") password_count = 0 passphrase_count = 0 authenticate_buf = b\"\" with self._channel_lock(): while True: buf = self.read() authenticate_buf += buf.lower() self._capabilities_buf += buf self._ssh_message_handler(output=authenticate_buf) if b\"password\" in authenticate_buf: # clear the authentication buffer so we don't re-read the password prompt authenticate_buf = b\"\" password_count += 1 if password_count > 2: msg = \"password prompt seen more than once, assuming auth failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) self.write(channel_input=auth_password, redacted=True) self.send_return() if b\"enter passphrase for key\" in authenticate_buf: # clear the authentication buffer so we don't re-read the passphrase prompt authenticate_buf = b\"\" passphrase_count += 1 if passphrase_count > 2: msg = \"passphrase prompt seen more than once, assuming auth failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) self.write(channel_input=auth_private_key_passphrase, redacted=True) self.send_return() if self._authenticate_check_hello(buf=authenticate_buf): self.logger.info( \"found start of server capabilities, authentication successful\" ) return @ChannelTimeout( \"timed out determining if session is authenticated/getting server capabilities\", ) def _get_server_capabilities(self) -> bytes: \"\"\" Read until all server capabilities have been sent by server Args: N/A Returns: bytes: raw bytes containing server capabilities Raises: N/A \"\"\" capabilities_buf = self._capabilities_buf # reset this to empty to avoid any confusion now that we are moving on self._capabilities_buf = b\"\" with self._channel_lock(): while b\"]]>]]>\" not in capabilities_buf: capabilities_buf += self.read() self.logger.debug(f\"received raw server capabilities: {repr(capabilities_buf)}\") return capabilities_buf @ChannelTimeout(\"timed out sending client capabilities\") def _send_client_capabilities( self, ) -> None: \"\"\" Send client capabilities to the netconf server Args: N/A Returns: None Raises: N/A \"\"\" with self._channel_lock(): bytes_client_capabilities = self._pre_send_client_capabilities( client_capabilities=self._netconf_base_channel_args.client_capabilities ) self._read_until_input(channel_input=bytes_client_capabilities) self.send_return() def _read_until_input(self, channel_input: bytes) -> bytes: \"\"\" Async read until all input has been entered. Args: channel_input: string to write to channel Returns: bytes: output read from channel Raises: N/A \"\"\" output = b\"\" if self._server_echo is None or self._server_echo is False: # if server_echo is `None` we dont know if the server echoes yet, so just return nothing # if its False we know it doesnt echo and we can return empty byte string anyway return output if not channel_input: self.logger.info(f\"Read: {repr(output)}\") return output while True: output += self.read() # if we have all the input *or* we see the closing rpc tag we know we are done here if channel_input in output or b\"rpc>\" in output: break self.logger.info(f\"Read: {repr(output)}\") return output def send_input_netconf(self, channel_input: str) -> bytes: \"\"\" Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: ScrapliTimeout: re-raises channel timeouts with additional message if channel input may be big enough to require setting `use_compressed_parser` to false -- note that this has only been seen as an issue with NXOS so far. \"\"\" bytes_final_channel_input = channel_input.encode() buf: bytes buf, _ = super().send_input(channel_input=channel_input, strip_prompt=False, eager=True) if bytes_final_channel_input in buf: # if we got the input AND the rpc-reply we can strip out our inputs so we just have the # reply remaining buf = buf.split(bytes_final_channel_input)[1] try: buf = self._read_until_prompt(buf=buf) except ScrapliTimeout as exc: if len(channel_input) >= 4096: msg = ( \"timed out finding prompt after sending input, input is greater than 4096 \" \"chars, try setting 'use_compressed_parser' to False\" ) self.logger.info(msg) raise ScrapliTimeout(msg) from exc raise ScrapliTimeout from exc if self._server_echo is None: # At least per early drafts of the netconf over ssh rfcs the netconf servers MUST NOT # echo the input commands back to the client. In the case of \"normal\" scrapli netconf # with the system transport this happens anyway because we combine the stdin and stdout # fds into a single pty, however for other transports we have an actual stdin and # stdout fd to read/write. It seems that at the very least IOSXE with NETCONF 1.1 seems # to want to echo inputs back onto to the stdout for the channel. This is totally ok # and we can deal with it, we just need to *know* that it is happening, so while the # _server_echo attribute is still `None`, we can go ahead and see if the input we sent # is in the output we read off the channel. If it is *not* we know the server does *not* # echo and we can move on. If it *is* in the output, we know the server echoes, and we # also have one additional step in that we need to read \"until prompt\" again in order to # capture the reply to our rpc. # # See: https://tools.ietf.org/html/draft-ietf-netconf-ssh-02 (search for \"echo\") self.logger.debug(\"server echo is unset, determining if server echoes inputs now\") if bytes_final_channel_input in buf: self.logger.debug(\"server echoes inputs, setting _server_echo to 'true'\") self._server_echo = True # since echo is True and we only read until our input (because our inputs always end # with a \"prompt\" that we read until) we need to once again read until prompt, this # read will read all the way up through the *reply* to the prompt at end of the # reply message buf = self._read_until_prompt(buf=b\"\") else: self.logger.debug(\"server does *not* echo inputs, setting _server_echo to 'false'\") self._server_echo = False if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: # netconf 1.1 with \"chunking\" style message format needs an extra return char here self.send_return() return buf","title":"Module scrapli_netconf.channel.sync_channel"},{"location":"api_docs/channel/sync_channel/#classes","text":"","title":"Classes"},{"location":"api_docs/channel/sync_channel/#netconfchannel","text":"1 2 3 4 5 6 7 8 9 10 11 BaseChannel Object -- provides convenience methods to both sync and async Channels Args: transport: initialized scrapli Transport/AsyncTransport object base_channel_args: BaseChannelArgs object Returns: None Raises: N/A Expand source code class NetconfChannel(Channel, BaseNetconfChannel): def __init__( self, transport: Transport, base_channel_args: BaseChannelArgs, netconf_base_channel_args: NetconfBaseChannelArgs, ): super().__init__(transport=transport, base_channel_args=base_channel_args) self._netconf_base_channel_args = netconf_base_channel_args # always use `]]>]]>` as the initial prompt to match self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" self._server_echo: Optional[bool] = None self._capabilities_buf = b\"\" def open_netconf(self) -> None: \"\"\" Open the netconf channel Args: N/A Returns: None Raises: N/A \"\"\" # open in scrapli core is where we open channel log (if applicable), do that self.open() raw_server_capabilities = self._get_server_capabilities() self._process_capabilities_exchange(raw_server_capabilities=raw_server_capabilities) self._send_client_capabilities() @staticmethod def _authenticate_check_hello(buf: bytes) -> bool: \"\"\" Check if \"hello\" message is in output Args: buf: bytes output from the channel Returns: bool: true if hello message is seen, otherwise false Raises: N/A \"\"\" hello_match = re.search(pattern=HELLO_MATCH, string=buf) if hello_match: return True return False @ChannelTimeout(\"timed out during in channel netconf authentication\") def channel_authenticate_netconf( self, auth_password: str, auth_private_key_passphrase: str ) -> None: \"\"\" Handle SSH Authentication for transports that only operate \"in the channel\" (i.e. system) Args: auth_password: password to authenticate with auth_private_key_passphrase: passphrase for ssh key if necessary Returns: None Raises: ScrapliAuthenticationFailed: if password prompt seen more than twice ScrapliAuthenticationFailed: if passphrase prompt seen more than twice \"\"\" self.logger.debug(\"attempting in channel netconf authentication\") password_count = 0 passphrase_count = 0 authenticate_buf = b\"\" with self._channel_lock(): while True: buf = self.read() authenticate_buf += buf.lower() self._capabilities_buf += buf self._ssh_message_handler(output=authenticate_buf) if b\"password\" in authenticate_buf: # clear the authentication buffer so we don't re-read the password prompt authenticate_buf = b\"\" password_count += 1 if password_count > 2: msg = \"password prompt seen more than once, assuming auth failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) self.write(channel_input=auth_password, redacted=True) self.send_return() if b\"enter passphrase for key\" in authenticate_buf: # clear the authentication buffer so we don't re-read the passphrase prompt authenticate_buf = b\"\" passphrase_count += 1 if passphrase_count > 2: msg = \"passphrase prompt seen more than once, assuming auth failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) self.write(channel_input=auth_private_key_passphrase, redacted=True) self.send_return() if self._authenticate_check_hello(buf=authenticate_buf): self.logger.info( \"found start of server capabilities, authentication successful\" ) return @ChannelTimeout( \"timed out determining if session is authenticated/getting server capabilities\", ) def _get_server_capabilities(self) -> bytes: \"\"\" Read until all server capabilities have been sent by server Args: N/A Returns: bytes: raw bytes containing server capabilities Raises: N/A \"\"\" capabilities_buf = self._capabilities_buf # reset this to empty to avoid any confusion now that we are moving on self._capabilities_buf = b\"\" with self._channel_lock(): while b\"]]>]]>\" not in capabilities_buf: capabilities_buf += self.read() self.logger.debug(f\"received raw server capabilities: {repr(capabilities_buf)}\") return capabilities_buf @ChannelTimeout(\"timed out sending client capabilities\") def _send_client_capabilities( self, ) -> None: \"\"\" Send client capabilities to the netconf server Args: N/A Returns: None Raises: N/A \"\"\" with self._channel_lock(): bytes_client_capabilities = self._pre_send_client_capabilities( client_capabilities=self._netconf_base_channel_args.client_capabilities ) self._read_until_input(channel_input=bytes_client_capabilities) self.send_return() def _read_until_input(self, channel_input: bytes) -> bytes: \"\"\" Async read until all input has been entered. Args: channel_input: string to write to channel Returns: bytes: output read from channel Raises: N/A \"\"\" output = b\"\" if self._server_echo is None or self._server_echo is False: # if server_echo is `None` we dont know if the server echoes yet, so just return nothing # if its False we know it doesnt echo and we can return empty byte string anyway return output if not channel_input: self.logger.info(f\"Read: {repr(output)}\") return output while True: output += self.read() # if we have all the input *or* we see the closing rpc tag we know we are done here if channel_input in output or b\"rpc>\" in output: break self.logger.info(f\"Read: {repr(output)}\") return output def send_input_netconf(self, channel_input: str) -> bytes: \"\"\" Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: ScrapliTimeout: re-raises channel timeouts with additional message if channel input may be big enough to require setting `use_compressed_parser` to false -- note that this has only been seen as an issue with NXOS so far. \"\"\" bytes_final_channel_input = channel_input.encode() buf: bytes buf, _ = super().send_input(channel_input=channel_input, strip_prompt=False, eager=True) if bytes_final_channel_input in buf: # if we got the input AND the rpc-reply we can strip out our inputs so we just have the # reply remaining buf = buf.split(bytes_final_channel_input)[1] try: buf = self._read_until_prompt(buf=buf) except ScrapliTimeout as exc: if len(channel_input) >= 4096: msg = ( \"timed out finding prompt after sending input, input is greater than 4096 \" \"chars, try setting 'use_compressed_parser' to False\" ) self.logger.info(msg) raise ScrapliTimeout(msg) from exc raise ScrapliTimeout from exc if self._server_echo is None: # At least per early drafts of the netconf over ssh rfcs the netconf servers MUST NOT # echo the input commands back to the client. In the case of \"normal\" scrapli netconf # with the system transport this happens anyway because we combine the stdin and stdout # fds into a single pty, however for other transports we have an actual stdin and # stdout fd to read/write. It seems that at the very least IOSXE with NETCONF 1.1 seems # to want to echo inputs back onto to the stdout for the channel. This is totally ok # and we can deal with it, we just need to *know* that it is happening, so while the # _server_echo attribute is still `None`, we can go ahead and see if the input we sent # is in the output we read off the channel. If it is *not* we know the server does *not* # echo and we can move on. If it *is* in the output, we know the server echoes, and we # also have one additional step in that we need to read \"until prompt\" again in order to # capture the reply to our rpc. # # See: https://tools.ietf.org/html/draft-ietf-netconf-ssh-02 (search for \"echo\") self.logger.debug(\"server echo is unset, determining if server echoes inputs now\") if bytes_final_channel_input in buf: self.logger.debug(\"server echoes inputs, setting _server_echo to 'true'\") self._server_echo = True # since echo is True and we only read until our input (because our inputs always end # with a \"prompt\" that we read until) we need to once again read until prompt, this # read will read all the way up through the *reply* to the prompt at end of the # reply message buf = self._read_until_prompt(buf=b\"\") else: self.logger.debug(\"server does *not* echo inputs, setting _server_echo to 'false'\") self._server_echo = False if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_1: # netconf 1.1 with \"chunking\" style message format needs an extra return char here self.send_return() return buf","title":"NetconfChannel"},{"location":"api_docs/channel/sync_channel/#ancestors-in-mro","text":"scrapli.channel.sync_channel.Channel scrapli_netconf.channel.base_channel.BaseNetconfChannel scrapli.channel.base_channel.BaseChannel","title":"Ancestors (in MRO)"},{"location":"api_docs/channel/sync_channel/#methods","text":"","title":"Methods"},{"location":"api_docs/channel/sync_channel/#channel_authenticate_netconf","text":"channel_authenticate_netconf(self, auth_password: str, auth_private_key_passphrase: str) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 Handle SSH Authentication for transports that only operate \"in the channel\" (i.e. system) Args: auth_password: password to authenticate with auth_private_key_passphrase: passphrase for ssh key if necessary Returns: None Raises: ScrapliAuthenticationFailed: if password prompt seen more than twice ScrapliAuthenticationFailed: if passphrase prompt seen more than twice","title":"channel_authenticate_netconf"},{"location":"api_docs/channel/sync_channel/#open_netconf","text":"open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 Open the netconf channel Args: N/A Returns: None Raises: N/A","title":"open_netconf"},{"location":"api_docs/channel/sync_channel/#send_input_netconf","text":"send_input_netconf(self, channel_input: str) \u2011> bytes 1 2 3 4 5 6 7 8 9 10 11 12 Send inputs to netconf server Args: channel_input: string of the base xml message to send to netconf server Returns: bytes: bytes result of message sent to netconf server Raises: ScrapliTimeout: re-raises channel timeouts with additional message if channel input may be big enough to require setting `use_compressed_parser` to false -- note that this has only been seen as an issue with NXOS so far.","title":"send_input_netconf"},{"location":"api_docs/driver/async_driver/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.driver.async_driver \u00b6 scrapli_netconf.driver.async_driver Expand source code \"\"\"scrapli_netconf.driver.async_driver\"\"\" from typing import Any, Callable, Dict, List, Optional, Union from lxml.etree import _Element from scrapli import AsyncDriver from scrapli_netconf.channel.async_channel import AsyncNetconfChannel from scrapli_netconf.channel.base_channel import NetconfBaseChannelArgs from scrapli_netconf.driver.base_driver import NetconfBaseDriver from scrapli_netconf.response import NetconfResponse class AsyncNetconfDriver(AsyncDriver, NetconfBaseDriver): # kinda hate this but need to tell mypy that channel in netconf land is in fact a channel of # type `NetconfChannel` channel: AsyncNetconfChannel def __init__( self, host: str, port: int = 830, strip_namespaces: bool = False, strict_datastores: bool = False, auth_username: str = \"\", auth_password: str = \"\", auth_private_key: str = \"\", auth_private_key_passphrase: str = \"\", auth_strict_key: bool = True, auth_bypass: bool = False, timeout_socket: float = 15.0, timeout_transport: float = 30.0, timeout_ops: float = 30.0, comms_prompt_pattern: str = r\"^[a-z0-9.\\-@()/:]{1,48}[#>$]\\s*$\", comms_return_char: str = \"\\n\", ssh_config_file: Union[str, bool] = False, ssh_known_hosts_file: Union[str, bool] = False, on_init: Optional[Callable[..., Any]] = None, on_open: Optional[Callable[..., Any]] = None, on_close: Optional[Callable[..., Any]] = None, transport: str = \"system\", transport_options: Optional[Dict[str, Any]] = None, channel_log: Union[str, bool] = False, channel_lock: bool = False, preferred_netconf_version: Optional[str] = None, use_compressed_parser: bool = True, ) -> None: super().__init__( host=host, port=port, auth_username=auth_username, auth_password=auth_password, auth_private_key=auth_private_key, auth_private_key_passphrase=auth_private_key_passphrase, auth_strict_key=auth_strict_key, auth_bypass=auth_bypass, timeout_socket=timeout_socket, timeout_transport=timeout_transport, timeout_ops=timeout_ops, comms_prompt_pattern=comms_prompt_pattern, comms_return_char=comms_return_char, ssh_config_file=ssh_config_file, ssh_known_hosts_file=ssh_known_hosts_file, on_init=on_init, on_open=on_open, on_close=on_close, transport=transport, transport_options=transport_options, channel_log=channel_log, channel_lock=channel_lock, ) _preferred_netconf_version = self._determine_preferred_netconf_version( preferred_netconf_version=preferred_netconf_version ) _preferred_xml_parser = self._determine_preferred_xml_parser( use_compressed_parser=use_compressed_parser ) self._netconf_base_channel_args = NetconfBaseChannelArgs( netconf_version=_preferred_netconf_version, xml_parser=_preferred_xml_parser ) self.channel = AsyncNetconfChannel( transport=self.transport, base_channel_args=self._base_channel_args, netconf_base_channel_args=self._netconf_base_channel_args, ) self.strip_namespaces = strip_namespaces self.strict_datastores = strict_datastores self.server_capabilities: List[str] = [] self.readable_datastores: List[str] = [] self.writeable_datastores: List[str] = [] self.message_id = 101 async def open(self) -> None: \"\"\" Open netconf connection to server Args: N/A Returns: None Raises: N/A \"\"\" self._pre_open_closing_log(closing=False) await self.transport.open_netconf() await self.channel.open_netconf() self._build_readable_datastores() self._build_writeable_datastores() self._post_open_closing_log(closing=False) async def get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Netconf get operation Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get(filter_=filter_, filter_type=filter_type) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get_config( source=source, filter_=filter_, filter_type=filter_type, default_type=default_type ) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_edit_config(config=config, target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def delete_config(self, target: str = \"candidate\") -> NetconfResponse: \"\"\" Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_delete_config(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def commit(self) -> NetconfResponse: \"\"\" Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_commit() raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def discard(self) -> NetconfResponse: \"\"\" Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_discard() raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def lock(self, target: str) -> NetconfResponse: \"\"\" Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_lock(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def unlock(self, target: str) -> NetconfResponse: \"\"\" Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_unlock(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_rpc(filter_=filter_) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def validate(self, source: str) -> NetconfResponse: \"\"\" Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_validate(source=source) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_copy_config(source=source, target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response Classes \u00b6 AsyncNetconfDriver \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 BaseDriver Object BaseDriver is the root for all Scrapli driver classes. The synchronous and asyncio driver base driver classes can be used to provide a semi-pexpect like experience over top of whatever transport a user prefers. Generally, however, the base driver classes should not be used directly. It is best to use the GenericDriver (or AsyncGenericDriver) or NetworkDriver (or AsyncNetworkDriver) sub-classes of the base drivers. Args: host: host ip/name to connect to port: port to connect to auth_username: username for authentication auth_private_key: path to private key for authentication auth_private_key_passphrase: passphrase for decrypting ssh key if necessary auth_password: password for authentication auth_strict_key: strict host checking or not auth_bypass: bypass \"in channel\" authentication -- only supported with telnet, asynctelnet, and system transport plugins timeout_socket: timeout for establishing socket/initial connection in seconds timeout_transport: timeout for ssh|telnet transport in seconds timeout_ops: timeout for ssh channel operations comms_prompt_pattern: raw string regex pattern -- preferably use `^` and `$` anchors! this is the single most important attribute here! if this does not match a prompt, scrapli will not work! IMPORTANT: regex search uses multi-line + case insensitive flags. multi-line allows for highly reliably matching for prompts however we do NOT strip trailing whitespace for each line, so be sure to add '\\\\s?' or similar if your device needs that. This should be mostly sorted for you if using network drivers (i.e. `IOSXEDriver`). Lastly, the case insensitive is just a convenience factor so i can be lazy. comms_return_char: character to use to send returns to host ssh_config_file: string to path for ssh config file, True to use default ssh config file or False to ignore default ssh config file ssh_known_hosts_file: string to path for ssh known hosts file, True to use default known file locations. Only applicable/needed if `auth_strict_key` is set to True on_init: callable that accepts the class instance as its only argument. this callable, if provided, is executed as the last step of object instantiation -- its purpose is primarily to provide a mechanism for scrapli community platforms to have an easy way to modify initialization arguments/object attributes without needing to create a class that extends the driver, instead allowing the community platforms to simply build from the GenericDriver or NetworkDriver classes, and pass this callable to do things such as appending to a username (looking at you RouterOS!!). Note that this is *always* a synchronous function (even for asyncio drivers)! on_open: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately after authentication is completed. Common use cases for this callable would be to disable paging or accept any kind of banner message that prompts a user upon connection on_close: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately prior to closing the underlying transport. Common use cases for this callable would be to save configurations prior to exiting, or to logout properly to free up vtys or similar transport: name of the transport plugin to use for the actual telnet/ssh/netconf connection. Available \"core\" transports are: - system - telnet - asynctelnet - ssh2 - paramiko - asyncssh Please see relevant transport plugin section for details. Additionally third party transport plugins may be available. transport_options: dictionary of options to pass to selected transport class; see docs for given transport class for details of what to pass here channel_lock: True/False to lock the channel (threading.Lock/asyncio.Lock) during any channel operations, defaults to False channel_log: True/False or a string path to a file of where to write out channel logs -- these are not \"logs\" in the normal logging module sense, but only the output that is read from the channel. In other words, the output of the channel log should look similar to what you would see as a human connecting to a device channel_log_mode: \"write\"|\"append\", all other values will raise ValueError, does what it sounds like it should by setting the channel log to the provided mode logging_uid: unique identifier (string) to associate to log messages; useful if you have multiple connections to the same device (i.e. one console, one ssh, or one to each supervisor module, etc.) Returns: None Raises: N/A Expand source code class AsyncNetconfDriver(AsyncDriver, NetconfBaseDriver): # kinda hate this but need to tell mypy that channel in netconf land is in fact a channel of # type `NetconfChannel` channel: AsyncNetconfChannel def __init__( self, host: str, port: int = 830, strip_namespaces: bool = False, strict_datastores: bool = False, auth_username: str = \"\", auth_password: str = \"\", auth_private_key: str = \"\", auth_private_key_passphrase: str = \"\", auth_strict_key: bool = True, auth_bypass: bool = False, timeout_socket: float = 15.0, timeout_transport: float = 30.0, timeout_ops: float = 30.0, comms_prompt_pattern: str = r\"^[a-z0-9.\\-@()/:]{1,48}[#>$]\\s*$\", comms_return_char: str = \"\\n\", ssh_config_file: Union[str, bool] = False, ssh_known_hosts_file: Union[str, bool] = False, on_init: Optional[Callable[..., Any]] = None, on_open: Optional[Callable[..., Any]] = None, on_close: Optional[Callable[..., Any]] = None, transport: str = \"system\", transport_options: Optional[Dict[str, Any]] = None, channel_log: Union[str, bool] = False, channel_lock: bool = False, preferred_netconf_version: Optional[str] = None, use_compressed_parser: bool = True, ) -> None: super().__init__( host=host, port=port, auth_username=auth_username, auth_password=auth_password, auth_private_key=auth_private_key, auth_private_key_passphrase=auth_private_key_passphrase, auth_strict_key=auth_strict_key, auth_bypass=auth_bypass, timeout_socket=timeout_socket, timeout_transport=timeout_transport, timeout_ops=timeout_ops, comms_prompt_pattern=comms_prompt_pattern, comms_return_char=comms_return_char, ssh_config_file=ssh_config_file, ssh_known_hosts_file=ssh_known_hosts_file, on_init=on_init, on_open=on_open, on_close=on_close, transport=transport, transport_options=transport_options, channel_log=channel_log, channel_lock=channel_lock, ) _preferred_netconf_version = self._determine_preferred_netconf_version( preferred_netconf_version=preferred_netconf_version ) _preferred_xml_parser = self._determine_preferred_xml_parser( use_compressed_parser=use_compressed_parser ) self._netconf_base_channel_args = NetconfBaseChannelArgs( netconf_version=_preferred_netconf_version, xml_parser=_preferred_xml_parser ) self.channel = AsyncNetconfChannel( transport=self.transport, base_channel_args=self._base_channel_args, netconf_base_channel_args=self._netconf_base_channel_args, ) self.strip_namespaces = strip_namespaces self.strict_datastores = strict_datastores self.server_capabilities: List[str] = [] self.readable_datastores: List[str] = [] self.writeable_datastores: List[str] = [] self.message_id = 101 async def open(self) -> None: \"\"\" Open netconf connection to server Args: N/A Returns: None Raises: N/A \"\"\" self._pre_open_closing_log(closing=False) await self.transport.open_netconf() await self.channel.open_netconf() self._build_readable_datastores() self._build_writeable_datastores() self._post_open_closing_log(closing=False) async def get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Netconf get operation Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get(filter_=filter_, filter_type=filter_type) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get_config( source=source, filter_=filter_, filter_type=filter_type, default_type=default_type ) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_edit_config(config=config, target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def delete_config(self, target: str = \"candidate\") -> NetconfResponse: \"\"\" Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_delete_config(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def commit(self) -> NetconfResponse: \"\"\" Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_commit() raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def discard(self) -> NetconfResponse: \"\"\" Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_discard() raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def lock(self, target: str) -> NetconfResponse: \"\"\" Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_lock(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def unlock(self, target: str) -> NetconfResponse: \"\"\" Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_unlock(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_rpc(filter_=filter_) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def validate(self, source: str) -> NetconfResponse: \"\"\" Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_validate(source=source) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_copy_config(source=source, target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response Ancestors (in MRO) \u00b6 scrapli.driver.base.async_driver.AsyncDriver scrapli_netconf.driver.base_driver.NetconfBaseDriver scrapli.driver.base.base_driver.BaseDriver Class variables \u00b6 channel: scrapli_netconf.channel.async_channel.AsyncNetconfChannel Methods \u00b6 commit \u00b6 commit(self) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A copy_config \u00b6 copy_config(self, source: str, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A delete_config \u00b6 delete_config(self, target: str = 'candidate') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A discard \u00b6 discard(self) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A edit_config \u00b6 edit_config(self, config: str, target: str = 'running') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A get \u00b6 get(self, filter_: str, filter_type: str = 'subtree') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf get operation Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A get_config \u00b6 get_config(self, source: str = 'running', filter_: Optional[str] = None, filter_type: str = 'subtree', default_type: Optional[str] = None) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A lock \u00b6 lock(self, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A open \u00b6 open(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 Open netconf connection to server Args: N/A Returns: None Raises: N/A rpc \u00b6 rpc(self, filter_: Union[str, lxml.etree._Element]) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A unlock \u00b6 unlock(self, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A validate \u00b6 validate(self, source: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"Async Driver"},{"location":"api_docs/driver/async_driver/#module-scrapli_netconfdriverasync_driver","text":"scrapli_netconf.driver.async_driver Expand source code \"\"\"scrapli_netconf.driver.async_driver\"\"\" from typing import Any, Callable, Dict, List, Optional, Union from lxml.etree import _Element from scrapli import AsyncDriver from scrapli_netconf.channel.async_channel import AsyncNetconfChannel from scrapli_netconf.channel.base_channel import NetconfBaseChannelArgs from scrapli_netconf.driver.base_driver import NetconfBaseDriver from scrapli_netconf.response import NetconfResponse class AsyncNetconfDriver(AsyncDriver, NetconfBaseDriver): # kinda hate this but need to tell mypy that channel in netconf land is in fact a channel of # type `NetconfChannel` channel: AsyncNetconfChannel def __init__( self, host: str, port: int = 830, strip_namespaces: bool = False, strict_datastores: bool = False, auth_username: str = \"\", auth_password: str = \"\", auth_private_key: str = \"\", auth_private_key_passphrase: str = \"\", auth_strict_key: bool = True, auth_bypass: bool = False, timeout_socket: float = 15.0, timeout_transport: float = 30.0, timeout_ops: float = 30.0, comms_prompt_pattern: str = r\"^[a-z0-9.\\-@()/:]{1,48}[#>$]\\s*$\", comms_return_char: str = \"\\n\", ssh_config_file: Union[str, bool] = False, ssh_known_hosts_file: Union[str, bool] = False, on_init: Optional[Callable[..., Any]] = None, on_open: Optional[Callable[..., Any]] = None, on_close: Optional[Callable[..., Any]] = None, transport: str = \"system\", transport_options: Optional[Dict[str, Any]] = None, channel_log: Union[str, bool] = False, channel_lock: bool = False, preferred_netconf_version: Optional[str] = None, use_compressed_parser: bool = True, ) -> None: super().__init__( host=host, port=port, auth_username=auth_username, auth_password=auth_password, auth_private_key=auth_private_key, auth_private_key_passphrase=auth_private_key_passphrase, auth_strict_key=auth_strict_key, auth_bypass=auth_bypass, timeout_socket=timeout_socket, timeout_transport=timeout_transport, timeout_ops=timeout_ops, comms_prompt_pattern=comms_prompt_pattern, comms_return_char=comms_return_char, ssh_config_file=ssh_config_file, ssh_known_hosts_file=ssh_known_hosts_file, on_init=on_init, on_open=on_open, on_close=on_close, transport=transport, transport_options=transport_options, channel_log=channel_log, channel_lock=channel_lock, ) _preferred_netconf_version = self._determine_preferred_netconf_version( preferred_netconf_version=preferred_netconf_version ) _preferred_xml_parser = self._determine_preferred_xml_parser( use_compressed_parser=use_compressed_parser ) self._netconf_base_channel_args = NetconfBaseChannelArgs( netconf_version=_preferred_netconf_version, xml_parser=_preferred_xml_parser ) self.channel = AsyncNetconfChannel( transport=self.transport, base_channel_args=self._base_channel_args, netconf_base_channel_args=self._netconf_base_channel_args, ) self.strip_namespaces = strip_namespaces self.strict_datastores = strict_datastores self.server_capabilities: List[str] = [] self.readable_datastores: List[str] = [] self.writeable_datastores: List[str] = [] self.message_id = 101 async def open(self) -> None: \"\"\" Open netconf connection to server Args: N/A Returns: None Raises: N/A \"\"\" self._pre_open_closing_log(closing=False) await self.transport.open_netconf() await self.channel.open_netconf() self._build_readable_datastores() self._build_writeable_datastores() self._post_open_closing_log(closing=False) async def get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Netconf get operation Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get(filter_=filter_, filter_type=filter_type) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get_config( source=source, filter_=filter_, filter_type=filter_type, default_type=default_type ) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_edit_config(config=config, target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def delete_config(self, target: str = \"candidate\") -> NetconfResponse: \"\"\" Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_delete_config(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def commit(self) -> NetconfResponse: \"\"\" Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_commit() raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def discard(self) -> NetconfResponse: \"\"\" Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_discard() raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def lock(self, target: str) -> NetconfResponse: \"\"\" Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_lock(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def unlock(self, target: str) -> NetconfResponse: \"\"\" Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_unlock(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_rpc(filter_=filter_) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def validate(self, source: str) -> NetconfResponse: \"\"\" Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_validate(source=source) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_copy_config(source=source, target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response","title":"Module scrapli_netconf.driver.async_driver"},{"location":"api_docs/driver/async_driver/#classes","text":"","title":"Classes"},{"location":"api_docs/driver/async_driver/#asyncnetconfdriver","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 BaseDriver Object BaseDriver is the root for all Scrapli driver classes. The synchronous and asyncio driver base driver classes can be used to provide a semi-pexpect like experience over top of whatever transport a user prefers. Generally, however, the base driver classes should not be used directly. It is best to use the GenericDriver (or AsyncGenericDriver) or NetworkDriver (or AsyncNetworkDriver) sub-classes of the base drivers. Args: host: host ip/name to connect to port: port to connect to auth_username: username for authentication auth_private_key: path to private key for authentication auth_private_key_passphrase: passphrase for decrypting ssh key if necessary auth_password: password for authentication auth_strict_key: strict host checking or not auth_bypass: bypass \"in channel\" authentication -- only supported with telnet, asynctelnet, and system transport plugins timeout_socket: timeout for establishing socket/initial connection in seconds timeout_transport: timeout for ssh|telnet transport in seconds timeout_ops: timeout for ssh channel operations comms_prompt_pattern: raw string regex pattern -- preferably use `^` and `$` anchors! this is the single most important attribute here! if this does not match a prompt, scrapli will not work! IMPORTANT: regex search uses multi-line + case insensitive flags. multi-line allows for highly reliably matching for prompts however we do NOT strip trailing whitespace for each line, so be sure to add '\\\\s?' or similar if your device needs that. This should be mostly sorted for you if using network drivers (i.e. `IOSXEDriver`). Lastly, the case insensitive is just a convenience factor so i can be lazy. comms_return_char: character to use to send returns to host ssh_config_file: string to path for ssh config file, True to use default ssh config file or False to ignore default ssh config file ssh_known_hosts_file: string to path for ssh known hosts file, True to use default known file locations. Only applicable/needed if `auth_strict_key` is set to True on_init: callable that accepts the class instance as its only argument. this callable, if provided, is executed as the last step of object instantiation -- its purpose is primarily to provide a mechanism for scrapli community platforms to have an easy way to modify initialization arguments/object attributes without needing to create a class that extends the driver, instead allowing the community platforms to simply build from the GenericDriver or NetworkDriver classes, and pass this callable to do things such as appending to a username (looking at you RouterOS!!). Note that this is *always* a synchronous function (even for asyncio drivers)! on_open: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately after authentication is completed. Common use cases for this callable would be to disable paging or accept any kind of banner message that prompts a user upon connection on_close: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately prior to closing the underlying transport. Common use cases for this callable would be to save configurations prior to exiting, or to logout properly to free up vtys or similar transport: name of the transport plugin to use for the actual telnet/ssh/netconf connection. Available \"core\" transports are: - system - telnet - asynctelnet - ssh2 - paramiko - asyncssh Please see relevant transport plugin section for details. Additionally third party transport plugins may be available. transport_options: dictionary of options to pass to selected transport class; see docs for given transport class for details of what to pass here channel_lock: True/False to lock the channel (threading.Lock/asyncio.Lock) during any channel operations, defaults to False channel_log: True/False or a string path to a file of where to write out channel logs -- these are not \"logs\" in the normal logging module sense, but only the output that is read from the channel. In other words, the output of the channel log should look similar to what you would see as a human connecting to a device channel_log_mode: \"write\"|\"append\", all other values will raise ValueError, does what it sounds like it should by setting the channel log to the provided mode logging_uid: unique identifier (string) to associate to log messages; useful if you have multiple connections to the same device (i.e. one console, one ssh, or one to each supervisor module, etc.) Returns: None Raises: N/A Expand source code class AsyncNetconfDriver(AsyncDriver, NetconfBaseDriver): # kinda hate this but need to tell mypy that channel in netconf land is in fact a channel of # type `NetconfChannel` channel: AsyncNetconfChannel def __init__( self, host: str, port: int = 830, strip_namespaces: bool = False, strict_datastores: bool = False, auth_username: str = \"\", auth_password: str = \"\", auth_private_key: str = \"\", auth_private_key_passphrase: str = \"\", auth_strict_key: bool = True, auth_bypass: bool = False, timeout_socket: float = 15.0, timeout_transport: float = 30.0, timeout_ops: float = 30.0, comms_prompt_pattern: str = r\"^[a-z0-9.\\-@()/:]{1,48}[#>$]\\s*$\", comms_return_char: str = \"\\n\", ssh_config_file: Union[str, bool] = False, ssh_known_hosts_file: Union[str, bool] = False, on_init: Optional[Callable[..., Any]] = None, on_open: Optional[Callable[..., Any]] = None, on_close: Optional[Callable[..., Any]] = None, transport: str = \"system\", transport_options: Optional[Dict[str, Any]] = None, channel_log: Union[str, bool] = False, channel_lock: bool = False, preferred_netconf_version: Optional[str] = None, use_compressed_parser: bool = True, ) -> None: super().__init__( host=host, port=port, auth_username=auth_username, auth_password=auth_password, auth_private_key=auth_private_key, auth_private_key_passphrase=auth_private_key_passphrase, auth_strict_key=auth_strict_key, auth_bypass=auth_bypass, timeout_socket=timeout_socket, timeout_transport=timeout_transport, timeout_ops=timeout_ops, comms_prompt_pattern=comms_prompt_pattern, comms_return_char=comms_return_char, ssh_config_file=ssh_config_file, ssh_known_hosts_file=ssh_known_hosts_file, on_init=on_init, on_open=on_open, on_close=on_close, transport=transport, transport_options=transport_options, channel_log=channel_log, channel_lock=channel_lock, ) _preferred_netconf_version = self._determine_preferred_netconf_version( preferred_netconf_version=preferred_netconf_version ) _preferred_xml_parser = self._determine_preferred_xml_parser( use_compressed_parser=use_compressed_parser ) self._netconf_base_channel_args = NetconfBaseChannelArgs( netconf_version=_preferred_netconf_version, xml_parser=_preferred_xml_parser ) self.channel = AsyncNetconfChannel( transport=self.transport, base_channel_args=self._base_channel_args, netconf_base_channel_args=self._netconf_base_channel_args, ) self.strip_namespaces = strip_namespaces self.strict_datastores = strict_datastores self.server_capabilities: List[str] = [] self.readable_datastores: List[str] = [] self.writeable_datastores: List[str] = [] self.message_id = 101 async def open(self) -> None: \"\"\" Open netconf connection to server Args: N/A Returns: None Raises: N/A \"\"\" self._pre_open_closing_log(closing=False) await self.transport.open_netconf() await self.channel.open_netconf() self._build_readable_datastores() self._build_writeable_datastores() self._post_open_closing_log(closing=False) async def get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Netconf get operation Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get(filter_=filter_, filter_type=filter_type) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get_config( source=source, filter_=filter_, filter_type=filter_type, default_type=default_type ) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_edit_config(config=config, target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def delete_config(self, target: str = \"candidate\") -> NetconfResponse: \"\"\" Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_delete_config(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def commit(self) -> NetconfResponse: \"\"\" Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_commit() raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def discard(self) -> NetconfResponse: \"\"\" Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_discard() raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def lock(self, target: str) -> NetconfResponse: \"\"\" Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_lock(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def unlock(self, target: str) -> NetconfResponse: \"\"\" Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_unlock(target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_rpc(filter_=filter_) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def validate(self, source: str) -> NetconfResponse: \"\"\" Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_validate(source=source) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response async def copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_copy_config(source=source, target=target) raw_response = await self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response","title":"AsyncNetconfDriver"},{"location":"api_docs/driver/async_driver/#ancestors-in-mro","text":"scrapli.driver.base.async_driver.AsyncDriver scrapli_netconf.driver.base_driver.NetconfBaseDriver scrapli.driver.base.base_driver.BaseDriver","title":"Ancestors (in MRO)"},{"location":"api_docs/driver/async_driver/#class-variables","text":"channel: scrapli_netconf.channel.async_channel.AsyncNetconfChannel","title":"Class variables"},{"location":"api_docs/driver/async_driver/#methods","text":"","title":"Methods"},{"location":"api_docs/driver/async_driver/#commit","text":"commit(self) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"commit"},{"location":"api_docs/driver/async_driver/#copy_config","text":"copy_config(self, source: str, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"copy_config"},{"location":"api_docs/driver/async_driver/#delete_config","text":"delete_config(self, target: str = 'candidate') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"delete_config"},{"location":"api_docs/driver/async_driver/#discard","text":"discard(self) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"discard"},{"location":"api_docs/driver/async_driver/#edit_config","text":"edit_config(self, config: str, target: str = 'running') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"edit_config"},{"location":"api_docs/driver/async_driver/#get","text":"get(self, filter_: str, filter_type: str = 'subtree') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf get operation Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"get"},{"location":"api_docs/driver/async_driver/#get_config","text":"get_config(self, source: str = 'running', filter_: Optional[str] = None, filter_type: str = 'subtree', default_type: Optional[str] = None) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"get_config"},{"location":"api_docs/driver/async_driver/#lock","text":"lock(self, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"lock"},{"location":"api_docs/driver/async_driver/#open","text":"open(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 Open netconf connection to server Args: N/A Returns: None Raises: N/A","title":"open"},{"location":"api_docs/driver/async_driver/#rpc","text":"rpc(self, filter_: Union[str, lxml.etree._Element]) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"rpc"},{"location":"api_docs/driver/async_driver/#unlock","text":"unlock(self, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"unlock"},{"location":"api_docs/driver/async_driver/#validate","text":"validate(self, source: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"validate"},{"location":"api_docs/driver/base_driver/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.driver.base_driver \u00b6 scrapli_netconf.driver.base_driver Expand source code # pylint: disable=C0302 \"\"\"scrapli_netconf.driver.base_driver\"\"\" import importlib from dataclasses import fields from enum import Enum from typing import Any, Callable, List, Optional, Tuple, Union from lxml import etree from lxml.etree import _Element from scrapli.driver.base.base_driver import BaseDriver from scrapli.exceptions import ScrapliTypeError, ScrapliValueError from scrapli.helper import user_warning from scrapli_netconf.channel.base_channel import NetconfBaseChannelArgs from scrapli_netconf.constants import NetconfClientCapabilities, NetconfVersion, XmlParserVersion from scrapli_netconf.exceptions import CapabilityNotSupported from scrapli_netconf.response import NetconfResponse COMPRESSED_PARSER = etree.XMLParser(remove_blank_text=True, recover=True) STANDARD_PARSER = etree.XMLParser(remove_blank_text=False, recover=True) class NetconfBaseOperations(Enum): FILTER_SUBTREE = \" \" FILTER_XPATH = \" \" WITH_DEFAULTS_SUBTREE = ( \" \" \"{default_type} \" ) GET = \" \" GET_CONFIG = \" < {source}/> \" EDIT_CONFIG = \" < {target}/> \" DELETE_CONFIG = \" < {target}/> \" COPY_CONFIG = ( \" < {target}/> < {source}/> \" ) COMMIT = \" \" DISCARD = \" \" LOCK = \" < {target}/> \" UNLOCK = \" < {target}/> \" RPC = \" \" VALIDATE = \" < {source}/> \" class NetconfBaseDriver(BaseDriver): host: str readable_datastores: List[str] writeable_datastores: List[str] strip_namespaces: bool strict_datastores: bool flatten_input: bool _netconf_base_channel_args: NetconfBaseChannelArgs @property def netconf_version(self) -> NetconfVersion: \"\"\" Getter for 'netconf_version' attribute Args: N/A Returns: NetconfVersion: netconf_version enum Raises: N/A \"\"\" return self._netconf_base_channel_args.netconf_version @netconf_version.setter def netconf_version(self, value: NetconfVersion) -> None: \"\"\" Setter for 'netconf_version' attribute Args: value: NetconfVersion Returns: None Raises: ScrapliTypeError: if value is not of type NetconfVersion \"\"\" if not isinstance(value, NetconfVersion): raise ScrapliTypeError self.logger.debug(f\"setting 'netconf_version' value to '{value.value}'\") self._netconf_base_channel_args.netconf_version = value if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_0: self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" else: self._base_channel_args.comms_prompt_pattern = r\"^##$\" @property def client_capabilities(self) -> NetconfClientCapabilities: \"\"\" Getter for 'client_capabilities' attribute Args: N/A Returns: NetconfClientCapabilities: netconf client capabilities enum Raises: N/A \"\"\" return self._netconf_base_channel_args.client_capabilities @client_capabilities.setter def client_capabilities(self, value: NetconfClientCapabilities) -> None: \"\"\" Setter for 'client_capabilities' attribute Args: value: NetconfClientCapabilities value for client_capabilities Returns: None Raises: ScrapliTypeError: if value is not of type NetconfClientCapabilities \"\"\" if not isinstance(value, NetconfClientCapabilities): raise ScrapliTypeError self.logger.debug(f\"setting 'client_capabilities' value to '{value.value}'\") self._netconf_base_channel_args.client_capabilities = value @property def server_capabilities(self) -> List[str]: \"\"\" Getter for 'server_capabilities' attribute Args: N/A Returns: list: list of strings of server capabilities Raises: N/A \"\"\" return self._netconf_base_channel_args.server_capabilities or [] @server_capabilities.setter def server_capabilities(self, value: NetconfClientCapabilities) -> None: \"\"\" Setter for 'server_capabilities' attribute Args: value: list of strings of netconf server capabilities Returns: None Raises: ScrapliTypeError: if value is not of type list \"\"\" if not isinstance(value, list): raise ScrapliTypeError self.logger.debug(f\"setting 'server_capabilities' value to '{value}'\") self._netconf_base_channel_args.server_capabilities = value @staticmethod def _determine_preferred_netconf_version( preferred_netconf_version: Optional[str], ) -> NetconfVersion: \"\"\" Determine users preferred netconf version (if applicable) Args: preferred_netconf_version: optional string indicating users preferred netconf version Returns: NetconfVersion: users preferred netconf version Raises: ScrapliValueError: if preferred_netconf_version is not None or a valid option \"\"\" if preferred_netconf_version is None: return NetconfVersion.UNKNOWN if preferred_netconf_version == \"1.0\": return NetconfVersion.VERSION_1_0 if preferred_netconf_version == \"1.1\": return NetconfVersion.VERSION_1_1 raise ScrapliValueError( \"'preferred_netconf_version' provided with invalid value, must be one of: \" \"None, '1.0', or '1.1'\" ) @staticmethod def _determine_preferred_xml_parser(use_compressed_parser: bool) -> XmlParserVersion: \"\"\" Determine users preferred xml payload parser Args: use_compressed_parser: bool indicating use of compressed parser or not Returns: XmlParserVersion: users xml parser version Raises: N/A \"\"\" if use_compressed_parser is True: return XmlParserVersion.COMPRESSED_PARSER return XmlParserVersion.STANDARD_PARSER @property def xml_parser(self) -> etree.XMLParser: \"\"\" Getter for 'xml_parser' attribute Args: N/A Returns: etree.XMLParser: parser to use for parsing xml documents Raises: N/A \"\"\" if self._netconf_base_channel_args.xml_parser == XmlParserVersion.COMPRESSED_PARSER: return COMPRESSED_PARSER return STANDARD_PARSER @xml_parser.setter def xml_parser(self, value: XmlParserVersion) -> None: \"\"\" Setter for 'xml_parser' attribute Args: value: enum indicating parser version to use Returns: None Raises: ScrapliTypeError: if value is not of type XmlParserVersion \"\"\" if not isinstance(value, XmlParserVersion): raise ScrapliTypeError self._netconf_base_channel_args.xml_parser = value def _transport_factory(self) -> Tuple[Callable[..., Any], object]: \"\"\" Determine proper transport class and necessary arguments to initialize that class Args: N/A Returns: Tuple[Callable[..., Any], object]: tuple of transport class and dataclass of transport class specific arguments Raises: N/A \"\"\" transport_plugin_module = importlib.import_module( f\"scrapli_netconf.transport.plugins.{self.transport_name}.transport\" ) transport_class = getattr( transport_plugin_module, f\"Netconf{self.transport_name.capitalize()}Transport\" ) plugin_transport_args_class = getattr(transport_plugin_module, \"PluginTransportArgs\") _plugin_transport_args = { field.name: getattr(self, field.name) for field in fields(plugin_transport_args_class) } plugin_transport_args = plugin_transport_args_class(**_plugin_transport_args) return transport_class, plugin_transport_args def _build_readable_datastores(self) -> None: \"\"\" Build a list of readable datastores based on server's advertised capabilities Args: N/A Returns: None Raises: N/A \"\"\" self.readable_datastores = [] self.readable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:candidate:1.0\" in self.server_capabilities: self.readable_datastores.append(\"candidate\") if \"urn:ietf:params:netconf:capability:startup:1.0\" in self.server_capabilities: self.readable_datastores.append(\"startup\") def _build_writeable_datastores(self) -> None: \"\"\" Build a list of writeable/editable datastores based on server's advertised capabilities Args: N/A Returns: None Raises: N/A \"\"\" self.writeable_datastores = [] if \"urn:ietf:params:netconf:capability:writeable-running:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:writable-running:1.0\" in self.server_capabilities: # NOTE: iosxe shows \"writable\" (as of 2020.07.01) despite RFC being \"writeable\" self.writeable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:candidate:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"candidate\") if \"urn:ietf:params:netconf:capability:startup:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"startup\") def _validate_get_config_target(self, source: str) -> None: \"\"\" Validate get-config source is acceptable Args: source: configuration source to get; typically one of running|startup|candidate Returns: None Raises: ScrapliValueError: if an invalid source was selected and strict_datastores is True \"\"\" if source not in self.readable_datastores: msg = f\"'source' should be one of {self.readable_datastores}, got '{source}'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore source!\", message=msg) def _validate_edit_config_target(self, target: str) -> None: \"\"\" Validate edit-config/lock/unlock target is acceptable Args: target: configuration source to edit/lock; typically one of running|startup|candidate Returns: None Raises: ScrapliValueError: if an invalid source was selected \"\"\" if target not in self.writeable_datastores: msg = f\"'target' should be one of {self.writeable_datastores}, got '{target}'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore target!\", message=msg) def _validate_delete_config_target(self, target: str) -> None: \"\"\" Validate delete-config/lock/unlock target is acceptable Args: target: configuration source to delete; typically one of startup|candidate Returns: None Raises: ScrapliValueError: if an invalid target was selected \"\"\" if target == \"running\" or target not in self.writeable_datastores: msg = f\"'target' should be one of {self.writeable_datastores}, got '{target}'\" if target == \"running\": msg = \"delete-config 'target' may not be 'running'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore target!\", message=msg) def _build_base_elem(self) -> _Element: \"\"\" Create base element for netconf operations Args: N/A Returns: _Element: lxml base element to use for netconf operation Raises: N/A \"\"\" # pylint did not seem to want to be ok with assigning this as a class attribute... and its # only used here so... here we are self.message_id: int # pylint: disable=W0201 self.logger.debug(f\"Building base element for message id {self.message_id}\") base_xml_str = NetconfBaseOperations.RPC.value.format(message_id=self.message_id) self.message_id += 1 base_elem = etree.fromstring(text=base_xml_str) return base_elem def _build_filter(self, filter_: str, filter_type: str = \"subtree\") -> _Element: \"\"\" Create filter element for a given rpc The `filter_` string may contain multiple xml elements at its \"root\" (subtree filters); we will simply place the payload into a temporary \"tmp\" outer tag so that when we cast it to an etree object the elements are all preserved; without this outer \"tmp\" tag, lxml will scoop up only the first element provided as it appears to be the root of the document presumably. An example valid (to scrapli netconf at least) xml filter would be: 1 2 3 4 5 6 7 <interface-configurations xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-ifmgr-cfg\"> <interface-configuration> <active>act</active> </interface-configuration> </interface-configurations> <netconf-yang xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-man-netconf-cfg\"> </netconf-yang> Args: filter_: strings of filters to build into a filter element or (for subtree) a full filter string (in filter tags) filter_type: type of filter; subtree|xpath Returns: _Element: lxml filter element to use for netconf operation Raises: CapabilityNotSupported: if xpath selected and not supported on server ScrapliValueError: if filter_type is not one of subtree|xpath \"\"\" if filter_type == \"subtree\": # tmp tags to place the users kinda not valid xml filter into _filter_ = f\" {filter_} \" # \"validate\" subtree filter by forcing it into xml, parser \"flattens\" it as well tmp_xml_filter_element = etree.fromstring(_filter_, parser=self.xml_parser) if tmp_xml_filter_element.getchildren()[0].tag == \"filter\": # if the user filter was already wrapped in filter tags we'll end up here, we will # blindly reuse the users filter but we'll make sure that the filter \"type\" is set xml_filter_elem = tmp_xml_filter_element.getchildren()[0] xml_filter_elem.attrib[\"type\"] = \"subtree\" else: xml_filter_elem = etree.fromstring( NetconfBaseOperations.FILTER_SUBTREE.value.format(filter_type=filter_type), ) # iterate through the children inside the tmp tags and insert *those* elements into # the actual final filter payload for xml_filter_element in tmp_xml_filter_element: # insert the subtree filter into the parent filter element xml_filter_elem.insert(1, xml_filter_element) elif filter_type == \"xpath\": if \"urn:ietf:params:netconf:capability:xpath:1.0\" not in self.server_capabilities: msg = \"xpath filter requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) xml_filter_elem = etree.fromstring( NetconfBaseOperations.FILTER_XPATH.value.format( filter_type=filter_type, xpath=filter_ ), parser=self.xml_parser, ) else: raise ScrapliValueError( f\"'filter_type' should be one of subtree|xpath, got '{filter_type}'\" ) return xml_filter_elem def _build_with_defaults(self, default_type: str = \"report-all\") -> _Element: \"\"\" Create with-defaults element for a given operation Args: default_type: enumeration of with-defaults; report-all|trim|explicit|report-all-tagged Returns: _Element: lxml with-defaults element to use for netconf operation Raises: CapabilityNotSupported: if default_type provided but not supported by device ScrapliValueError: if default_type is not one of report-all|trim|explicit|report-all-tagged \"\"\" if default_type in [\"report-all\", \"trim\", \"explicit\", \"report-all-tagged\"]: if ( \"urn:ietf:params:netconf:capability:with-defaults:1.0\" not in self.server_capabilities ): msg = \"with-defaults requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) xml_with_defaults_element = etree.fromstring( NetconfBaseOperations.WITH_DEFAULTS_SUBTREE.value.format(default_type=default_type), parser=self.xml_parser, ) else: raise ScrapliValueError( \"'default_type' should be one of report-all|trim|explicit|report-all-tagged, \" f\"got '{default_type}'\" ) return xml_with_defaults_element def _finalize_channel_input(self, xml_request: _Element) -> bytes: \"\"\" Create finalized channel input (as bytes) Args: xml_request: finalized xml element to cast to bytes and add declaration to Returns: bytes: finalized bytes input -- with 1.0 delimiter or 1.1 encoding Raises: N/A \"\"\" channel_input: bytes = etree.tostring( element_or_tree=xml_request, xml_declaration=True, encoding=\"utf-8\" ) if self.netconf_version == NetconfVersion.VERSION_1_0: channel_input = channel_input + b\"]]>]]>\" else: # format message for chunk (netconf 1.1) style message channel_input = b\"#%b\\n\" % str(len(channel_input)).encode() + channel_input + b\"\\n##\" return channel_input def _pre_get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Handle pre \"get\" tasks for consistency between sync/async versions *NOTE* The channel input (filter_) is loaded up as an lxml etree element here, this is done with a parser that removes whitespace. This has a somewhat undesirable effect of making any \"pretty\" input not pretty, however... after we load the xml object (which we do to validate that it is valid xml) we dump that xml object back to a string to be used as the actual raw payload we send down the channel, which means we are sending \"flattened\" (not pretty/ indented xml) to the device. This is important it seems! Some devices seme to not mind having the \"nicely\" formatted input (pretty xml). But! On devices that \"echo\" the inputs back -- sometimes the device will respond to our rpc without \"finishing\" echoing our inputs to the device, this breaks the core \"read until input\" processing that scrapli always does. For whatever reason if there are no line breaks this does not seem to happen? /shrug. Note that this comment applies to all of the \"pre\" methods that we parse a filter/payload! Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'get' operation. filter_type: {filter_type}, filter_: {filter_}\" ) # build base request and insert the get element xml_request = self._build_base_elem() xml_get_element = etree.fromstring(NetconfBaseOperations.GET.value) xml_request.insert(0, xml_get_element) # build filter element xml_filter_elem = self._build_filter(filter_=filter_, filter_type=filter_type) # insert filter element into parent get element get_element = xml_request.find(\"get\") get_element.insert(0, xml_filter_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'get' operation. Payload: {channel_input.decode()}\") return response def _pre_get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Handle pre \"get_config\" tasks for consistency between sync/async versions Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'get-config' operation. source: {source}, filter_type: \" f\"{filter_type}, filter: {filter_}, default_type: {default_type}\" ) self._validate_get_config_target(source=source) # build base request and insert the get-config element xml_request = self._build_base_elem() xml_get_config_element = etree.fromstring( NetconfBaseOperations.GET_CONFIG.value.format(source=source), parser=self.xml_parser ) xml_request.insert(0, xml_get_config_element) if filter_ is not None: xml_filter_elem = self._build_filter(filter_=filter_, filter_type=filter_type) # insert filter element into parent get element get_element = xml_request.find(\"get-config\") # insert *after* source, otherwise juniper seems to gripe, maybe/probably others as well get_element.insert(1, xml_filter_elem) if default_type is not None: xml_with_defaults_elem = self._build_with_defaults(default_type=default_type) get_element = xml_request.find(\"get-config\") get_element.insert(2, xml_with_defaults_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'get-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Handle pre \"edit_config\" tasks for consistency between sync/async versions Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'edit-config' operation. target: {target}, config: {config}\" ) self._validate_edit_config_target(target=target) xml_config = etree.fromstring(config, parser=self.xml_parser) # build base request and insert the edit-config element xml_request = self._build_base_elem() xml_edit_config_element = etree.fromstring( NetconfBaseOperations.EDIT_CONFIG.value.format(target=target) ) xml_request.insert(0, xml_edit_config_element) # insert parent filter element to first position so that target stays first just for nice # output/readability edit_config_element = xml_request.find(\"edit-config\") edit_config_element.insert(1, xml_config) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'edit-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_delete_config(self, target: str = \"running\") -> NetconfResponse: \"\"\" Handle pre \"edit_config\" tasks for consistency between sync/async versions Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(f\"Building payload for 'delete-config' operation. target: {target}\") self._validate_delete_config_target(target=target) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.DELETE_CONFIG.value.format(target=target), parser=self.xml_parser ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'delete-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_commit(self) -> NetconfResponse: \"\"\" Handle pre \"commit\" tasks for consistency between sync/async versions Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'commit' operation\") xml_request = self._build_base_elem() xml_commit_element = etree.fromstring( NetconfBaseOperations.COMMIT.value, parser=self.xml_parser ) xml_request.insert(0, xml_commit_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'commit' operation. Payload: {channel_input.decode()}\" ) return response def _pre_discard(self) -> NetconfResponse: \"\"\" Handle pre \"discard\" tasks for consistency between sync/async versions Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'discard' operation.\") xml_request = self._build_base_elem() xml_commit_element = etree.fromstring( NetconfBaseOperations.DISCARD.value, parser=self.xml_parser ) xml_request.insert(0, xml_commit_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'discard' operation. Payload: {channel_input.decode()}\" ) return response def _pre_lock(self, target: str) -> NetconfResponse: \"\"\" Handle pre \"lock\" tasks for consistency between sync/async versions Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'lock' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_lock_element = etree.fromstring( NetconfBaseOperations.LOCK.value.format(target=target), parser=self.xml_parser ) xml_request.insert(0, xml_lock_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'lock' operation. Payload: {channel_input.decode()}\") return response def _pre_unlock(self, target: str) -> NetconfResponse: \"\"\" Handle pre \"unlock\" tasks for consistency between sync/async versions Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'unlock' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_lock_element = etree.fromstring( NetconfBaseOperations.UNLOCK.value.format(target=target, parser=self.xml_parser) ) xml_request.insert(0, xml_lock_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'unlock' operation. Payload: {channel_input.decode()}\" ) return response def _pre_rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Handle pre \"rpc\" tasks for consistency between sync/async versions Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'rpc' operation.\") xml_request = self._build_base_elem() # build filter element if isinstance(filter_, str): xml_filter_elem = etree.fromstring(filter_, parser=self.xml_parser) else: xml_filter_elem = filter_ # insert filter element xml_request.insert(0, xml_filter_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'rpc' operation. Payload: {channel_input.decode()}\") return response def _pre_validate(self, source: str) -> NetconfResponse: \"\"\" Handle pre \"validate\" tasks for consistency between sync/async versions Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: CapabilityNotSupported: if 'validate' capability does not exist \"\"\" self.logger.debug(\"Building payload for 'validate' operation.\") if not any( cap in self.server_capabilities for cap in ( \"urn:ietf:params:netconf:capability:validate:1.0\", \"urn:ietf:params:netconf:capability:validate:1.1\", ) ): msg = \"validate requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) self._validate_edit_config_target(target=source) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.VALIDATE.value.format(source=source), parser=self.xml_parser ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'validate' operation. Payload: {channel_input.decode()}\" ) return response def _pre_copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Handle pre \"copy_config\" tasks for consistency between sync/async versions Note that source is not validated/checked since it could be a url or a full configuration element itself. Args: source: configuration, url, or datastore to copy into the target datastore target: copy config destination/target; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'copy_config' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.COPY_CONFIG.value.format(source=source, target=target), parser=self.xml_parser, ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'copy-config' operation. Payload: {channel_input.decode()}\" ) return response Classes \u00b6 NetconfBaseDriver \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 BaseDriver Object BaseDriver is the root for all Scrapli driver classes. The synchronous and asyncio driver base driver classes can be used to provide a semi-pexpect like experience over top of whatever transport a user prefers. Generally, however, the base driver classes should not be used directly. It is best to use the GenericDriver (or AsyncGenericDriver) or NetworkDriver (or AsyncNetworkDriver) sub-classes of the base drivers. Args: host: host ip/name to connect to port: port to connect to auth_username: username for authentication auth_private_key: path to private key for authentication auth_private_key_passphrase: passphrase for decrypting ssh key if necessary auth_password: password for authentication auth_strict_key: strict host checking or not auth_bypass: bypass \"in channel\" authentication -- only supported with telnet, asynctelnet, and system transport plugins timeout_socket: timeout for establishing socket/initial connection in seconds timeout_transport: timeout for ssh|telnet transport in seconds timeout_ops: timeout for ssh channel operations comms_prompt_pattern: raw string regex pattern -- preferably use `^` and `$` anchors! this is the single most important attribute here! if this does not match a prompt, scrapli will not work! IMPORTANT: regex search uses multi-line + case insensitive flags. multi-line allows for highly reliably matching for prompts however we do NOT strip trailing whitespace for each line, so be sure to add '\\\\s?' or similar if your device needs that. This should be mostly sorted for you if using network drivers (i.e. `IOSXEDriver`). Lastly, the case insensitive is just a convenience factor so i can be lazy. comms_return_char: character to use to send returns to host ssh_config_file: string to path for ssh config file, True to use default ssh config file or False to ignore default ssh config file ssh_known_hosts_file: string to path for ssh known hosts file, True to use default known file locations. Only applicable/needed if `auth_strict_key` is set to True on_init: callable that accepts the class instance as its only argument. this callable, if provided, is executed as the last step of object instantiation -- its purpose is primarily to provide a mechanism for scrapli community platforms to have an easy way to modify initialization arguments/object attributes without needing to create a class that extends the driver, instead allowing the community platforms to simply build from the GenericDriver or NetworkDriver classes, and pass this callable to do things such as appending to a username (looking at you RouterOS!!). Note that this is *always* a synchronous function (even for asyncio drivers)! on_open: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately after authentication is completed. Common use cases for this callable would be to disable paging or accept any kind of banner message that prompts a user upon connection on_close: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately prior to closing the underlying transport. Common use cases for this callable would be to save configurations prior to exiting, or to logout properly to free up vtys or similar transport: name of the transport plugin to use for the actual telnet/ssh/netconf connection. Available \"core\" transports are: - system - telnet - asynctelnet - ssh2 - paramiko - asyncssh Please see relevant transport plugin section for details. Additionally third party transport plugins may be available. transport_options: dictionary of options to pass to selected transport class; see docs for given transport class for details of what to pass here channel_lock: True/False to lock the channel (threading.Lock/asyncio.Lock) during any channel operations, defaults to False channel_log: True/False or a string path to a file of where to write out channel logs -- these are not \"logs\" in the normal logging module sense, but only the output that is read from the channel. In other words, the output of the channel log should look similar to what you would see as a human connecting to a device channel_log_mode: \"write\"|\"append\", all other values will raise ValueError, does what it sounds like it should by setting the channel log to the provided mode logging_uid: unique identifier (string) to associate to log messages; useful if you have multiple connections to the same device (i.e. one console, one ssh, or one to each supervisor module, etc.) Returns: None Raises: N/A Expand source code class NetconfBaseDriver(BaseDriver): host: str readable_datastores: List[str] writeable_datastores: List[str] strip_namespaces: bool strict_datastores: bool flatten_input: bool _netconf_base_channel_args: NetconfBaseChannelArgs @property def netconf_version(self) -> NetconfVersion: \"\"\" Getter for 'netconf_version' attribute Args: N/A Returns: NetconfVersion: netconf_version enum Raises: N/A \"\"\" return self._netconf_base_channel_args.netconf_version @netconf_version.setter def netconf_version(self, value: NetconfVersion) -> None: \"\"\" Setter for 'netconf_version' attribute Args: value: NetconfVersion Returns: None Raises: ScrapliTypeError: if value is not of type NetconfVersion \"\"\" if not isinstance(value, NetconfVersion): raise ScrapliTypeError self.logger.debug(f\"setting 'netconf_version' value to '{value.value}'\") self._netconf_base_channel_args.netconf_version = value if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_0: self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" else: self._base_channel_args.comms_prompt_pattern = r\"^##$\" @property def client_capabilities(self) -> NetconfClientCapabilities: \"\"\" Getter for 'client_capabilities' attribute Args: N/A Returns: NetconfClientCapabilities: netconf client capabilities enum Raises: N/A \"\"\" return self._netconf_base_channel_args.client_capabilities @client_capabilities.setter def client_capabilities(self, value: NetconfClientCapabilities) -> None: \"\"\" Setter for 'client_capabilities' attribute Args: value: NetconfClientCapabilities value for client_capabilities Returns: None Raises: ScrapliTypeError: if value is not of type NetconfClientCapabilities \"\"\" if not isinstance(value, NetconfClientCapabilities): raise ScrapliTypeError self.logger.debug(f\"setting 'client_capabilities' value to '{value.value}'\") self._netconf_base_channel_args.client_capabilities = value @property def server_capabilities(self) -> List[str]: \"\"\" Getter for 'server_capabilities' attribute Args: N/A Returns: list: list of strings of server capabilities Raises: N/A \"\"\" return self._netconf_base_channel_args.server_capabilities or [] @server_capabilities.setter def server_capabilities(self, value: NetconfClientCapabilities) -> None: \"\"\" Setter for 'server_capabilities' attribute Args: value: list of strings of netconf server capabilities Returns: None Raises: ScrapliTypeError: if value is not of type list \"\"\" if not isinstance(value, list): raise ScrapliTypeError self.logger.debug(f\"setting 'server_capabilities' value to '{value}'\") self._netconf_base_channel_args.server_capabilities = value @staticmethod def _determine_preferred_netconf_version( preferred_netconf_version: Optional[str], ) -> NetconfVersion: \"\"\" Determine users preferred netconf version (if applicable) Args: preferred_netconf_version: optional string indicating users preferred netconf version Returns: NetconfVersion: users preferred netconf version Raises: ScrapliValueError: if preferred_netconf_version is not None or a valid option \"\"\" if preferred_netconf_version is None: return NetconfVersion.UNKNOWN if preferred_netconf_version == \"1.0\": return NetconfVersion.VERSION_1_0 if preferred_netconf_version == \"1.1\": return NetconfVersion.VERSION_1_1 raise ScrapliValueError( \"'preferred_netconf_version' provided with invalid value, must be one of: \" \"None, '1.0', or '1.1'\" ) @staticmethod def _determine_preferred_xml_parser(use_compressed_parser: bool) -> XmlParserVersion: \"\"\" Determine users preferred xml payload parser Args: use_compressed_parser: bool indicating use of compressed parser or not Returns: XmlParserVersion: users xml parser version Raises: N/A \"\"\" if use_compressed_parser is True: return XmlParserVersion.COMPRESSED_PARSER return XmlParserVersion.STANDARD_PARSER @property def xml_parser(self) -> etree.XMLParser: \"\"\" Getter for 'xml_parser' attribute Args: N/A Returns: etree.XMLParser: parser to use for parsing xml documents Raises: N/A \"\"\" if self._netconf_base_channel_args.xml_parser == XmlParserVersion.COMPRESSED_PARSER: return COMPRESSED_PARSER return STANDARD_PARSER @xml_parser.setter def xml_parser(self, value: XmlParserVersion) -> None: \"\"\" Setter for 'xml_parser' attribute Args: value: enum indicating parser version to use Returns: None Raises: ScrapliTypeError: if value is not of type XmlParserVersion \"\"\" if not isinstance(value, XmlParserVersion): raise ScrapliTypeError self._netconf_base_channel_args.xml_parser = value def _transport_factory(self) -> Tuple[Callable[..., Any], object]: \"\"\" Determine proper transport class and necessary arguments to initialize that class Args: N/A Returns: Tuple[Callable[..., Any], object]: tuple of transport class and dataclass of transport class specific arguments Raises: N/A \"\"\" transport_plugin_module = importlib.import_module( f\"scrapli_netconf.transport.plugins.{self.transport_name}.transport\" ) transport_class = getattr( transport_plugin_module, f\"Netconf{self.transport_name.capitalize()}Transport\" ) plugin_transport_args_class = getattr(transport_plugin_module, \"PluginTransportArgs\") _plugin_transport_args = { field.name: getattr(self, field.name) for field in fields(plugin_transport_args_class) } plugin_transport_args = plugin_transport_args_class(**_plugin_transport_args) return transport_class, plugin_transport_args def _build_readable_datastores(self) -> None: \"\"\" Build a list of readable datastores based on server's advertised capabilities Args: N/A Returns: None Raises: N/A \"\"\" self.readable_datastores = [] self.readable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:candidate:1.0\" in self.server_capabilities: self.readable_datastores.append(\"candidate\") if \"urn:ietf:params:netconf:capability:startup:1.0\" in self.server_capabilities: self.readable_datastores.append(\"startup\") def _build_writeable_datastores(self) -> None: \"\"\" Build a list of writeable/editable datastores based on server's advertised capabilities Args: N/A Returns: None Raises: N/A \"\"\" self.writeable_datastores = [] if \"urn:ietf:params:netconf:capability:writeable-running:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:writable-running:1.0\" in self.server_capabilities: # NOTE: iosxe shows \"writable\" (as of 2020.07.01) despite RFC being \"writeable\" self.writeable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:candidate:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"candidate\") if \"urn:ietf:params:netconf:capability:startup:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"startup\") def _validate_get_config_target(self, source: str) -> None: \"\"\" Validate get-config source is acceptable Args: source: configuration source to get; typically one of running|startup|candidate Returns: None Raises: ScrapliValueError: if an invalid source was selected and strict_datastores is True \"\"\" if source not in self.readable_datastores: msg = f\"'source' should be one of {self.readable_datastores}, got '{source}'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore source!\", message=msg) def _validate_edit_config_target(self, target: str) -> None: \"\"\" Validate edit-config/lock/unlock target is acceptable Args: target: configuration source to edit/lock; typically one of running|startup|candidate Returns: None Raises: ScrapliValueError: if an invalid source was selected \"\"\" if target not in self.writeable_datastores: msg = f\"'target' should be one of {self.writeable_datastores}, got '{target}'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore target!\", message=msg) def _validate_delete_config_target(self, target: str) -> None: \"\"\" Validate delete-config/lock/unlock target is acceptable Args: target: configuration source to delete; typically one of startup|candidate Returns: None Raises: ScrapliValueError: if an invalid target was selected \"\"\" if target == \"running\" or target not in self.writeable_datastores: msg = f\"'target' should be one of {self.writeable_datastores}, got '{target}'\" if target == \"running\": msg = \"delete-config 'target' may not be 'running'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore target!\", message=msg) def _build_base_elem(self) -> _Element: \"\"\" Create base element for netconf operations Args: N/A Returns: _Element: lxml base element to use for netconf operation Raises: N/A \"\"\" # pylint did not seem to want to be ok with assigning this as a class attribute... and its # only used here so... here we are self.message_id: int # pylint: disable=W0201 self.logger.debug(f\"Building base element for message id {self.message_id}\") base_xml_str = NetconfBaseOperations.RPC.value.format(message_id=self.message_id) self.message_id += 1 base_elem = etree.fromstring(text=base_xml_str) return base_elem def _build_filter(self, filter_: str, filter_type: str = \"subtree\") -> _Element: \"\"\" Create filter element for a given rpc The `filter_` string may contain multiple xml elements at its \"root\" (subtree filters); we will simply place the payload into a temporary \"tmp\" outer tag so that when we cast it to an etree object the elements are all preserved; without this outer \"tmp\" tag, lxml will scoop up only the first element provided as it appears to be the root of the document presumably. An example valid (to scrapli netconf at least) xml filter would be: 1 2 3 4 5 6 7 <interface-configurations xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-ifmgr-cfg\"> <interface-configuration> <active>act</active> </interface-configuration> </interface-configurations> <netconf-yang xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-man-netconf-cfg\"> </netconf-yang> Args: filter_: strings of filters to build into a filter element or (for subtree) a full filter string (in filter tags) filter_type: type of filter; subtree|xpath Returns: _Element: lxml filter element to use for netconf operation Raises: CapabilityNotSupported: if xpath selected and not supported on server ScrapliValueError: if filter_type is not one of subtree|xpath \"\"\" if filter_type == \"subtree\": # tmp tags to place the users kinda not valid xml filter into _filter_ = f\" {filter_} \" # \"validate\" subtree filter by forcing it into xml, parser \"flattens\" it as well tmp_xml_filter_element = etree.fromstring(_filter_, parser=self.xml_parser) if tmp_xml_filter_element.getchildren()[0].tag == \"filter\": # if the user filter was already wrapped in filter tags we'll end up here, we will # blindly reuse the users filter but we'll make sure that the filter \"type\" is set xml_filter_elem = tmp_xml_filter_element.getchildren()[0] xml_filter_elem.attrib[\"type\"] = \"subtree\" else: xml_filter_elem = etree.fromstring( NetconfBaseOperations.FILTER_SUBTREE.value.format(filter_type=filter_type), ) # iterate through the children inside the tmp tags and insert *those* elements into # the actual final filter payload for xml_filter_element in tmp_xml_filter_element: # insert the subtree filter into the parent filter element xml_filter_elem.insert(1, xml_filter_element) elif filter_type == \"xpath\": if \"urn:ietf:params:netconf:capability:xpath:1.0\" not in self.server_capabilities: msg = \"xpath filter requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) xml_filter_elem = etree.fromstring( NetconfBaseOperations.FILTER_XPATH.value.format( filter_type=filter_type, xpath=filter_ ), parser=self.xml_parser, ) else: raise ScrapliValueError( f\"'filter_type' should be one of subtree|xpath, got '{filter_type}'\" ) return xml_filter_elem def _build_with_defaults(self, default_type: str = \"report-all\") -> _Element: \"\"\" Create with-defaults element for a given operation Args: default_type: enumeration of with-defaults; report-all|trim|explicit|report-all-tagged Returns: _Element: lxml with-defaults element to use for netconf operation Raises: CapabilityNotSupported: if default_type provided but not supported by device ScrapliValueError: if default_type is not one of report-all|trim|explicit|report-all-tagged \"\"\" if default_type in [\"report-all\", \"trim\", \"explicit\", \"report-all-tagged\"]: if ( \"urn:ietf:params:netconf:capability:with-defaults:1.0\" not in self.server_capabilities ): msg = \"with-defaults requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) xml_with_defaults_element = etree.fromstring( NetconfBaseOperations.WITH_DEFAULTS_SUBTREE.value.format(default_type=default_type), parser=self.xml_parser, ) else: raise ScrapliValueError( \"'default_type' should be one of report-all|trim|explicit|report-all-tagged, \" f\"got '{default_type}'\" ) return xml_with_defaults_element def _finalize_channel_input(self, xml_request: _Element) -> bytes: \"\"\" Create finalized channel input (as bytes) Args: xml_request: finalized xml element to cast to bytes and add declaration to Returns: bytes: finalized bytes input -- with 1.0 delimiter or 1.1 encoding Raises: N/A \"\"\" channel_input: bytes = etree.tostring( element_or_tree=xml_request, xml_declaration=True, encoding=\"utf-8\" ) if self.netconf_version == NetconfVersion.VERSION_1_0: channel_input = channel_input + b\"]]>]]>\" else: # format message for chunk (netconf 1.1) style message channel_input = b\"#%b\\n\" % str(len(channel_input)).encode() + channel_input + b\"\\n##\" return channel_input def _pre_get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Handle pre \"get\" tasks for consistency between sync/async versions *NOTE* The channel input (filter_) is loaded up as an lxml etree element here, this is done with a parser that removes whitespace. This has a somewhat undesirable effect of making any \"pretty\" input not pretty, however... after we load the xml object (which we do to validate that it is valid xml) we dump that xml object back to a string to be used as the actual raw payload we send down the channel, which means we are sending \"flattened\" (not pretty/ indented xml) to the device. This is important it seems! Some devices seme to not mind having the \"nicely\" formatted input (pretty xml). But! On devices that \"echo\" the inputs back -- sometimes the device will respond to our rpc without \"finishing\" echoing our inputs to the device, this breaks the core \"read until input\" processing that scrapli always does. For whatever reason if there are no line breaks this does not seem to happen? /shrug. Note that this comment applies to all of the \"pre\" methods that we parse a filter/payload! Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'get' operation. filter_type: {filter_type}, filter_: {filter_}\" ) # build base request and insert the get element xml_request = self._build_base_elem() xml_get_element = etree.fromstring(NetconfBaseOperations.GET.value) xml_request.insert(0, xml_get_element) # build filter element xml_filter_elem = self._build_filter(filter_=filter_, filter_type=filter_type) # insert filter element into parent get element get_element = xml_request.find(\"get\") get_element.insert(0, xml_filter_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'get' operation. Payload: {channel_input.decode()}\") return response def _pre_get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Handle pre \"get_config\" tasks for consistency between sync/async versions Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'get-config' operation. source: {source}, filter_type: \" f\"{filter_type}, filter: {filter_}, default_type: {default_type}\" ) self._validate_get_config_target(source=source) # build base request and insert the get-config element xml_request = self._build_base_elem() xml_get_config_element = etree.fromstring( NetconfBaseOperations.GET_CONFIG.value.format(source=source), parser=self.xml_parser ) xml_request.insert(0, xml_get_config_element) if filter_ is not None: xml_filter_elem = self._build_filter(filter_=filter_, filter_type=filter_type) # insert filter element into parent get element get_element = xml_request.find(\"get-config\") # insert *after* source, otherwise juniper seems to gripe, maybe/probably others as well get_element.insert(1, xml_filter_elem) if default_type is not None: xml_with_defaults_elem = self._build_with_defaults(default_type=default_type) get_element = xml_request.find(\"get-config\") get_element.insert(2, xml_with_defaults_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'get-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Handle pre \"edit_config\" tasks for consistency between sync/async versions Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'edit-config' operation. target: {target}, config: {config}\" ) self._validate_edit_config_target(target=target) xml_config = etree.fromstring(config, parser=self.xml_parser) # build base request and insert the edit-config element xml_request = self._build_base_elem() xml_edit_config_element = etree.fromstring( NetconfBaseOperations.EDIT_CONFIG.value.format(target=target) ) xml_request.insert(0, xml_edit_config_element) # insert parent filter element to first position so that target stays first just for nice # output/readability edit_config_element = xml_request.find(\"edit-config\") edit_config_element.insert(1, xml_config) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'edit-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_delete_config(self, target: str = \"running\") -> NetconfResponse: \"\"\" Handle pre \"edit_config\" tasks for consistency between sync/async versions Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(f\"Building payload for 'delete-config' operation. target: {target}\") self._validate_delete_config_target(target=target) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.DELETE_CONFIG.value.format(target=target), parser=self.xml_parser ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'delete-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_commit(self) -> NetconfResponse: \"\"\" Handle pre \"commit\" tasks for consistency between sync/async versions Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'commit' operation\") xml_request = self._build_base_elem() xml_commit_element = etree.fromstring( NetconfBaseOperations.COMMIT.value, parser=self.xml_parser ) xml_request.insert(0, xml_commit_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'commit' operation. Payload: {channel_input.decode()}\" ) return response def _pre_discard(self) -> NetconfResponse: \"\"\" Handle pre \"discard\" tasks for consistency between sync/async versions Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'discard' operation.\") xml_request = self._build_base_elem() xml_commit_element = etree.fromstring( NetconfBaseOperations.DISCARD.value, parser=self.xml_parser ) xml_request.insert(0, xml_commit_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'discard' operation. Payload: {channel_input.decode()}\" ) return response def _pre_lock(self, target: str) -> NetconfResponse: \"\"\" Handle pre \"lock\" tasks for consistency between sync/async versions Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'lock' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_lock_element = etree.fromstring( NetconfBaseOperations.LOCK.value.format(target=target), parser=self.xml_parser ) xml_request.insert(0, xml_lock_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'lock' operation. Payload: {channel_input.decode()}\") return response def _pre_unlock(self, target: str) -> NetconfResponse: \"\"\" Handle pre \"unlock\" tasks for consistency between sync/async versions Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'unlock' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_lock_element = etree.fromstring( NetconfBaseOperations.UNLOCK.value.format(target=target, parser=self.xml_parser) ) xml_request.insert(0, xml_lock_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'unlock' operation. Payload: {channel_input.decode()}\" ) return response def _pre_rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Handle pre \"rpc\" tasks for consistency between sync/async versions Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'rpc' operation.\") xml_request = self._build_base_elem() # build filter element if isinstance(filter_, str): xml_filter_elem = etree.fromstring(filter_, parser=self.xml_parser) else: xml_filter_elem = filter_ # insert filter element xml_request.insert(0, xml_filter_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'rpc' operation. Payload: {channel_input.decode()}\") return response def _pre_validate(self, source: str) -> NetconfResponse: \"\"\" Handle pre \"validate\" tasks for consistency between sync/async versions Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: CapabilityNotSupported: if 'validate' capability does not exist \"\"\" self.logger.debug(\"Building payload for 'validate' operation.\") if not any( cap in self.server_capabilities for cap in ( \"urn:ietf:params:netconf:capability:validate:1.0\", \"urn:ietf:params:netconf:capability:validate:1.1\", ) ): msg = \"validate requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) self._validate_edit_config_target(target=source) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.VALIDATE.value.format(source=source), parser=self.xml_parser ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'validate' operation. Payload: {channel_input.decode()}\" ) return response def _pre_copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Handle pre \"copy_config\" tasks for consistency between sync/async versions Note that source is not validated/checked since it could be a url or a full configuration element itself. Args: source: configuration, url, or datastore to copy into the target datastore target: copy config destination/target; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'copy_config' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.COPY_CONFIG.value.format(source=source, target=target), parser=self.xml_parser, ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'copy-config' operation. Payload: {channel_input.decode()}\" ) return response Ancestors (in MRO) \u00b6 scrapli.driver.base.base_driver.BaseDriver Descendants \u00b6 scrapli_netconf.driver.async_driver.AsyncNetconfDriver scrapli_netconf.driver.sync_driver.NetconfDriver Class variables \u00b6 flatten_input: bool host: str readable_datastores: List[str] strict_datastores: bool strip_namespaces: bool writeable_datastores: List[str] Instance variables \u00b6 client_capabilities: scrapli_netconf.constants.NetconfClientCapabilities 1 2 3 4 5 6 7 8 9 10 Getter for 'client_capabilities' attribute Args: N/A Returns: NetconfClientCapabilities: netconf client capabilities enum Raises: N/A netconf_version: scrapli_netconf.constants.NetconfVersion 1 2 3 4 5 6 7 8 9 10 Getter for 'netconf_version' attribute Args: N/A Returns: NetconfVersion: netconf_version enum Raises: N/A server_capabilities: List[str] 1 2 3 4 5 6 7 8 9 10 Getter for 'server_capabilities' attribute Args: N/A Returns: list: list of strings of server capabilities Raises: N/A xml_parser: lxml.etree.XMLParser 1 2 3 4 5 6 7 8 9 10 Getter for 'xml_parser' attribute Args: N/A Returns: etree.XMLParser: parser to use for parsing xml documents Raises: N/A NetconfBaseOperations \u00b6 1 An enumeration. Expand source code class NetconfBaseOperations(Enum): FILTER_SUBTREE = \" \" FILTER_XPATH = \" \" WITH_DEFAULTS_SUBTREE = ( \" \" \"{default_type} \" ) GET = \" \" GET_CONFIG = \" < {source}/> \" EDIT_CONFIG = \" < {target}/> \" DELETE_CONFIG = \" < {target}/> \" COPY_CONFIG = ( \" < {target}/> < {source}/> \" ) COMMIT = \" \" DISCARD = \" \" LOCK = \" < {target}/> \" UNLOCK = \" < {target}/> \" RPC = \" \" VALIDATE = \" < {source}/> \" Ancestors (in MRO) \u00b6 enum.Enum Class variables \u00b6 COMMIT COPY_CONFIG DELETE_CONFIG DISCARD EDIT_CONFIG FILTER_SUBTREE FILTER_XPATH GET GET_CONFIG LOCK RPC UNLOCK VALIDATE WITH_DEFAULTS_SUBTREE","title":"Base Driver"},{"location":"api_docs/driver/base_driver/#module-scrapli_netconfdriverbase_driver","text":"scrapli_netconf.driver.base_driver Expand source code # pylint: disable=C0302 \"\"\"scrapli_netconf.driver.base_driver\"\"\" import importlib from dataclasses import fields from enum import Enum from typing import Any, Callable, List, Optional, Tuple, Union from lxml import etree from lxml.etree import _Element from scrapli.driver.base.base_driver import BaseDriver from scrapli.exceptions import ScrapliTypeError, ScrapliValueError from scrapli.helper import user_warning from scrapli_netconf.channel.base_channel import NetconfBaseChannelArgs from scrapli_netconf.constants import NetconfClientCapabilities, NetconfVersion, XmlParserVersion from scrapli_netconf.exceptions import CapabilityNotSupported from scrapli_netconf.response import NetconfResponse COMPRESSED_PARSER = etree.XMLParser(remove_blank_text=True, recover=True) STANDARD_PARSER = etree.XMLParser(remove_blank_text=False, recover=True) class NetconfBaseOperations(Enum): FILTER_SUBTREE = \" \" FILTER_XPATH = \" \" WITH_DEFAULTS_SUBTREE = ( \" \" \"{default_type} \" ) GET = \" \" GET_CONFIG = \" < {source}/> \" EDIT_CONFIG = \" < {target}/> \" DELETE_CONFIG = \" < {target}/> \" COPY_CONFIG = ( \" < {target}/> < {source}/> \" ) COMMIT = \" \" DISCARD = \" \" LOCK = \" < {target}/> \" UNLOCK = \" < {target}/> \" RPC = \" \" VALIDATE = \" < {source}/> \" class NetconfBaseDriver(BaseDriver): host: str readable_datastores: List[str] writeable_datastores: List[str] strip_namespaces: bool strict_datastores: bool flatten_input: bool _netconf_base_channel_args: NetconfBaseChannelArgs @property def netconf_version(self) -> NetconfVersion: \"\"\" Getter for 'netconf_version' attribute Args: N/A Returns: NetconfVersion: netconf_version enum Raises: N/A \"\"\" return self._netconf_base_channel_args.netconf_version @netconf_version.setter def netconf_version(self, value: NetconfVersion) -> None: \"\"\" Setter for 'netconf_version' attribute Args: value: NetconfVersion Returns: None Raises: ScrapliTypeError: if value is not of type NetconfVersion \"\"\" if not isinstance(value, NetconfVersion): raise ScrapliTypeError self.logger.debug(f\"setting 'netconf_version' value to '{value.value}'\") self._netconf_base_channel_args.netconf_version = value if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_0: self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" else: self._base_channel_args.comms_prompt_pattern = r\"^##$\" @property def client_capabilities(self) -> NetconfClientCapabilities: \"\"\" Getter for 'client_capabilities' attribute Args: N/A Returns: NetconfClientCapabilities: netconf client capabilities enum Raises: N/A \"\"\" return self._netconf_base_channel_args.client_capabilities @client_capabilities.setter def client_capabilities(self, value: NetconfClientCapabilities) -> None: \"\"\" Setter for 'client_capabilities' attribute Args: value: NetconfClientCapabilities value for client_capabilities Returns: None Raises: ScrapliTypeError: if value is not of type NetconfClientCapabilities \"\"\" if not isinstance(value, NetconfClientCapabilities): raise ScrapliTypeError self.logger.debug(f\"setting 'client_capabilities' value to '{value.value}'\") self._netconf_base_channel_args.client_capabilities = value @property def server_capabilities(self) -> List[str]: \"\"\" Getter for 'server_capabilities' attribute Args: N/A Returns: list: list of strings of server capabilities Raises: N/A \"\"\" return self._netconf_base_channel_args.server_capabilities or [] @server_capabilities.setter def server_capabilities(self, value: NetconfClientCapabilities) -> None: \"\"\" Setter for 'server_capabilities' attribute Args: value: list of strings of netconf server capabilities Returns: None Raises: ScrapliTypeError: if value is not of type list \"\"\" if not isinstance(value, list): raise ScrapliTypeError self.logger.debug(f\"setting 'server_capabilities' value to '{value}'\") self._netconf_base_channel_args.server_capabilities = value @staticmethod def _determine_preferred_netconf_version( preferred_netconf_version: Optional[str], ) -> NetconfVersion: \"\"\" Determine users preferred netconf version (if applicable) Args: preferred_netconf_version: optional string indicating users preferred netconf version Returns: NetconfVersion: users preferred netconf version Raises: ScrapliValueError: if preferred_netconf_version is not None or a valid option \"\"\" if preferred_netconf_version is None: return NetconfVersion.UNKNOWN if preferred_netconf_version == \"1.0\": return NetconfVersion.VERSION_1_0 if preferred_netconf_version == \"1.1\": return NetconfVersion.VERSION_1_1 raise ScrapliValueError( \"'preferred_netconf_version' provided with invalid value, must be one of: \" \"None, '1.0', or '1.1'\" ) @staticmethod def _determine_preferred_xml_parser(use_compressed_parser: bool) -> XmlParserVersion: \"\"\" Determine users preferred xml payload parser Args: use_compressed_parser: bool indicating use of compressed parser or not Returns: XmlParserVersion: users xml parser version Raises: N/A \"\"\" if use_compressed_parser is True: return XmlParserVersion.COMPRESSED_PARSER return XmlParserVersion.STANDARD_PARSER @property def xml_parser(self) -> etree.XMLParser: \"\"\" Getter for 'xml_parser' attribute Args: N/A Returns: etree.XMLParser: parser to use for parsing xml documents Raises: N/A \"\"\" if self._netconf_base_channel_args.xml_parser == XmlParserVersion.COMPRESSED_PARSER: return COMPRESSED_PARSER return STANDARD_PARSER @xml_parser.setter def xml_parser(self, value: XmlParserVersion) -> None: \"\"\" Setter for 'xml_parser' attribute Args: value: enum indicating parser version to use Returns: None Raises: ScrapliTypeError: if value is not of type XmlParserVersion \"\"\" if not isinstance(value, XmlParserVersion): raise ScrapliTypeError self._netconf_base_channel_args.xml_parser = value def _transport_factory(self) -> Tuple[Callable[..., Any], object]: \"\"\" Determine proper transport class and necessary arguments to initialize that class Args: N/A Returns: Tuple[Callable[..., Any], object]: tuple of transport class and dataclass of transport class specific arguments Raises: N/A \"\"\" transport_plugin_module = importlib.import_module( f\"scrapli_netconf.transport.plugins.{self.transport_name}.transport\" ) transport_class = getattr( transport_plugin_module, f\"Netconf{self.transport_name.capitalize()}Transport\" ) plugin_transport_args_class = getattr(transport_plugin_module, \"PluginTransportArgs\") _plugin_transport_args = { field.name: getattr(self, field.name) for field in fields(plugin_transport_args_class) } plugin_transport_args = plugin_transport_args_class(**_plugin_transport_args) return transport_class, plugin_transport_args def _build_readable_datastores(self) -> None: \"\"\" Build a list of readable datastores based on server's advertised capabilities Args: N/A Returns: None Raises: N/A \"\"\" self.readable_datastores = [] self.readable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:candidate:1.0\" in self.server_capabilities: self.readable_datastores.append(\"candidate\") if \"urn:ietf:params:netconf:capability:startup:1.0\" in self.server_capabilities: self.readable_datastores.append(\"startup\") def _build_writeable_datastores(self) -> None: \"\"\" Build a list of writeable/editable datastores based on server's advertised capabilities Args: N/A Returns: None Raises: N/A \"\"\" self.writeable_datastores = [] if \"urn:ietf:params:netconf:capability:writeable-running:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:writable-running:1.0\" in self.server_capabilities: # NOTE: iosxe shows \"writable\" (as of 2020.07.01) despite RFC being \"writeable\" self.writeable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:candidate:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"candidate\") if \"urn:ietf:params:netconf:capability:startup:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"startup\") def _validate_get_config_target(self, source: str) -> None: \"\"\" Validate get-config source is acceptable Args: source: configuration source to get; typically one of running|startup|candidate Returns: None Raises: ScrapliValueError: if an invalid source was selected and strict_datastores is True \"\"\" if source not in self.readable_datastores: msg = f\"'source' should be one of {self.readable_datastores}, got '{source}'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore source!\", message=msg) def _validate_edit_config_target(self, target: str) -> None: \"\"\" Validate edit-config/lock/unlock target is acceptable Args: target: configuration source to edit/lock; typically one of running|startup|candidate Returns: None Raises: ScrapliValueError: if an invalid source was selected \"\"\" if target not in self.writeable_datastores: msg = f\"'target' should be one of {self.writeable_datastores}, got '{target}'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore target!\", message=msg) def _validate_delete_config_target(self, target: str) -> None: \"\"\" Validate delete-config/lock/unlock target is acceptable Args: target: configuration source to delete; typically one of startup|candidate Returns: None Raises: ScrapliValueError: if an invalid target was selected \"\"\" if target == \"running\" or target not in self.writeable_datastores: msg = f\"'target' should be one of {self.writeable_datastores}, got '{target}'\" if target == \"running\": msg = \"delete-config 'target' may not be 'running'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore target!\", message=msg) def _build_base_elem(self) -> _Element: \"\"\" Create base element for netconf operations Args: N/A Returns: _Element: lxml base element to use for netconf operation Raises: N/A \"\"\" # pylint did not seem to want to be ok with assigning this as a class attribute... and its # only used here so... here we are self.message_id: int # pylint: disable=W0201 self.logger.debug(f\"Building base element for message id {self.message_id}\") base_xml_str = NetconfBaseOperations.RPC.value.format(message_id=self.message_id) self.message_id += 1 base_elem = etree.fromstring(text=base_xml_str) return base_elem def _build_filter(self, filter_: str, filter_type: str = \"subtree\") -> _Element: \"\"\" Create filter element for a given rpc The `filter_` string may contain multiple xml elements at its \"root\" (subtree filters); we will simply place the payload into a temporary \"tmp\" outer tag so that when we cast it to an etree object the elements are all preserved; without this outer \"tmp\" tag, lxml will scoop up only the first element provided as it appears to be the root of the document presumably. An example valid (to scrapli netconf at least) xml filter would be: 1 2 3 4 5 6 7 <interface-configurations xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-ifmgr-cfg\"> <interface-configuration> <active>act</active> </interface-configuration> </interface-configurations> <netconf-yang xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-man-netconf-cfg\"> </netconf-yang> Args: filter_: strings of filters to build into a filter element or (for subtree) a full filter string (in filter tags) filter_type: type of filter; subtree|xpath Returns: _Element: lxml filter element to use for netconf operation Raises: CapabilityNotSupported: if xpath selected and not supported on server ScrapliValueError: if filter_type is not one of subtree|xpath \"\"\" if filter_type == \"subtree\": # tmp tags to place the users kinda not valid xml filter into _filter_ = f\" {filter_} \" # \"validate\" subtree filter by forcing it into xml, parser \"flattens\" it as well tmp_xml_filter_element = etree.fromstring(_filter_, parser=self.xml_parser) if tmp_xml_filter_element.getchildren()[0].tag == \"filter\": # if the user filter was already wrapped in filter tags we'll end up here, we will # blindly reuse the users filter but we'll make sure that the filter \"type\" is set xml_filter_elem = tmp_xml_filter_element.getchildren()[0] xml_filter_elem.attrib[\"type\"] = \"subtree\" else: xml_filter_elem = etree.fromstring( NetconfBaseOperations.FILTER_SUBTREE.value.format(filter_type=filter_type), ) # iterate through the children inside the tmp tags and insert *those* elements into # the actual final filter payload for xml_filter_element in tmp_xml_filter_element: # insert the subtree filter into the parent filter element xml_filter_elem.insert(1, xml_filter_element) elif filter_type == \"xpath\": if \"urn:ietf:params:netconf:capability:xpath:1.0\" not in self.server_capabilities: msg = \"xpath filter requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) xml_filter_elem = etree.fromstring( NetconfBaseOperations.FILTER_XPATH.value.format( filter_type=filter_type, xpath=filter_ ), parser=self.xml_parser, ) else: raise ScrapliValueError( f\"'filter_type' should be one of subtree|xpath, got '{filter_type}'\" ) return xml_filter_elem def _build_with_defaults(self, default_type: str = \"report-all\") -> _Element: \"\"\" Create with-defaults element for a given operation Args: default_type: enumeration of with-defaults; report-all|trim|explicit|report-all-tagged Returns: _Element: lxml with-defaults element to use for netconf operation Raises: CapabilityNotSupported: if default_type provided but not supported by device ScrapliValueError: if default_type is not one of report-all|trim|explicit|report-all-tagged \"\"\" if default_type in [\"report-all\", \"trim\", \"explicit\", \"report-all-tagged\"]: if ( \"urn:ietf:params:netconf:capability:with-defaults:1.0\" not in self.server_capabilities ): msg = \"with-defaults requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) xml_with_defaults_element = etree.fromstring( NetconfBaseOperations.WITH_DEFAULTS_SUBTREE.value.format(default_type=default_type), parser=self.xml_parser, ) else: raise ScrapliValueError( \"'default_type' should be one of report-all|trim|explicit|report-all-tagged, \" f\"got '{default_type}'\" ) return xml_with_defaults_element def _finalize_channel_input(self, xml_request: _Element) -> bytes: \"\"\" Create finalized channel input (as bytes) Args: xml_request: finalized xml element to cast to bytes and add declaration to Returns: bytes: finalized bytes input -- with 1.0 delimiter or 1.1 encoding Raises: N/A \"\"\" channel_input: bytes = etree.tostring( element_or_tree=xml_request, xml_declaration=True, encoding=\"utf-8\" ) if self.netconf_version == NetconfVersion.VERSION_1_0: channel_input = channel_input + b\"]]>]]>\" else: # format message for chunk (netconf 1.1) style message channel_input = b\"#%b\\n\" % str(len(channel_input)).encode() + channel_input + b\"\\n##\" return channel_input def _pre_get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Handle pre \"get\" tasks for consistency between sync/async versions *NOTE* The channel input (filter_) is loaded up as an lxml etree element here, this is done with a parser that removes whitespace. This has a somewhat undesirable effect of making any \"pretty\" input not pretty, however... after we load the xml object (which we do to validate that it is valid xml) we dump that xml object back to a string to be used as the actual raw payload we send down the channel, which means we are sending \"flattened\" (not pretty/ indented xml) to the device. This is important it seems! Some devices seme to not mind having the \"nicely\" formatted input (pretty xml). But! On devices that \"echo\" the inputs back -- sometimes the device will respond to our rpc without \"finishing\" echoing our inputs to the device, this breaks the core \"read until input\" processing that scrapli always does. For whatever reason if there are no line breaks this does not seem to happen? /shrug. Note that this comment applies to all of the \"pre\" methods that we parse a filter/payload! Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'get' operation. filter_type: {filter_type}, filter_: {filter_}\" ) # build base request and insert the get element xml_request = self._build_base_elem() xml_get_element = etree.fromstring(NetconfBaseOperations.GET.value) xml_request.insert(0, xml_get_element) # build filter element xml_filter_elem = self._build_filter(filter_=filter_, filter_type=filter_type) # insert filter element into parent get element get_element = xml_request.find(\"get\") get_element.insert(0, xml_filter_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'get' operation. Payload: {channel_input.decode()}\") return response def _pre_get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Handle pre \"get_config\" tasks for consistency between sync/async versions Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'get-config' operation. source: {source}, filter_type: \" f\"{filter_type}, filter: {filter_}, default_type: {default_type}\" ) self._validate_get_config_target(source=source) # build base request and insert the get-config element xml_request = self._build_base_elem() xml_get_config_element = etree.fromstring( NetconfBaseOperations.GET_CONFIG.value.format(source=source), parser=self.xml_parser ) xml_request.insert(0, xml_get_config_element) if filter_ is not None: xml_filter_elem = self._build_filter(filter_=filter_, filter_type=filter_type) # insert filter element into parent get element get_element = xml_request.find(\"get-config\") # insert *after* source, otherwise juniper seems to gripe, maybe/probably others as well get_element.insert(1, xml_filter_elem) if default_type is not None: xml_with_defaults_elem = self._build_with_defaults(default_type=default_type) get_element = xml_request.find(\"get-config\") get_element.insert(2, xml_with_defaults_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'get-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Handle pre \"edit_config\" tasks for consistency between sync/async versions Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'edit-config' operation. target: {target}, config: {config}\" ) self._validate_edit_config_target(target=target) xml_config = etree.fromstring(config, parser=self.xml_parser) # build base request and insert the edit-config element xml_request = self._build_base_elem() xml_edit_config_element = etree.fromstring( NetconfBaseOperations.EDIT_CONFIG.value.format(target=target) ) xml_request.insert(0, xml_edit_config_element) # insert parent filter element to first position so that target stays first just for nice # output/readability edit_config_element = xml_request.find(\"edit-config\") edit_config_element.insert(1, xml_config) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'edit-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_delete_config(self, target: str = \"running\") -> NetconfResponse: \"\"\" Handle pre \"edit_config\" tasks for consistency between sync/async versions Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(f\"Building payload for 'delete-config' operation. target: {target}\") self._validate_delete_config_target(target=target) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.DELETE_CONFIG.value.format(target=target), parser=self.xml_parser ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'delete-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_commit(self) -> NetconfResponse: \"\"\" Handle pre \"commit\" tasks for consistency between sync/async versions Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'commit' operation\") xml_request = self._build_base_elem() xml_commit_element = etree.fromstring( NetconfBaseOperations.COMMIT.value, parser=self.xml_parser ) xml_request.insert(0, xml_commit_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'commit' operation. Payload: {channel_input.decode()}\" ) return response def _pre_discard(self) -> NetconfResponse: \"\"\" Handle pre \"discard\" tasks for consistency between sync/async versions Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'discard' operation.\") xml_request = self._build_base_elem() xml_commit_element = etree.fromstring( NetconfBaseOperations.DISCARD.value, parser=self.xml_parser ) xml_request.insert(0, xml_commit_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'discard' operation. Payload: {channel_input.decode()}\" ) return response def _pre_lock(self, target: str) -> NetconfResponse: \"\"\" Handle pre \"lock\" tasks for consistency between sync/async versions Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'lock' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_lock_element = etree.fromstring( NetconfBaseOperations.LOCK.value.format(target=target), parser=self.xml_parser ) xml_request.insert(0, xml_lock_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'lock' operation. Payload: {channel_input.decode()}\") return response def _pre_unlock(self, target: str) -> NetconfResponse: \"\"\" Handle pre \"unlock\" tasks for consistency between sync/async versions Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'unlock' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_lock_element = etree.fromstring( NetconfBaseOperations.UNLOCK.value.format(target=target, parser=self.xml_parser) ) xml_request.insert(0, xml_lock_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'unlock' operation. Payload: {channel_input.decode()}\" ) return response def _pre_rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Handle pre \"rpc\" tasks for consistency between sync/async versions Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'rpc' operation.\") xml_request = self._build_base_elem() # build filter element if isinstance(filter_, str): xml_filter_elem = etree.fromstring(filter_, parser=self.xml_parser) else: xml_filter_elem = filter_ # insert filter element xml_request.insert(0, xml_filter_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'rpc' operation. Payload: {channel_input.decode()}\") return response def _pre_validate(self, source: str) -> NetconfResponse: \"\"\" Handle pre \"validate\" tasks for consistency between sync/async versions Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: CapabilityNotSupported: if 'validate' capability does not exist \"\"\" self.logger.debug(\"Building payload for 'validate' operation.\") if not any( cap in self.server_capabilities for cap in ( \"urn:ietf:params:netconf:capability:validate:1.0\", \"urn:ietf:params:netconf:capability:validate:1.1\", ) ): msg = \"validate requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) self._validate_edit_config_target(target=source) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.VALIDATE.value.format(source=source), parser=self.xml_parser ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'validate' operation. Payload: {channel_input.decode()}\" ) return response def _pre_copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Handle pre \"copy_config\" tasks for consistency between sync/async versions Note that source is not validated/checked since it could be a url or a full configuration element itself. Args: source: configuration, url, or datastore to copy into the target datastore target: copy config destination/target; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'copy_config' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.COPY_CONFIG.value.format(source=source, target=target), parser=self.xml_parser, ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'copy-config' operation. Payload: {channel_input.decode()}\" ) return response","title":"Module scrapli_netconf.driver.base_driver"},{"location":"api_docs/driver/base_driver/#classes","text":"","title":"Classes"},{"location":"api_docs/driver/base_driver/#netconfbasedriver","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 BaseDriver Object BaseDriver is the root for all Scrapli driver classes. The synchronous and asyncio driver base driver classes can be used to provide a semi-pexpect like experience over top of whatever transport a user prefers. Generally, however, the base driver classes should not be used directly. It is best to use the GenericDriver (or AsyncGenericDriver) or NetworkDriver (or AsyncNetworkDriver) sub-classes of the base drivers. Args: host: host ip/name to connect to port: port to connect to auth_username: username for authentication auth_private_key: path to private key for authentication auth_private_key_passphrase: passphrase for decrypting ssh key if necessary auth_password: password for authentication auth_strict_key: strict host checking or not auth_bypass: bypass \"in channel\" authentication -- only supported with telnet, asynctelnet, and system transport plugins timeout_socket: timeout for establishing socket/initial connection in seconds timeout_transport: timeout for ssh|telnet transport in seconds timeout_ops: timeout for ssh channel operations comms_prompt_pattern: raw string regex pattern -- preferably use `^` and `$` anchors! this is the single most important attribute here! if this does not match a prompt, scrapli will not work! IMPORTANT: regex search uses multi-line + case insensitive flags. multi-line allows for highly reliably matching for prompts however we do NOT strip trailing whitespace for each line, so be sure to add '\\\\s?' or similar if your device needs that. This should be mostly sorted for you if using network drivers (i.e. `IOSXEDriver`). Lastly, the case insensitive is just a convenience factor so i can be lazy. comms_return_char: character to use to send returns to host ssh_config_file: string to path for ssh config file, True to use default ssh config file or False to ignore default ssh config file ssh_known_hosts_file: string to path for ssh known hosts file, True to use default known file locations. Only applicable/needed if `auth_strict_key` is set to True on_init: callable that accepts the class instance as its only argument. this callable, if provided, is executed as the last step of object instantiation -- its purpose is primarily to provide a mechanism for scrapli community platforms to have an easy way to modify initialization arguments/object attributes without needing to create a class that extends the driver, instead allowing the community platforms to simply build from the GenericDriver or NetworkDriver classes, and pass this callable to do things such as appending to a username (looking at you RouterOS!!). Note that this is *always* a synchronous function (even for asyncio drivers)! on_open: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately after authentication is completed. Common use cases for this callable would be to disable paging or accept any kind of banner message that prompts a user upon connection on_close: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately prior to closing the underlying transport. Common use cases for this callable would be to save configurations prior to exiting, or to logout properly to free up vtys or similar transport: name of the transport plugin to use for the actual telnet/ssh/netconf connection. Available \"core\" transports are: - system - telnet - asynctelnet - ssh2 - paramiko - asyncssh Please see relevant transport plugin section for details. Additionally third party transport plugins may be available. transport_options: dictionary of options to pass to selected transport class; see docs for given transport class for details of what to pass here channel_lock: True/False to lock the channel (threading.Lock/asyncio.Lock) during any channel operations, defaults to False channel_log: True/False or a string path to a file of where to write out channel logs -- these are not \"logs\" in the normal logging module sense, but only the output that is read from the channel. In other words, the output of the channel log should look similar to what you would see as a human connecting to a device channel_log_mode: \"write\"|\"append\", all other values will raise ValueError, does what it sounds like it should by setting the channel log to the provided mode logging_uid: unique identifier (string) to associate to log messages; useful if you have multiple connections to the same device (i.e. one console, one ssh, or one to each supervisor module, etc.) Returns: None Raises: N/A Expand source code class NetconfBaseDriver(BaseDriver): host: str readable_datastores: List[str] writeable_datastores: List[str] strip_namespaces: bool strict_datastores: bool flatten_input: bool _netconf_base_channel_args: NetconfBaseChannelArgs @property def netconf_version(self) -> NetconfVersion: \"\"\" Getter for 'netconf_version' attribute Args: N/A Returns: NetconfVersion: netconf_version enum Raises: N/A \"\"\" return self._netconf_base_channel_args.netconf_version @netconf_version.setter def netconf_version(self, value: NetconfVersion) -> None: \"\"\" Setter for 'netconf_version' attribute Args: value: NetconfVersion Returns: None Raises: ScrapliTypeError: if value is not of type NetconfVersion \"\"\" if not isinstance(value, NetconfVersion): raise ScrapliTypeError self.logger.debug(f\"setting 'netconf_version' value to '{value.value}'\") self._netconf_base_channel_args.netconf_version = value if self._netconf_base_channel_args.netconf_version == NetconfVersion.VERSION_1_0: self._base_channel_args.comms_prompt_pattern = \"]]>]]>\" else: self._base_channel_args.comms_prompt_pattern = r\"^##$\" @property def client_capabilities(self) -> NetconfClientCapabilities: \"\"\" Getter for 'client_capabilities' attribute Args: N/A Returns: NetconfClientCapabilities: netconf client capabilities enum Raises: N/A \"\"\" return self._netconf_base_channel_args.client_capabilities @client_capabilities.setter def client_capabilities(self, value: NetconfClientCapabilities) -> None: \"\"\" Setter for 'client_capabilities' attribute Args: value: NetconfClientCapabilities value for client_capabilities Returns: None Raises: ScrapliTypeError: if value is not of type NetconfClientCapabilities \"\"\" if not isinstance(value, NetconfClientCapabilities): raise ScrapliTypeError self.logger.debug(f\"setting 'client_capabilities' value to '{value.value}'\") self._netconf_base_channel_args.client_capabilities = value @property def server_capabilities(self) -> List[str]: \"\"\" Getter for 'server_capabilities' attribute Args: N/A Returns: list: list of strings of server capabilities Raises: N/A \"\"\" return self._netconf_base_channel_args.server_capabilities or [] @server_capabilities.setter def server_capabilities(self, value: NetconfClientCapabilities) -> None: \"\"\" Setter for 'server_capabilities' attribute Args: value: list of strings of netconf server capabilities Returns: None Raises: ScrapliTypeError: if value is not of type list \"\"\" if not isinstance(value, list): raise ScrapliTypeError self.logger.debug(f\"setting 'server_capabilities' value to '{value}'\") self._netconf_base_channel_args.server_capabilities = value @staticmethod def _determine_preferred_netconf_version( preferred_netconf_version: Optional[str], ) -> NetconfVersion: \"\"\" Determine users preferred netconf version (if applicable) Args: preferred_netconf_version: optional string indicating users preferred netconf version Returns: NetconfVersion: users preferred netconf version Raises: ScrapliValueError: if preferred_netconf_version is not None or a valid option \"\"\" if preferred_netconf_version is None: return NetconfVersion.UNKNOWN if preferred_netconf_version == \"1.0\": return NetconfVersion.VERSION_1_0 if preferred_netconf_version == \"1.1\": return NetconfVersion.VERSION_1_1 raise ScrapliValueError( \"'preferred_netconf_version' provided with invalid value, must be one of: \" \"None, '1.0', or '1.1'\" ) @staticmethod def _determine_preferred_xml_parser(use_compressed_parser: bool) -> XmlParserVersion: \"\"\" Determine users preferred xml payload parser Args: use_compressed_parser: bool indicating use of compressed parser or not Returns: XmlParserVersion: users xml parser version Raises: N/A \"\"\" if use_compressed_parser is True: return XmlParserVersion.COMPRESSED_PARSER return XmlParserVersion.STANDARD_PARSER @property def xml_parser(self) -> etree.XMLParser: \"\"\" Getter for 'xml_parser' attribute Args: N/A Returns: etree.XMLParser: parser to use for parsing xml documents Raises: N/A \"\"\" if self._netconf_base_channel_args.xml_parser == XmlParserVersion.COMPRESSED_PARSER: return COMPRESSED_PARSER return STANDARD_PARSER @xml_parser.setter def xml_parser(self, value: XmlParserVersion) -> None: \"\"\" Setter for 'xml_parser' attribute Args: value: enum indicating parser version to use Returns: None Raises: ScrapliTypeError: if value is not of type XmlParserVersion \"\"\" if not isinstance(value, XmlParserVersion): raise ScrapliTypeError self._netconf_base_channel_args.xml_parser = value def _transport_factory(self) -> Tuple[Callable[..., Any], object]: \"\"\" Determine proper transport class and necessary arguments to initialize that class Args: N/A Returns: Tuple[Callable[..., Any], object]: tuple of transport class and dataclass of transport class specific arguments Raises: N/A \"\"\" transport_plugin_module = importlib.import_module( f\"scrapli_netconf.transport.plugins.{self.transport_name}.transport\" ) transport_class = getattr( transport_plugin_module, f\"Netconf{self.transport_name.capitalize()}Transport\" ) plugin_transport_args_class = getattr(transport_plugin_module, \"PluginTransportArgs\") _plugin_transport_args = { field.name: getattr(self, field.name) for field in fields(plugin_transport_args_class) } plugin_transport_args = plugin_transport_args_class(**_plugin_transport_args) return transport_class, plugin_transport_args def _build_readable_datastores(self) -> None: \"\"\" Build a list of readable datastores based on server's advertised capabilities Args: N/A Returns: None Raises: N/A \"\"\" self.readable_datastores = [] self.readable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:candidate:1.0\" in self.server_capabilities: self.readable_datastores.append(\"candidate\") if \"urn:ietf:params:netconf:capability:startup:1.0\" in self.server_capabilities: self.readable_datastores.append(\"startup\") def _build_writeable_datastores(self) -> None: \"\"\" Build a list of writeable/editable datastores based on server's advertised capabilities Args: N/A Returns: None Raises: N/A \"\"\" self.writeable_datastores = [] if \"urn:ietf:params:netconf:capability:writeable-running:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:writable-running:1.0\" in self.server_capabilities: # NOTE: iosxe shows \"writable\" (as of 2020.07.01) despite RFC being \"writeable\" self.writeable_datastores.append(\"running\") if \"urn:ietf:params:netconf:capability:candidate:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"candidate\") if \"urn:ietf:params:netconf:capability:startup:1.0\" in self.server_capabilities: self.writeable_datastores.append(\"startup\") def _validate_get_config_target(self, source: str) -> None: \"\"\" Validate get-config source is acceptable Args: source: configuration source to get; typically one of running|startup|candidate Returns: None Raises: ScrapliValueError: if an invalid source was selected and strict_datastores is True \"\"\" if source not in self.readable_datastores: msg = f\"'source' should be one of {self.readable_datastores}, got '{source}'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore source!\", message=msg) def _validate_edit_config_target(self, target: str) -> None: \"\"\" Validate edit-config/lock/unlock target is acceptable Args: target: configuration source to edit/lock; typically one of running|startup|candidate Returns: None Raises: ScrapliValueError: if an invalid source was selected \"\"\" if target not in self.writeable_datastores: msg = f\"'target' should be one of {self.writeable_datastores}, got '{target}'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore target!\", message=msg) def _validate_delete_config_target(self, target: str) -> None: \"\"\" Validate delete-config/lock/unlock target is acceptable Args: target: configuration source to delete; typically one of startup|candidate Returns: None Raises: ScrapliValueError: if an invalid target was selected \"\"\" if target == \"running\" or target not in self.writeable_datastores: msg = f\"'target' should be one of {self.writeable_datastores}, got '{target}'\" if target == \"running\": msg = \"delete-config 'target' may not be 'running'\" self.logger.warning(msg) if self.strict_datastores is True: raise ScrapliValueError(msg) user_warning(title=\"Invalid datastore target!\", message=msg) def _build_base_elem(self) -> _Element: \"\"\" Create base element for netconf operations Args: N/A Returns: _Element: lxml base element to use for netconf operation Raises: N/A \"\"\" # pylint did not seem to want to be ok with assigning this as a class attribute... and its # only used here so... here we are self.message_id: int # pylint: disable=W0201 self.logger.debug(f\"Building base element for message id {self.message_id}\") base_xml_str = NetconfBaseOperations.RPC.value.format(message_id=self.message_id) self.message_id += 1 base_elem = etree.fromstring(text=base_xml_str) return base_elem def _build_filter(self, filter_: str, filter_type: str = \"subtree\") -> _Element: \"\"\" Create filter element for a given rpc The `filter_` string may contain multiple xml elements at its \"root\" (subtree filters); we will simply place the payload into a temporary \"tmp\" outer tag so that when we cast it to an etree object the elements are all preserved; without this outer \"tmp\" tag, lxml will scoop up only the first element provided as it appears to be the root of the document presumably. An example valid (to scrapli netconf at least) xml filter would be: 1 2 3 4 5 6 7 <interface-configurations xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-ifmgr-cfg\"> <interface-configuration> <active>act</active> </interface-configuration> </interface-configurations> <netconf-yang xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-man-netconf-cfg\"> </netconf-yang> Args: filter_: strings of filters to build into a filter element or (for subtree) a full filter string (in filter tags) filter_type: type of filter; subtree|xpath Returns: _Element: lxml filter element to use for netconf operation Raises: CapabilityNotSupported: if xpath selected and not supported on server ScrapliValueError: if filter_type is not one of subtree|xpath \"\"\" if filter_type == \"subtree\": # tmp tags to place the users kinda not valid xml filter into _filter_ = f\" {filter_} \" # \"validate\" subtree filter by forcing it into xml, parser \"flattens\" it as well tmp_xml_filter_element = etree.fromstring(_filter_, parser=self.xml_parser) if tmp_xml_filter_element.getchildren()[0].tag == \"filter\": # if the user filter was already wrapped in filter tags we'll end up here, we will # blindly reuse the users filter but we'll make sure that the filter \"type\" is set xml_filter_elem = tmp_xml_filter_element.getchildren()[0] xml_filter_elem.attrib[\"type\"] = \"subtree\" else: xml_filter_elem = etree.fromstring( NetconfBaseOperations.FILTER_SUBTREE.value.format(filter_type=filter_type), ) # iterate through the children inside the tmp tags and insert *those* elements into # the actual final filter payload for xml_filter_element in tmp_xml_filter_element: # insert the subtree filter into the parent filter element xml_filter_elem.insert(1, xml_filter_element) elif filter_type == \"xpath\": if \"urn:ietf:params:netconf:capability:xpath:1.0\" not in self.server_capabilities: msg = \"xpath filter requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) xml_filter_elem = etree.fromstring( NetconfBaseOperations.FILTER_XPATH.value.format( filter_type=filter_type, xpath=filter_ ), parser=self.xml_parser, ) else: raise ScrapliValueError( f\"'filter_type' should be one of subtree|xpath, got '{filter_type}'\" ) return xml_filter_elem def _build_with_defaults(self, default_type: str = \"report-all\") -> _Element: \"\"\" Create with-defaults element for a given operation Args: default_type: enumeration of with-defaults; report-all|trim|explicit|report-all-tagged Returns: _Element: lxml with-defaults element to use for netconf operation Raises: CapabilityNotSupported: if default_type provided but not supported by device ScrapliValueError: if default_type is not one of report-all|trim|explicit|report-all-tagged \"\"\" if default_type in [\"report-all\", \"trim\", \"explicit\", \"report-all-tagged\"]: if ( \"urn:ietf:params:netconf:capability:with-defaults:1.0\" not in self.server_capabilities ): msg = \"with-defaults requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) xml_with_defaults_element = etree.fromstring( NetconfBaseOperations.WITH_DEFAULTS_SUBTREE.value.format(default_type=default_type), parser=self.xml_parser, ) else: raise ScrapliValueError( \"'default_type' should be one of report-all|trim|explicit|report-all-tagged, \" f\"got '{default_type}'\" ) return xml_with_defaults_element def _finalize_channel_input(self, xml_request: _Element) -> bytes: \"\"\" Create finalized channel input (as bytes) Args: xml_request: finalized xml element to cast to bytes and add declaration to Returns: bytes: finalized bytes input -- with 1.0 delimiter or 1.1 encoding Raises: N/A \"\"\" channel_input: bytes = etree.tostring( element_or_tree=xml_request, xml_declaration=True, encoding=\"utf-8\" ) if self.netconf_version == NetconfVersion.VERSION_1_0: channel_input = channel_input + b\"]]>]]>\" else: # format message for chunk (netconf 1.1) style message channel_input = b\"#%b\\n\" % str(len(channel_input)).encode() + channel_input + b\"\\n##\" return channel_input def _pre_get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Handle pre \"get\" tasks for consistency between sync/async versions *NOTE* The channel input (filter_) is loaded up as an lxml etree element here, this is done with a parser that removes whitespace. This has a somewhat undesirable effect of making any \"pretty\" input not pretty, however... after we load the xml object (which we do to validate that it is valid xml) we dump that xml object back to a string to be used as the actual raw payload we send down the channel, which means we are sending \"flattened\" (not pretty/ indented xml) to the device. This is important it seems! Some devices seme to not mind having the \"nicely\" formatted input (pretty xml). But! On devices that \"echo\" the inputs back -- sometimes the device will respond to our rpc without \"finishing\" echoing our inputs to the device, this breaks the core \"read until input\" processing that scrapli always does. For whatever reason if there are no line breaks this does not seem to happen? /shrug. Note that this comment applies to all of the \"pre\" methods that we parse a filter/payload! Args: filter_: string filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'get' operation. filter_type: {filter_type}, filter_: {filter_}\" ) # build base request and insert the get element xml_request = self._build_base_elem() xml_get_element = etree.fromstring(NetconfBaseOperations.GET.value) xml_request.insert(0, xml_get_element) # build filter element xml_filter_elem = self._build_filter(filter_=filter_, filter_type=filter_type) # insert filter element into parent get element get_element = xml_request.find(\"get\") get_element.insert(0, xml_filter_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'get' operation. Payload: {channel_input.decode()}\") return response def _pre_get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Handle pre \"get_config\" tasks for consistency between sync/async versions Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'get-config' operation. source: {source}, filter_type: \" f\"{filter_type}, filter: {filter_}, default_type: {default_type}\" ) self._validate_get_config_target(source=source) # build base request and insert the get-config element xml_request = self._build_base_elem() xml_get_config_element = etree.fromstring( NetconfBaseOperations.GET_CONFIG.value.format(source=source), parser=self.xml_parser ) xml_request.insert(0, xml_get_config_element) if filter_ is not None: xml_filter_elem = self._build_filter(filter_=filter_, filter_type=filter_type) # insert filter element into parent get element get_element = xml_request.find(\"get-config\") # insert *after* source, otherwise juniper seems to gripe, maybe/probably others as well get_element.insert(1, xml_filter_elem) if default_type is not None: xml_with_defaults_elem = self._build_with_defaults(default_type=default_type) get_element = xml_request.find(\"get-config\") get_element.insert(2, xml_with_defaults_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'get-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Handle pre \"edit_config\" tasks for consistency between sync/async versions Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug( f\"Building payload for 'edit-config' operation. target: {target}, config: {config}\" ) self._validate_edit_config_target(target=target) xml_config = etree.fromstring(config, parser=self.xml_parser) # build base request and insert the edit-config element xml_request = self._build_base_elem() xml_edit_config_element = etree.fromstring( NetconfBaseOperations.EDIT_CONFIG.value.format(target=target) ) xml_request.insert(0, xml_edit_config_element) # insert parent filter element to first position so that target stays first just for nice # output/readability edit_config_element = xml_request.find(\"edit-config\") edit_config_element.insert(1, xml_config) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'edit-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_delete_config(self, target: str = \"running\") -> NetconfResponse: \"\"\" Handle pre \"edit_config\" tasks for consistency between sync/async versions Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(f\"Building payload for 'delete-config' operation. target: {target}\") self._validate_delete_config_target(target=target) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.DELETE_CONFIG.value.format(target=target), parser=self.xml_parser ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'delete-config' operation. Payload: {channel_input.decode()}\" ) return response def _pre_commit(self) -> NetconfResponse: \"\"\" Handle pre \"commit\" tasks for consistency between sync/async versions Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'commit' operation\") xml_request = self._build_base_elem() xml_commit_element = etree.fromstring( NetconfBaseOperations.COMMIT.value, parser=self.xml_parser ) xml_request.insert(0, xml_commit_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'commit' operation. Payload: {channel_input.decode()}\" ) return response def _pre_discard(self) -> NetconfResponse: \"\"\" Handle pre \"discard\" tasks for consistency between sync/async versions Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'discard' operation.\") xml_request = self._build_base_elem() xml_commit_element = etree.fromstring( NetconfBaseOperations.DISCARD.value, parser=self.xml_parser ) xml_request.insert(0, xml_commit_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'discard' operation. Payload: {channel_input.decode()}\" ) return response def _pre_lock(self, target: str) -> NetconfResponse: \"\"\" Handle pre \"lock\" tasks for consistency between sync/async versions Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'lock' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_lock_element = etree.fromstring( NetconfBaseOperations.LOCK.value.format(target=target), parser=self.xml_parser ) xml_request.insert(0, xml_lock_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'lock' operation. Payload: {channel_input.decode()}\") return response def _pre_unlock(self, target: str) -> NetconfResponse: \"\"\" Handle pre \"unlock\" tasks for consistency between sync/async versions Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'unlock' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_lock_element = etree.fromstring( NetconfBaseOperations.UNLOCK.value.format(target=target, parser=self.xml_parser) ) xml_request.insert(0, xml_lock_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'unlock' operation. Payload: {channel_input.decode()}\" ) return response def _pre_rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Handle pre \"rpc\" tasks for consistency between sync/async versions Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'rpc' operation.\") xml_request = self._build_base_elem() # build filter element if isinstance(filter_, str): xml_filter_elem = etree.fromstring(filter_, parser=self.xml_parser) else: xml_filter_elem = filter_ # insert filter element xml_request.insert(0, xml_filter_elem) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug(f\"Built payload for 'rpc' operation. Payload: {channel_input.decode()}\") return response def _pre_validate(self, source: str) -> NetconfResponse: \"\"\" Handle pre \"validate\" tasks for consistency between sync/async versions Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: CapabilityNotSupported: if 'validate' capability does not exist \"\"\" self.logger.debug(\"Building payload for 'validate' operation.\") if not any( cap in self.server_capabilities for cap in ( \"urn:ietf:params:netconf:capability:validate:1.0\", \"urn:ietf:params:netconf:capability:validate:1.1\", ) ): msg = \"validate requested, but is not supported by the server\" self.logger.exception(msg) raise CapabilityNotSupported(msg) self._validate_edit_config_target(target=source) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.VALIDATE.value.format(source=source), parser=self.xml_parser ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'validate' operation. Payload: {channel_input.decode()}\" ) return response def _pre_copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Handle pre \"copy_config\" tasks for consistency between sync/async versions Note that source is not validated/checked since it could be a url or a full configuration element itself. Args: source: configuration, url, or datastore to copy into the target datastore target: copy config destination/target; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object containing all the necessary channel inputs (string and xml) Raises: N/A \"\"\" self.logger.debug(\"Building payload for 'copy_config' operation.\") self._validate_edit_config_target(target=target) xml_request = self._build_base_elem() xml_validate_element = etree.fromstring( NetconfBaseOperations.COPY_CONFIG.value.format(source=source, target=target), parser=self.xml_parser, ) xml_request.insert(0, xml_validate_element) channel_input = self._finalize_channel_input(xml_request=xml_request) response = NetconfResponse( host=self.host, channel_input=channel_input.decode(), xml_input=xml_request, netconf_version=self.netconf_version, strip_namespaces=self.strip_namespaces, ) self.logger.debug( f\"Built payload for 'copy-config' operation. Payload: {channel_input.decode()}\" ) return response","title":"NetconfBaseDriver"},{"location":"api_docs/driver/base_driver/#ancestors-in-mro","text":"scrapli.driver.base.base_driver.BaseDriver","title":"Ancestors (in MRO)"},{"location":"api_docs/driver/base_driver/#descendants","text":"scrapli_netconf.driver.async_driver.AsyncNetconfDriver scrapli_netconf.driver.sync_driver.NetconfDriver","title":"Descendants"},{"location":"api_docs/driver/base_driver/#class-variables","text":"flatten_input: bool host: str readable_datastores: List[str] strict_datastores: bool strip_namespaces: bool writeable_datastores: List[str]","title":"Class variables"},{"location":"api_docs/driver/base_driver/#instance-variables","text":"client_capabilities: scrapli_netconf.constants.NetconfClientCapabilities 1 2 3 4 5 6 7 8 9 10 Getter for 'client_capabilities' attribute Args: N/A Returns: NetconfClientCapabilities: netconf client capabilities enum Raises: N/A netconf_version: scrapli_netconf.constants.NetconfVersion 1 2 3 4 5 6 7 8 9 10 Getter for 'netconf_version' attribute Args: N/A Returns: NetconfVersion: netconf_version enum Raises: N/A server_capabilities: List[str] 1 2 3 4 5 6 7 8 9 10 Getter for 'server_capabilities' attribute Args: N/A Returns: list: list of strings of server capabilities Raises: N/A xml_parser: lxml.etree.XMLParser 1 2 3 4 5 6 7 8 9 10 Getter for 'xml_parser' attribute Args: N/A Returns: etree.XMLParser: parser to use for parsing xml documents Raises: N/A","title":"Instance variables"},{"location":"api_docs/driver/base_driver/#netconfbaseoperations","text":"1 An enumeration. Expand source code class NetconfBaseOperations(Enum): FILTER_SUBTREE = \" \" FILTER_XPATH = \" \" WITH_DEFAULTS_SUBTREE = ( \" \" \"{default_type} \" ) GET = \" \" GET_CONFIG = \" < {source}/> \" EDIT_CONFIG = \" < {target}/> \" DELETE_CONFIG = \" < {target}/> \" COPY_CONFIG = ( \" < {target}/> < {source}/> \" ) COMMIT = \" \" DISCARD = \" \" LOCK = \" < {target}/> \" UNLOCK = \" < {target}/> \" RPC = \" \" VALIDATE = \" < {source}/> \"","title":"NetconfBaseOperations"},{"location":"api_docs/driver/base_driver/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"api_docs/driver/base_driver/#class-variables_1","text":"COMMIT COPY_CONFIG DELETE_CONFIG DISCARD EDIT_CONFIG FILTER_SUBTREE FILTER_XPATH GET GET_CONFIG LOCK RPC UNLOCK VALIDATE WITH_DEFAULTS_SUBTREE","title":"Class variables"},{"location":"api_docs/driver/sync_driver/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.driver.sync_driver \u00b6 scrapli_netconf.driver.sync_driver Expand source code \"\"\"scrapli_netconf.driver.sync_driver\"\"\" from typing import Any, Callable, Dict, List, Optional, Union from lxml.etree import _Element from scrapli import Driver from scrapli_netconf.channel.base_channel import NetconfBaseChannelArgs from scrapli_netconf.channel.sync_channel import NetconfChannel from scrapli_netconf.driver.base_driver import NetconfBaseDriver from scrapli_netconf.response import NetconfResponse class NetconfDriver(Driver, NetconfBaseDriver): # kinda hate this but need to tell mypy that channel in netconf land is in fact a channel of # type `NetconfChannel` channel: NetconfChannel def __init__( self, host: str, port: int = 830, strip_namespaces: bool = False, strict_datastores: bool = False, auth_username: str = \"\", auth_password: str = \"\", auth_private_key: str = \"\", auth_private_key_passphrase: str = \"\", auth_strict_key: bool = True, auth_bypass: bool = False, timeout_socket: float = 15.0, timeout_transport: float = 30.0, timeout_ops: float = 30.0, comms_prompt_pattern: str = r\"^[a-z0-9.\\-@()/:]{1,48}[#>$]\\s*$\", comms_return_char: str = \"\\n\", ssh_config_file: Union[str, bool] = False, ssh_known_hosts_file: Union[str, bool] = False, on_init: Optional[Callable[..., Any]] = None, on_open: Optional[Callable[..., Any]] = None, on_close: Optional[Callable[..., Any]] = None, transport: str = \"system\", transport_options: Optional[Dict[str, Any]] = None, channel_log: Union[str, bool] = False, channel_lock: bool = False, preferred_netconf_version: Optional[str] = None, use_compressed_parser: bool = True, ) -> None: super().__init__( host=host, port=port, auth_username=auth_username, auth_password=auth_password, auth_private_key=auth_private_key, auth_private_key_passphrase=auth_private_key_passphrase, auth_strict_key=auth_strict_key, auth_bypass=auth_bypass, timeout_socket=timeout_socket, timeout_transport=timeout_transport, timeout_ops=timeout_ops, comms_prompt_pattern=comms_prompt_pattern, comms_return_char=comms_return_char, ssh_config_file=ssh_config_file, ssh_known_hosts_file=ssh_known_hosts_file, on_init=on_init, on_open=on_open, on_close=on_close, transport=transport, transport_options=transport_options, channel_log=channel_log, channel_lock=channel_lock, ) _preferred_netconf_version = self._determine_preferred_netconf_version( preferred_netconf_version=preferred_netconf_version ) _preferred_xml_parser = self._determine_preferred_xml_parser( use_compressed_parser=use_compressed_parser ) self._netconf_base_channel_args = NetconfBaseChannelArgs( netconf_version=_preferred_netconf_version, xml_parser=_preferred_xml_parser ) self.channel = NetconfChannel( transport=self.transport, base_channel_args=self._base_channel_args, netconf_base_channel_args=self._netconf_base_channel_args, ) self.strip_namespaces = strip_namespaces self.strict_datastores = strict_datastores self.server_capabilities: List[str] = [] self.readable_datastores: List[str] = [] self.writeable_datastores: List[str] = [] self.message_id = 101 def open(self) -> None: \"\"\" Open netconf connection to server Args: N/A Returns: None Raises: N/A \"\"\" self._pre_open_closing_log(closing=False) self.transport.open_netconf() # in the future this and scrapli core should just have a class attribute of the transports # that require this \"in channel\" auth so we can dynamically figure that out rather than # just look at the name of the transport if \"system\" in self.transport_name: self.channel.channel_authenticate_netconf( auth_password=self.auth_password, auth_private_key_passphrase=self.auth_private_key_passphrase, ) self.channel.open_netconf() self._build_readable_datastores() self._build_writeable_datastores() self._post_open_closing_log(closing=False) def get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Netconf get operation Args: filter_: filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get(filter_=filter_, filter_type=filter_type) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get_config( source=source, filter_=filter_, filter_type=filter_type, default_type=default_type ) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_edit_config(config=config, target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def delete_config(self, target: str = \"candidate\") -> NetconfResponse: \"\"\" Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_delete_config(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def commit(self) -> NetconfResponse: \"\"\" Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_commit() raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def discard(self) -> NetconfResponse: \"\"\" Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_discard() raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def lock(self, target: str) -> NetconfResponse: \"\"\" Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_lock(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def unlock(self, target: str) -> NetconfResponse: \"\"\" Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_unlock(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_rpc(filter_=filter_) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def validate(self, source: str) -> NetconfResponse: \"\"\" Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_validate(source=source) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_copy_config(source=source, target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response Classes \u00b6 NetconfDriver \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 BaseDriver Object BaseDriver is the root for all Scrapli driver classes. The synchronous and asyncio driver base driver classes can be used to provide a semi-pexpect like experience over top of whatever transport a user prefers. Generally, however, the base driver classes should not be used directly. It is best to use the GenericDriver (or AsyncGenericDriver) or NetworkDriver (or AsyncNetworkDriver) sub-classes of the base drivers. Args: host: host ip/name to connect to port: port to connect to auth_username: username for authentication auth_private_key: path to private key for authentication auth_private_key_passphrase: passphrase for decrypting ssh key if necessary auth_password: password for authentication auth_strict_key: strict host checking or not auth_bypass: bypass \"in channel\" authentication -- only supported with telnet, asynctelnet, and system transport plugins timeout_socket: timeout for establishing socket/initial connection in seconds timeout_transport: timeout for ssh|telnet transport in seconds timeout_ops: timeout for ssh channel operations comms_prompt_pattern: raw string regex pattern -- preferably use `^` and `$` anchors! this is the single most important attribute here! if this does not match a prompt, scrapli will not work! IMPORTANT: regex search uses multi-line + case insensitive flags. multi-line allows for highly reliably matching for prompts however we do NOT strip trailing whitespace for each line, so be sure to add '\\\\s?' or similar if your device needs that. This should be mostly sorted for you if using network drivers (i.e. `IOSXEDriver`). Lastly, the case insensitive is just a convenience factor so i can be lazy. comms_return_char: character to use to send returns to host ssh_config_file: string to path for ssh config file, True to use default ssh config file or False to ignore default ssh config file ssh_known_hosts_file: string to path for ssh known hosts file, True to use default known file locations. Only applicable/needed if `auth_strict_key` is set to True on_init: callable that accepts the class instance as its only argument. this callable, if provided, is executed as the last step of object instantiation -- its purpose is primarily to provide a mechanism for scrapli community platforms to have an easy way to modify initialization arguments/object attributes without needing to create a class that extends the driver, instead allowing the community platforms to simply build from the GenericDriver or NetworkDriver classes, and pass this callable to do things such as appending to a username (looking at you RouterOS!!). Note that this is *always* a synchronous function (even for asyncio drivers)! on_open: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately after authentication is completed. Common use cases for this callable would be to disable paging or accept any kind of banner message that prompts a user upon connection on_close: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately prior to closing the underlying transport. Common use cases for this callable would be to save configurations prior to exiting, or to logout properly to free up vtys or similar transport: name of the transport plugin to use for the actual telnet/ssh/netconf connection. Available \"core\" transports are: - system - telnet - asynctelnet - ssh2 - paramiko - asyncssh Please see relevant transport plugin section for details. Additionally third party transport plugins may be available. transport_options: dictionary of options to pass to selected transport class; see docs for given transport class for details of what to pass here channel_lock: True/False to lock the channel (threading.Lock/asyncio.Lock) during any channel operations, defaults to False channel_log: True/False or a string path to a file of where to write out channel logs -- these are not \"logs\" in the normal logging module sense, but only the output that is read from the channel. In other words, the output of the channel log should look similar to what you would see as a human connecting to a device channel_log_mode: \"write\"|\"append\", all other values will raise ValueError, does what it sounds like it should by setting the channel log to the provided mode logging_uid: unique identifier (string) to associate to log messages; useful if you have multiple connections to the same device (i.e. one console, one ssh, or one to each supervisor module, etc.) Returns: None Raises: N/A Expand source code class NetconfDriver(Driver, NetconfBaseDriver): # kinda hate this but need to tell mypy that channel in netconf land is in fact a channel of # type `NetconfChannel` channel: NetconfChannel def __init__( self, host: str, port: int = 830, strip_namespaces: bool = False, strict_datastores: bool = False, auth_username: str = \"\", auth_password: str = \"\", auth_private_key: str = \"\", auth_private_key_passphrase: str = \"\", auth_strict_key: bool = True, auth_bypass: bool = False, timeout_socket: float = 15.0, timeout_transport: float = 30.0, timeout_ops: float = 30.0, comms_prompt_pattern: str = r\"^[a-z0-9.\\-@()/:]{1,48}[#>$]\\s*$\", comms_return_char: str = \"\\n\", ssh_config_file: Union[str, bool] = False, ssh_known_hosts_file: Union[str, bool] = False, on_init: Optional[Callable[..., Any]] = None, on_open: Optional[Callable[..., Any]] = None, on_close: Optional[Callable[..., Any]] = None, transport: str = \"system\", transport_options: Optional[Dict[str, Any]] = None, channel_log: Union[str, bool] = False, channel_lock: bool = False, preferred_netconf_version: Optional[str] = None, use_compressed_parser: bool = True, ) -> None: super().__init__( host=host, port=port, auth_username=auth_username, auth_password=auth_password, auth_private_key=auth_private_key, auth_private_key_passphrase=auth_private_key_passphrase, auth_strict_key=auth_strict_key, auth_bypass=auth_bypass, timeout_socket=timeout_socket, timeout_transport=timeout_transport, timeout_ops=timeout_ops, comms_prompt_pattern=comms_prompt_pattern, comms_return_char=comms_return_char, ssh_config_file=ssh_config_file, ssh_known_hosts_file=ssh_known_hosts_file, on_init=on_init, on_open=on_open, on_close=on_close, transport=transport, transport_options=transport_options, channel_log=channel_log, channel_lock=channel_lock, ) _preferred_netconf_version = self._determine_preferred_netconf_version( preferred_netconf_version=preferred_netconf_version ) _preferred_xml_parser = self._determine_preferred_xml_parser( use_compressed_parser=use_compressed_parser ) self._netconf_base_channel_args = NetconfBaseChannelArgs( netconf_version=_preferred_netconf_version, xml_parser=_preferred_xml_parser ) self.channel = NetconfChannel( transport=self.transport, base_channel_args=self._base_channel_args, netconf_base_channel_args=self._netconf_base_channel_args, ) self.strip_namespaces = strip_namespaces self.strict_datastores = strict_datastores self.server_capabilities: List[str] = [] self.readable_datastores: List[str] = [] self.writeable_datastores: List[str] = [] self.message_id = 101 def open(self) -> None: \"\"\" Open netconf connection to server Args: N/A Returns: None Raises: N/A \"\"\" self._pre_open_closing_log(closing=False) self.transport.open_netconf() # in the future this and scrapli core should just have a class attribute of the transports # that require this \"in channel\" auth so we can dynamically figure that out rather than # just look at the name of the transport if \"system\" in self.transport_name: self.channel.channel_authenticate_netconf( auth_password=self.auth_password, auth_private_key_passphrase=self.auth_private_key_passphrase, ) self.channel.open_netconf() self._build_readable_datastores() self._build_writeable_datastores() self._post_open_closing_log(closing=False) def get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Netconf get operation Args: filter_: filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get(filter_=filter_, filter_type=filter_type) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get_config( source=source, filter_=filter_, filter_type=filter_type, default_type=default_type ) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_edit_config(config=config, target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def delete_config(self, target: str = \"candidate\") -> NetconfResponse: \"\"\" Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_delete_config(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def commit(self) -> NetconfResponse: \"\"\" Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_commit() raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def discard(self) -> NetconfResponse: \"\"\" Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_discard() raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def lock(self, target: str) -> NetconfResponse: \"\"\" Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_lock(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def unlock(self, target: str) -> NetconfResponse: \"\"\" Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_unlock(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_rpc(filter_=filter_) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def validate(self, source: str) -> NetconfResponse: \"\"\" Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_validate(source=source) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_copy_config(source=source, target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response Ancestors (in MRO) \u00b6 scrapli.driver.base.sync_driver.Driver scrapli_netconf.driver.base_driver.NetconfBaseDriver scrapli.driver.base.base_driver.BaseDriver Class variables \u00b6 channel: scrapli_netconf.channel.sync_channel.NetconfChannel Methods \u00b6 commit \u00b6 commit(self) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A copy_config \u00b6 copy_config(self, source: str, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A delete_config \u00b6 delete_config(self, target: str = 'candidate') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A discard \u00b6 discard(self) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A edit_config \u00b6 edit_config(self, config: str, target: str = 'running') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A get \u00b6 get(self, filter_: str, filter_type: str = 'subtree') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf get operation Args: filter_: filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A get_config \u00b6 get_config(self, source: str = 'running', filter_: Optional[str] = None, filter_type: str = 'subtree', default_type: Optional[str] = None) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A lock \u00b6 lock(self, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A open \u00b6 open(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 Open netconf connection to server Args: N/A Returns: None Raises: N/A rpc \u00b6 rpc(self, filter_: Union[str, lxml.etree._Element]) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A unlock \u00b6 unlock(self, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A validate \u00b6 validate(self, source: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"Sync Driver"},{"location":"api_docs/driver/sync_driver/#module-scrapli_netconfdriversync_driver","text":"scrapli_netconf.driver.sync_driver Expand source code \"\"\"scrapli_netconf.driver.sync_driver\"\"\" from typing import Any, Callable, Dict, List, Optional, Union from lxml.etree import _Element from scrapli import Driver from scrapli_netconf.channel.base_channel import NetconfBaseChannelArgs from scrapli_netconf.channel.sync_channel import NetconfChannel from scrapli_netconf.driver.base_driver import NetconfBaseDriver from scrapli_netconf.response import NetconfResponse class NetconfDriver(Driver, NetconfBaseDriver): # kinda hate this but need to tell mypy that channel in netconf land is in fact a channel of # type `NetconfChannel` channel: NetconfChannel def __init__( self, host: str, port: int = 830, strip_namespaces: bool = False, strict_datastores: bool = False, auth_username: str = \"\", auth_password: str = \"\", auth_private_key: str = \"\", auth_private_key_passphrase: str = \"\", auth_strict_key: bool = True, auth_bypass: bool = False, timeout_socket: float = 15.0, timeout_transport: float = 30.0, timeout_ops: float = 30.0, comms_prompt_pattern: str = r\"^[a-z0-9.\\-@()/:]{1,48}[#>$]\\s*$\", comms_return_char: str = \"\\n\", ssh_config_file: Union[str, bool] = False, ssh_known_hosts_file: Union[str, bool] = False, on_init: Optional[Callable[..., Any]] = None, on_open: Optional[Callable[..., Any]] = None, on_close: Optional[Callable[..., Any]] = None, transport: str = \"system\", transport_options: Optional[Dict[str, Any]] = None, channel_log: Union[str, bool] = False, channel_lock: bool = False, preferred_netconf_version: Optional[str] = None, use_compressed_parser: bool = True, ) -> None: super().__init__( host=host, port=port, auth_username=auth_username, auth_password=auth_password, auth_private_key=auth_private_key, auth_private_key_passphrase=auth_private_key_passphrase, auth_strict_key=auth_strict_key, auth_bypass=auth_bypass, timeout_socket=timeout_socket, timeout_transport=timeout_transport, timeout_ops=timeout_ops, comms_prompt_pattern=comms_prompt_pattern, comms_return_char=comms_return_char, ssh_config_file=ssh_config_file, ssh_known_hosts_file=ssh_known_hosts_file, on_init=on_init, on_open=on_open, on_close=on_close, transport=transport, transport_options=transport_options, channel_log=channel_log, channel_lock=channel_lock, ) _preferred_netconf_version = self._determine_preferred_netconf_version( preferred_netconf_version=preferred_netconf_version ) _preferred_xml_parser = self._determine_preferred_xml_parser( use_compressed_parser=use_compressed_parser ) self._netconf_base_channel_args = NetconfBaseChannelArgs( netconf_version=_preferred_netconf_version, xml_parser=_preferred_xml_parser ) self.channel = NetconfChannel( transport=self.transport, base_channel_args=self._base_channel_args, netconf_base_channel_args=self._netconf_base_channel_args, ) self.strip_namespaces = strip_namespaces self.strict_datastores = strict_datastores self.server_capabilities: List[str] = [] self.readable_datastores: List[str] = [] self.writeable_datastores: List[str] = [] self.message_id = 101 def open(self) -> None: \"\"\" Open netconf connection to server Args: N/A Returns: None Raises: N/A \"\"\" self._pre_open_closing_log(closing=False) self.transport.open_netconf() # in the future this and scrapli core should just have a class attribute of the transports # that require this \"in channel\" auth so we can dynamically figure that out rather than # just look at the name of the transport if \"system\" in self.transport_name: self.channel.channel_authenticate_netconf( auth_password=self.auth_password, auth_private_key_passphrase=self.auth_private_key_passphrase, ) self.channel.open_netconf() self._build_readable_datastores() self._build_writeable_datastores() self._post_open_closing_log(closing=False) def get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Netconf get operation Args: filter_: filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get(filter_=filter_, filter_type=filter_type) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get_config( source=source, filter_=filter_, filter_type=filter_type, default_type=default_type ) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_edit_config(config=config, target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def delete_config(self, target: str = \"candidate\") -> NetconfResponse: \"\"\" Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_delete_config(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def commit(self) -> NetconfResponse: \"\"\" Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_commit() raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def discard(self) -> NetconfResponse: \"\"\" Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_discard() raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def lock(self, target: str) -> NetconfResponse: \"\"\" Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_lock(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def unlock(self, target: str) -> NetconfResponse: \"\"\" Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_unlock(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_rpc(filter_=filter_) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def validate(self, source: str) -> NetconfResponse: \"\"\" Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_validate(source=source) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_copy_config(source=source, target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response","title":"Module scrapli_netconf.driver.sync_driver"},{"location":"api_docs/driver/sync_driver/#classes","text":"","title":"Classes"},{"location":"api_docs/driver/sync_driver/#netconfdriver","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 BaseDriver Object BaseDriver is the root for all Scrapli driver classes. The synchronous and asyncio driver base driver classes can be used to provide a semi-pexpect like experience over top of whatever transport a user prefers. Generally, however, the base driver classes should not be used directly. It is best to use the GenericDriver (or AsyncGenericDriver) or NetworkDriver (or AsyncNetworkDriver) sub-classes of the base drivers. Args: host: host ip/name to connect to port: port to connect to auth_username: username for authentication auth_private_key: path to private key for authentication auth_private_key_passphrase: passphrase for decrypting ssh key if necessary auth_password: password for authentication auth_strict_key: strict host checking or not auth_bypass: bypass \"in channel\" authentication -- only supported with telnet, asynctelnet, and system transport plugins timeout_socket: timeout for establishing socket/initial connection in seconds timeout_transport: timeout for ssh|telnet transport in seconds timeout_ops: timeout for ssh channel operations comms_prompt_pattern: raw string regex pattern -- preferably use `^` and `$` anchors! this is the single most important attribute here! if this does not match a prompt, scrapli will not work! IMPORTANT: regex search uses multi-line + case insensitive flags. multi-line allows for highly reliably matching for prompts however we do NOT strip trailing whitespace for each line, so be sure to add '\\\\s?' or similar if your device needs that. This should be mostly sorted for you if using network drivers (i.e. `IOSXEDriver`). Lastly, the case insensitive is just a convenience factor so i can be lazy. comms_return_char: character to use to send returns to host ssh_config_file: string to path for ssh config file, True to use default ssh config file or False to ignore default ssh config file ssh_known_hosts_file: string to path for ssh known hosts file, True to use default known file locations. Only applicable/needed if `auth_strict_key` is set to True on_init: callable that accepts the class instance as its only argument. this callable, if provided, is executed as the last step of object instantiation -- its purpose is primarily to provide a mechanism for scrapli community platforms to have an easy way to modify initialization arguments/object attributes without needing to create a class that extends the driver, instead allowing the community platforms to simply build from the GenericDriver or NetworkDriver classes, and pass this callable to do things such as appending to a username (looking at you RouterOS!!). Note that this is *always* a synchronous function (even for asyncio drivers)! on_open: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately after authentication is completed. Common use cases for this callable would be to disable paging or accept any kind of banner message that prompts a user upon connection on_close: callable that accepts the class instance as its only argument. this callable, if provided, is executed immediately prior to closing the underlying transport. Common use cases for this callable would be to save configurations prior to exiting, or to logout properly to free up vtys or similar transport: name of the transport plugin to use for the actual telnet/ssh/netconf connection. Available \"core\" transports are: - system - telnet - asynctelnet - ssh2 - paramiko - asyncssh Please see relevant transport plugin section for details. Additionally third party transport plugins may be available. transport_options: dictionary of options to pass to selected transport class; see docs for given transport class for details of what to pass here channel_lock: True/False to lock the channel (threading.Lock/asyncio.Lock) during any channel operations, defaults to False channel_log: True/False or a string path to a file of where to write out channel logs -- these are not \"logs\" in the normal logging module sense, but only the output that is read from the channel. In other words, the output of the channel log should look similar to what you would see as a human connecting to a device channel_log_mode: \"write\"|\"append\", all other values will raise ValueError, does what it sounds like it should by setting the channel log to the provided mode logging_uid: unique identifier (string) to associate to log messages; useful if you have multiple connections to the same device (i.e. one console, one ssh, or one to each supervisor module, etc.) Returns: None Raises: N/A Expand source code class NetconfDriver(Driver, NetconfBaseDriver): # kinda hate this but need to tell mypy that channel in netconf land is in fact a channel of # type `NetconfChannel` channel: NetconfChannel def __init__( self, host: str, port: int = 830, strip_namespaces: bool = False, strict_datastores: bool = False, auth_username: str = \"\", auth_password: str = \"\", auth_private_key: str = \"\", auth_private_key_passphrase: str = \"\", auth_strict_key: bool = True, auth_bypass: bool = False, timeout_socket: float = 15.0, timeout_transport: float = 30.0, timeout_ops: float = 30.0, comms_prompt_pattern: str = r\"^[a-z0-9.\\-@()/:]{1,48}[#>$]\\s*$\", comms_return_char: str = \"\\n\", ssh_config_file: Union[str, bool] = False, ssh_known_hosts_file: Union[str, bool] = False, on_init: Optional[Callable[..., Any]] = None, on_open: Optional[Callable[..., Any]] = None, on_close: Optional[Callable[..., Any]] = None, transport: str = \"system\", transport_options: Optional[Dict[str, Any]] = None, channel_log: Union[str, bool] = False, channel_lock: bool = False, preferred_netconf_version: Optional[str] = None, use_compressed_parser: bool = True, ) -> None: super().__init__( host=host, port=port, auth_username=auth_username, auth_password=auth_password, auth_private_key=auth_private_key, auth_private_key_passphrase=auth_private_key_passphrase, auth_strict_key=auth_strict_key, auth_bypass=auth_bypass, timeout_socket=timeout_socket, timeout_transport=timeout_transport, timeout_ops=timeout_ops, comms_prompt_pattern=comms_prompt_pattern, comms_return_char=comms_return_char, ssh_config_file=ssh_config_file, ssh_known_hosts_file=ssh_known_hosts_file, on_init=on_init, on_open=on_open, on_close=on_close, transport=transport, transport_options=transport_options, channel_log=channel_log, channel_lock=channel_lock, ) _preferred_netconf_version = self._determine_preferred_netconf_version( preferred_netconf_version=preferred_netconf_version ) _preferred_xml_parser = self._determine_preferred_xml_parser( use_compressed_parser=use_compressed_parser ) self._netconf_base_channel_args = NetconfBaseChannelArgs( netconf_version=_preferred_netconf_version, xml_parser=_preferred_xml_parser ) self.channel = NetconfChannel( transport=self.transport, base_channel_args=self._base_channel_args, netconf_base_channel_args=self._netconf_base_channel_args, ) self.strip_namespaces = strip_namespaces self.strict_datastores = strict_datastores self.server_capabilities: List[str] = [] self.readable_datastores: List[str] = [] self.writeable_datastores: List[str] = [] self.message_id = 101 def open(self) -> None: \"\"\" Open netconf connection to server Args: N/A Returns: None Raises: N/A \"\"\" self._pre_open_closing_log(closing=False) self.transport.open_netconf() # in the future this and scrapli core should just have a class attribute of the transports # that require this \"in channel\" auth so we can dynamically figure that out rather than # just look at the name of the transport if \"system\" in self.transport_name: self.channel.channel_authenticate_netconf( auth_password=self.auth_password, auth_private_key_passphrase=self.auth_private_key_passphrase, ) self.channel.open_netconf() self._build_readable_datastores() self._build_writeable_datastores() self._post_open_closing_log(closing=False) def get(self, filter_: str, filter_type: str = \"subtree\") -> NetconfResponse: \"\"\" Netconf get operation Args: filter_: filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get(filter_=filter_, filter_type=filter_type) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def get_config( self, source: str = \"running\", filter_: Optional[str] = None, filter_type: str = \"subtree\", default_type: Optional[str] = None, ) -> NetconfResponse: \"\"\" Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_get_config( source=source, filter_=filter_, filter_type=filter_type, default_type=default_type ) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def edit_config(self, config: str, target: str = \"running\") -> NetconfResponse: \"\"\" Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_edit_config(config=config, target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def delete_config(self, target: str = \"candidate\") -> NetconfResponse: \"\"\" Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_delete_config(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def commit(self) -> NetconfResponse: \"\"\" Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_commit() raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def discard(self) -> NetconfResponse: \"\"\" Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_discard() raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def lock(self, target: str) -> NetconfResponse: \"\"\" Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_lock(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def unlock(self, target: str) -> NetconfResponse: \"\"\" Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_unlock(target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def rpc(self, filter_: Union[str, _Element]) -> NetconfResponse: \"\"\" Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_rpc(filter_=filter_) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def validate(self, source: str) -> NetconfResponse: \"\"\" Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_validate(source=source) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response def copy_config(self, source: str, target: str) -> NetconfResponse: \"\"\" Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A \"\"\" response = self._pre_copy_config(source=source, target=target) raw_response = self.channel.send_input_netconf(response.channel_input) response.record_response(raw_response) return response","title":"NetconfDriver"},{"location":"api_docs/driver/sync_driver/#ancestors-in-mro","text":"scrapli.driver.base.sync_driver.Driver scrapli_netconf.driver.base_driver.NetconfBaseDriver scrapli.driver.base.base_driver.BaseDriver","title":"Ancestors (in MRO)"},{"location":"api_docs/driver/sync_driver/#class-variables","text":"channel: scrapli_netconf.channel.sync_channel.NetconfChannel","title":"Class variables"},{"location":"api_docs/driver/sync_driver/#methods","text":"","title":"Methods"},{"location":"api_docs/driver/sync_driver/#commit","text":"commit(self) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf commit config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"commit"},{"location":"api_docs/driver/sync_driver/#copy_config","text":"copy_config(self, source: str, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf \"copy-config\" operation Args: source: configuration, url, or datastore to copy into the target datastore target: destination to copy the source to Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"copy_config"},{"location":"api_docs/driver/sync_driver/#delete_config","text":"delete_config(self, target: str = 'candidate') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf delete-config operation Args: target: configuration source to target; startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"delete_config"},{"location":"api_docs/driver/sync_driver/#discard","text":"discard(self) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf discard config operation Args: N/A Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"discard"},{"location":"api_docs/driver/sync_driver/#edit_config","text":"edit_config(self, config: str, target: str = 'running') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf get-config operation Args: config: configuration to send to device target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"edit_config"},{"location":"api_docs/driver/sync_driver/#get","text":"get(self, filter_: str, filter_type: str = 'subtree') \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 Netconf get operation Args: filter_: filter to apply to the get filter_type: type of filter; subtree|xpath Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"get"},{"location":"api_docs/driver/sync_driver/#get_config","text":"get_config(self, source: str = 'running', filter_: Optional[str] = None, filter_type: str = 'subtree', default_type: Optional[str] = None) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 Netconf get-config operation Args: source: configuration source to get; typically one of running|startup|candidate filter_: string of filter(s) to apply to configuration filter_type: type of filter; subtree|xpath default_type: string of with-default mode to apply when retrieving configuration Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"get_config"},{"location":"api_docs/driver/sync_driver/#lock","text":"lock(self, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf lock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"lock"},{"location":"api_docs/driver/sync_driver/#open","text":"open(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 Open netconf connection to server Args: N/A Returns: None Raises: N/A","title":"open"},{"location":"api_docs/driver/sync_driver/#rpc","text":"rpc(self, filter_: Union[str, lxml.etree._Element]) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Netconf \"rpc\" operation Typically used with juniper devices or if you want to build/send your own payload in a more manual fashion. You can provide a string that will be loaded as an lxml element, or you can provide an lxml element yourself. Args: filter_: filter/rpc to execute Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"rpc"},{"location":"api_docs/driver/sync_driver/#unlock","text":"unlock(self, target: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf unlock operation Args: target: configuration source to target; running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"unlock"},{"location":"api_docs/driver/sync_driver/#validate","text":"validate(self, source: str) \u2011> scrapli_netconf.response.NetconfResponse 1 2 3 4 5 6 7 8 9 10 Netconf \"validate\" operation Args: source: configuration source to validate; typically one of running|startup|candidate Returns: NetconfResponse: scrapli_netconf NetconfResponse object Raises: N/A","title":"validate"},{"location":"api_docs/transport/plugins/asyncssh/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.transport.plugins.asyncssh.transport \u00b6 scrapli_netconf.transport.plugins.asyncssh.transport Expand source code \"\"\"scrapli_netconf.transport.plugins.asyncssh.transport\"\"\" import asyncio from asyncssh.connection import SSHClientConnection, connect from asyncssh.misc import ChannelOpenError, PermissionDenied from scrapli.exceptions import ScrapliAuthenticationFailed, ScrapliConnectionNotOpened from scrapli.transport.plugins.asyncssh.transport import AsyncsshTransport, PluginTransportArgs # imported from base driver _ = PluginTransportArgs class NetconfAsyncsshTransport(AsyncsshTransport): async def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: ScrapliAuthenticationFailed: if auth fails ScrapliConnectionNotOpened: if connection cant be opened (but is not an auth failure) \"\"\" if self.plugin_transport_args.auth_strict_key: self.logger.debug( f\"Attempting to validate {self._base_transport_args.host} public key is in known \" f\"hosts\" ) self._verify_key() # we already fetched host/port/user from the user input and/or the ssh config file, so we # want to use those explicitly. likewise we pass config file we already found. set known # hosts and agent to None so we can not have an agent and deal w/ known hosts ourselves common_args = { \"host\": self._base_transport_args.host, \"port\": self._base_transport_args.port, \"username\": self.plugin_transport_args.auth_username, \"known_hosts\": None, \"agent_path\": None, \"config\": self.plugin_transport_args.ssh_config_file, } try: self.session: SSHClientConnection = await asyncio.wait_for( connect( client_keys=self.plugin_transport_args.auth_private_key, password=self.plugin_transport_args.auth_password, preferred_auth=( \"publickey\", \"keyboard-interactive\", \"password\", ), **common_args, ), timeout=self._base_transport_args.timeout_socket, ) except PermissionDenied as exc: msg = \"all authentication methods failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) from exc except asyncio.TimeoutError as exc: msg = \"timed out opening connection to device\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) from exc # it seems we must pass a terminal type to force a pty(?) which i think we want in like... # every case?? https://invisible-island.net/ncurses/ncurses.faq.html#xterm_color # set encoding to None so we get bytes for consistency w/ other scrapli transports # request_pty seems to be safe to set to \"false\" but leaving it at auto which seems closer # to the default behavior. With this omitted (as was previously) connecting to junos devices # on port 830 worked w/ system transport but *not* asyncssh because the pty request failed try: self.stdin, self.stdout, _ = await self.session.open_session( term_type=\"xterm\", encoding=None, subsystem=\"netconf\", request_pty=\"auto\" ) except ChannelOpenError as exc: msg = ( \"Failed to open Channel -- do you have the right port? Most often the netconf \" \"port is 22 or 830!\" ) self.logger.critical(msg) raise ScrapliConnectionNotOpened(msg) from exc if not self.session: raise ScrapliConnectionNotOpened if self.plugin_transport_args.auth_strict_key: self.logger.debug( f\"Attempting to validate {self._base_transport_args.host} public key is in known \" f\"hosts and is valid\" ) self._verify_key_value() Classes \u00b6 NetconfAsyncsshTransport \u00b6 1 2 3 4 5 6 7 8 9 Helper class that provides a standard way to create an ABC using inheritance. Asyncssh transport plugin. Important note: some ssh servers may refuse connections if too many ssh host key algorithms are passed to it during the connection opening -- Asyncssh sends a bunch by default! If you encounter this issue, you can simply update your SSH config file to set a smaller (or one) number of ssh host key algorithms to work around this like so: Host * HostKeyAlgorithms ssh-rsa 1 2 3 4 5 6 7 8 Thank you to @davaeron [https://github.com/davaeron] for reporting this in #173, see also asyncssh #323 here: https://github.com/ronf/asyncssh/issues/323. This transport supports some additional `transport_options` to control behavior -- `asyncssh` is a dictionary that contains options that are passed directly to asyncssh during connection creation, you can find the SSH Client options of asyncssh here: https://asyncssh.readthedocs.io/en/latest/api.html#sshclientconnectionoptions. Below is an example of passing in options to modify kex and encryption algorithms device = { \"host\": \"localhost\", \"transport_options\": { \"asyncssh\": { \"kex_algs\": [\"diffie-hellman-group14-sha1\", \"diffie-hellman-group1-sha1\"], \"encryption_algs\": [\"aes256-cbc\", \"aes192-cbc\", \"aes256-ctr\", \"aes192-ctr\"], } }, \"platform\": \"cisco_iosxe\" } conn = Scrapli(**device) 1 2 3 4 5 6 7 8 9 Args: base_transport_args: scrapli base transport plugin arguments plugin_transport_args: asyncssh ssh specific transport plugin arguments Returns: N/A Raises: N/A Expand source code class NetconfAsyncsshTransport(AsyncsshTransport): async def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: ScrapliAuthenticationFailed: if auth fails ScrapliConnectionNotOpened: if connection cant be opened (but is not an auth failure) \"\"\" if self.plugin_transport_args.auth_strict_key: self.logger.debug( f\"Attempting to validate {self._base_transport_args.host} public key is in known \" f\"hosts\" ) self._verify_key() # we already fetched host/port/user from the user input and/or the ssh config file, so we # want to use those explicitly. likewise we pass config file we already found. set known # hosts and agent to None so we can not have an agent and deal w/ known hosts ourselves common_args = { \"host\": self._base_transport_args.host, \"port\": self._base_transport_args.port, \"username\": self.plugin_transport_args.auth_username, \"known_hosts\": None, \"agent_path\": None, \"config\": self.plugin_transport_args.ssh_config_file, } try: self.session: SSHClientConnection = await asyncio.wait_for( connect( client_keys=self.plugin_transport_args.auth_private_key, password=self.plugin_transport_args.auth_password, preferred_auth=( \"publickey\", \"keyboard-interactive\", \"password\", ), **common_args, ), timeout=self._base_transport_args.timeout_socket, ) except PermissionDenied as exc: msg = \"all authentication methods failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) from exc except asyncio.TimeoutError as exc: msg = \"timed out opening connection to device\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) from exc # it seems we must pass a terminal type to force a pty(?) which i think we want in like... # every case?? https://invisible-island.net/ncurses/ncurses.faq.html#xterm_color # set encoding to None so we get bytes for consistency w/ other scrapli transports # request_pty seems to be safe to set to \"false\" but leaving it at auto which seems closer # to the default behavior. With this omitted (as was previously) connecting to junos devices # on port 830 worked w/ system transport but *not* asyncssh because the pty request failed try: self.stdin, self.stdout, _ = await self.session.open_session( term_type=\"xterm\", encoding=None, subsystem=\"netconf\", request_pty=\"auto\" ) except ChannelOpenError as exc: msg = ( \"Failed to open Channel -- do you have the right port? Most often the netconf \" \"port is 22 or 830!\" ) self.logger.critical(msg) raise ScrapliConnectionNotOpened(msg) from exc if not self.session: raise ScrapliConnectionNotOpened if self.plugin_transport_args.auth_strict_key: self.logger.debug( f\"Attempting to validate {self._base_transport_args.host} public key is in known \" f\"hosts and is valid\" ) self._verify_key_value() Ancestors (in MRO) \u00b6 scrapli.transport.plugins.asyncssh.transport.AsyncsshTransport scrapli.transport.base.async_transport.AsyncTransport scrapli.transport.base.base_transport.BaseTransport abc.ABC Methods \u00b6 open_netconf \u00b6 open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 13 Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: ScrapliAuthenticationFailed: if auth fails ScrapliConnectionNotOpened: if connection cant be opened (but is not an auth failure)","title":"Asyncssh"},{"location":"api_docs/transport/plugins/asyncssh/#module-scrapli_netconftransportpluginsasyncsshtransport","text":"scrapli_netconf.transport.plugins.asyncssh.transport Expand source code \"\"\"scrapli_netconf.transport.plugins.asyncssh.transport\"\"\" import asyncio from asyncssh.connection import SSHClientConnection, connect from asyncssh.misc import ChannelOpenError, PermissionDenied from scrapli.exceptions import ScrapliAuthenticationFailed, ScrapliConnectionNotOpened from scrapli.transport.plugins.asyncssh.transport import AsyncsshTransport, PluginTransportArgs # imported from base driver _ = PluginTransportArgs class NetconfAsyncsshTransport(AsyncsshTransport): async def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: ScrapliAuthenticationFailed: if auth fails ScrapliConnectionNotOpened: if connection cant be opened (but is not an auth failure) \"\"\" if self.plugin_transport_args.auth_strict_key: self.logger.debug( f\"Attempting to validate {self._base_transport_args.host} public key is in known \" f\"hosts\" ) self._verify_key() # we already fetched host/port/user from the user input and/or the ssh config file, so we # want to use those explicitly. likewise we pass config file we already found. set known # hosts and agent to None so we can not have an agent and deal w/ known hosts ourselves common_args = { \"host\": self._base_transport_args.host, \"port\": self._base_transport_args.port, \"username\": self.plugin_transport_args.auth_username, \"known_hosts\": None, \"agent_path\": None, \"config\": self.plugin_transport_args.ssh_config_file, } try: self.session: SSHClientConnection = await asyncio.wait_for( connect( client_keys=self.plugin_transport_args.auth_private_key, password=self.plugin_transport_args.auth_password, preferred_auth=( \"publickey\", \"keyboard-interactive\", \"password\", ), **common_args, ), timeout=self._base_transport_args.timeout_socket, ) except PermissionDenied as exc: msg = \"all authentication methods failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) from exc except asyncio.TimeoutError as exc: msg = \"timed out opening connection to device\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) from exc # it seems we must pass a terminal type to force a pty(?) which i think we want in like... # every case?? https://invisible-island.net/ncurses/ncurses.faq.html#xterm_color # set encoding to None so we get bytes for consistency w/ other scrapli transports # request_pty seems to be safe to set to \"false\" but leaving it at auto which seems closer # to the default behavior. With this omitted (as was previously) connecting to junos devices # on port 830 worked w/ system transport but *not* asyncssh because the pty request failed try: self.stdin, self.stdout, _ = await self.session.open_session( term_type=\"xterm\", encoding=None, subsystem=\"netconf\", request_pty=\"auto\" ) except ChannelOpenError as exc: msg = ( \"Failed to open Channel -- do you have the right port? Most often the netconf \" \"port is 22 or 830!\" ) self.logger.critical(msg) raise ScrapliConnectionNotOpened(msg) from exc if not self.session: raise ScrapliConnectionNotOpened if self.plugin_transport_args.auth_strict_key: self.logger.debug( f\"Attempting to validate {self._base_transport_args.host} public key is in known \" f\"hosts and is valid\" ) self._verify_key_value()","title":"Module scrapli_netconf.transport.plugins.asyncssh.transport"},{"location":"api_docs/transport/plugins/asyncssh/#classes","text":"","title":"Classes"},{"location":"api_docs/transport/plugins/asyncssh/#netconfasyncsshtransport","text":"1 2 3 4 5 6 7 8 9 Helper class that provides a standard way to create an ABC using inheritance. Asyncssh transport plugin. Important note: some ssh servers may refuse connections if too many ssh host key algorithms are passed to it during the connection opening -- Asyncssh sends a bunch by default! If you encounter this issue, you can simply update your SSH config file to set a smaller (or one) number of ssh host key algorithms to work around this like so: Host * HostKeyAlgorithms ssh-rsa 1 2 3 4 5 6 7 8 Thank you to @davaeron [https://github.com/davaeron] for reporting this in #173, see also asyncssh #323 here: https://github.com/ronf/asyncssh/issues/323. This transport supports some additional `transport_options` to control behavior -- `asyncssh` is a dictionary that contains options that are passed directly to asyncssh during connection creation, you can find the SSH Client options of asyncssh here: https://asyncssh.readthedocs.io/en/latest/api.html#sshclientconnectionoptions. Below is an example of passing in options to modify kex and encryption algorithms device = { \"host\": \"localhost\", \"transport_options\": { \"asyncssh\": { \"kex_algs\": [\"diffie-hellman-group14-sha1\", \"diffie-hellman-group1-sha1\"], \"encryption_algs\": [\"aes256-cbc\", \"aes192-cbc\", \"aes256-ctr\", \"aes192-ctr\"], } }, \"platform\": \"cisco_iosxe\" } conn = Scrapli(**device) 1 2 3 4 5 6 7 8 9 Args: base_transport_args: scrapli base transport plugin arguments plugin_transport_args: asyncssh ssh specific transport plugin arguments Returns: N/A Raises: N/A Expand source code class NetconfAsyncsshTransport(AsyncsshTransport): async def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: ScrapliAuthenticationFailed: if auth fails ScrapliConnectionNotOpened: if connection cant be opened (but is not an auth failure) \"\"\" if self.plugin_transport_args.auth_strict_key: self.logger.debug( f\"Attempting to validate {self._base_transport_args.host} public key is in known \" f\"hosts\" ) self._verify_key() # we already fetched host/port/user from the user input and/or the ssh config file, so we # want to use those explicitly. likewise we pass config file we already found. set known # hosts and agent to None so we can not have an agent and deal w/ known hosts ourselves common_args = { \"host\": self._base_transport_args.host, \"port\": self._base_transport_args.port, \"username\": self.plugin_transport_args.auth_username, \"known_hosts\": None, \"agent_path\": None, \"config\": self.plugin_transport_args.ssh_config_file, } try: self.session: SSHClientConnection = await asyncio.wait_for( connect( client_keys=self.plugin_transport_args.auth_private_key, password=self.plugin_transport_args.auth_password, preferred_auth=( \"publickey\", \"keyboard-interactive\", \"password\", ), **common_args, ), timeout=self._base_transport_args.timeout_socket, ) except PermissionDenied as exc: msg = \"all authentication methods failed\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) from exc except asyncio.TimeoutError as exc: msg = \"timed out opening connection to device\" self.logger.critical(msg) raise ScrapliAuthenticationFailed(msg) from exc # it seems we must pass a terminal type to force a pty(?) which i think we want in like... # every case?? https://invisible-island.net/ncurses/ncurses.faq.html#xterm_color # set encoding to None so we get bytes for consistency w/ other scrapli transports # request_pty seems to be safe to set to \"false\" but leaving it at auto which seems closer # to the default behavior. With this omitted (as was previously) connecting to junos devices # on port 830 worked w/ system transport but *not* asyncssh because the pty request failed try: self.stdin, self.stdout, _ = await self.session.open_session( term_type=\"xterm\", encoding=None, subsystem=\"netconf\", request_pty=\"auto\" ) except ChannelOpenError as exc: msg = ( \"Failed to open Channel -- do you have the right port? Most often the netconf \" \"port is 22 or 830!\" ) self.logger.critical(msg) raise ScrapliConnectionNotOpened(msg) from exc if not self.session: raise ScrapliConnectionNotOpened if self.plugin_transport_args.auth_strict_key: self.logger.debug( f\"Attempting to validate {self._base_transport_args.host} public key is in known \" f\"hosts and is valid\" ) self._verify_key_value()","title":"NetconfAsyncsshTransport"},{"location":"api_docs/transport/plugins/asyncssh/#ancestors-in-mro","text":"scrapli.transport.plugins.asyncssh.transport.AsyncsshTransport scrapli.transport.base.async_transport.AsyncTransport scrapli.transport.base.base_transport.BaseTransport abc.ABC","title":"Ancestors (in MRO)"},{"location":"api_docs/transport/plugins/asyncssh/#methods","text":"","title":"Methods"},{"location":"api_docs/transport/plugins/asyncssh/#open_netconf","text":"open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 13 Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: ScrapliAuthenticationFailed: if auth fails ScrapliConnectionNotOpened: if connection cant be opened (but is not an auth failure)","title":"open_netconf"},{"location":"api_docs/transport/plugins/paramiko/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.transport.plugins.paramiko.transport \u00b6 scrapli_netconf.transport.plugins.paramiko.transport Expand source code \"\"\"scrapli_netconf.transport.plugins.paramiko.transport\"\"\" from scrapli.exceptions import ScrapliConnectionNotOpened from scrapli.transport.plugins.paramiko.transport import ParamikoTransport, PluginTransportArgs # imported from base driver _ = PluginTransportArgs class NetconfParamikoTransport(ParamikoTransport): def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" super().open() def _open_channel(self) -> None: \"\"\" Overriding the base open_channel to invoke netconf subsystem Args: N/A Returns: None Raises: ScrapliConnectionNotOpened: if session is unopened/None \"\"\" if not self.session: raise ScrapliConnectionNotOpened self.session_channel = self.session.open_session() self._set_timeout(self._base_transport_args.timeout_transport) self.session_channel.invoke_subsystem(\"netconf\") Classes \u00b6 NetconfParamikoTransport \u00b6 1 2 Helper class that provides a standard way to create an ABC using inheritance. Expand source code class NetconfParamikoTransport(ParamikoTransport): def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" super().open() def _open_channel(self) -> None: \"\"\" Overriding the base open_channel to invoke netconf subsystem Args: N/A Returns: None Raises: ScrapliConnectionNotOpened: if session is unopened/None \"\"\" if not self.session: raise ScrapliConnectionNotOpened self.session_channel = self.session.open_session() self._set_timeout(self._base_transport_args.timeout_transport) self.session_channel.invoke_subsystem(\"netconf\") Ancestors (in MRO) \u00b6 scrapli.transport.plugins.paramiko.transport.ParamikoTransport scrapli.transport.base.sync_transport.Transport scrapli.transport.base.base_transport.BaseTransport abc.ABC Methods \u00b6 open_netconf \u00b6 open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A","title":"Paramiko"},{"location":"api_docs/transport/plugins/paramiko/#module-scrapli_netconftransportpluginsparamikotransport","text":"scrapli_netconf.transport.plugins.paramiko.transport Expand source code \"\"\"scrapli_netconf.transport.plugins.paramiko.transport\"\"\" from scrapli.exceptions import ScrapliConnectionNotOpened from scrapli.transport.plugins.paramiko.transport import ParamikoTransport, PluginTransportArgs # imported from base driver _ = PluginTransportArgs class NetconfParamikoTransport(ParamikoTransport): def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" super().open() def _open_channel(self) -> None: \"\"\" Overriding the base open_channel to invoke netconf subsystem Args: N/A Returns: None Raises: ScrapliConnectionNotOpened: if session is unopened/None \"\"\" if not self.session: raise ScrapliConnectionNotOpened self.session_channel = self.session.open_session() self._set_timeout(self._base_transport_args.timeout_transport) self.session_channel.invoke_subsystem(\"netconf\")","title":"Module scrapli_netconf.transport.plugins.paramiko.transport"},{"location":"api_docs/transport/plugins/paramiko/#classes","text":"","title":"Classes"},{"location":"api_docs/transport/plugins/paramiko/#netconfparamikotransport","text":"1 2 Helper class that provides a standard way to create an ABC using inheritance. Expand source code class NetconfParamikoTransport(ParamikoTransport): def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" super().open() def _open_channel(self) -> None: \"\"\" Overriding the base open_channel to invoke netconf subsystem Args: N/A Returns: None Raises: ScrapliConnectionNotOpened: if session is unopened/None \"\"\" if not self.session: raise ScrapliConnectionNotOpened self.session_channel = self.session.open_session() self._set_timeout(self._base_transport_args.timeout_transport) self.session_channel.invoke_subsystem(\"netconf\")","title":"NetconfParamikoTransport"},{"location":"api_docs/transport/plugins/paramiko/#ancestors-in-mro","text":"scrapli.transport.plugins.paramiko.transport.ParamikoTransport scrapli.transport.base.sync_transport.Transport scrapli.transport.base.base_transport.BaseTransport abc.ABC","title":"Ancestors (in MRO)"},{"location":"api_docs/transport/plugins/paramiko/#methods","text":"","title":"Methods"},{"location":"api_docs/transport/plugins/paramiko/#open_netconf","text":"open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A","title":"open_netconf"},{"location":"api_docs/transport/plugins/ssh2/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.transport.plugins.ssh2.transport \u00b6 scrapli_netconf.transport.plugins.ssh2.transport Expand source code \"\"\"scrapli_netconf.transport.plugins.ssh2.transport\"\"\" from scrapli.exceptions import ScrapliConnectionNotOpened from scrapli.transport.plugins.ssh2.transport import PluginTransportArgs, Ssh2Transport # imported from base driver _ = PluginTransportArgs class NetconfSsh2Transport(Ssh2Transport): def open_netconf(self) -> bytes: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" super().open() return b\"\" def _open_channel(self) -> None: \"\"\" Overriding the base open_channel to invoke netconf subsystem Args: N/A Returns: None Raises: ScrapliConnectionNotOpened: if session is unopened/None \"\"\" if not self.session: raise ScrapliConnectionNotOpened self.session_channel = self.session.open_session() self.session_channel.subsystem(\"netconf\") Classes \u00b6 NetconfSsh2Transport \u00b6 1 2 Helper class that provides a standard way to create an ABC using inheritance. Expand source code class NetconfSsh2Transport(Ssh2Transport): def open_netconf(self) -> bytes: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" super().open() return b\"\" def _open_channel(self) -> None: \"\"\" Overriding the base open_channel to invoke netconf subsystem Args: N/A Returns: None Raises: ScrapliConnectionNotOpened: if session is unopened/None \"\"\" if not self.session: raise ScrapliConnectionNotOpened self.session_channel = self.session.open_session() self.session_channel.subsystem(\"netconf\") Ancestors (in MRO) \u00b6 scrapli.transport.plugins.ssh2.transport.Ssh2Transport scrapli.transport.base.sync_transport.Transport scrapli.transport.base.base_transport.BaseTransport abc.ABC Methods \u00b6 open_netconf \u00b6 open_netconf(self) \u2011> bytes 1 2 3 4 5 6 7 8 9 10 11 12 Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A","title":"SSH2"},{"location":"api_docs/transport/plugins/ssh2/#module-scrapli_netconftransportpluginsssh2transport","text":"scrapli_netconf.transport.plugins.ssh2.transport Expand source code \"\"\"scrapli_netconf.transport.plugins.ssh2.transport\"\"\" from scrapli.exceptions import ScrapliConnectionNotOpened from scrapli.transport.plugins.ssh2.transport import PluginTransportArgs, Ssh2Transport # imported from base driver _ = PluginTransportArgs class NetconfSsh2Transport(Ssh2Transport): def open_netconf(self) -> bytes: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" super().open() return b\"\" def _open_channel(self) -> None: \"\"\" Overriding the base open_channel to invoke netconf subsystem Args: N/A Returns: None Raises: ScrapliConnectionNotOpened: if session is unopened/None \"\"\" if not self.session: raise ScrapliConnectionNotOpened self.session_channel = self.session.open_session() self.session_channel.subsystem(\"netconf\")","title":"Module scrapli_netconf.transport.plugins.ssh2.transport"},{"location":"api_docs/transport/plugins/ssh2/#classes","text":"","title":"Classes"},{"location":"api_docs/transport/plugins/ssh2/#netconfssh2transport","text":"1 2 Helper class that provides a standard way to create an ABC using inheritance. Expand source code class NetconfSsh2Transport(Ssh2Transport): def open_netconf(self) -> bytes: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" super().open() return b\"\" def _open_channel(self) -> None: \"\"\" Overriding the base open_channel to invoke netconf subsystem Args: N/A Returns: None Raises: ScrapliConnectionNotOpened: if session is unopened/None \"\"\" if not self.session: raise ScrapliConnectionNotOpened self.session_channel = self.session.open_session() self.session_channel.subsystem(\"netconf\")","title":"NetconfSsh2Transport"},{"location":"api_docs/transport/plugins/ssh2/#ancestors-in-mro","text":"scrapli.transport.plugins.ssh2.transport.Ssh2Transport scrapli.transport.base.sync_transport.Transport scrapli.transport.base.base_transport.BaseTransport abc.ABC","title":"Ancestors (in MRO)"},{"location":"api_docs/transport/plugins/ssh2/#methods","text":"","title":"Methods"},{"location":"api_docs/transport/plugins/ssh2/#open_netconf","text":"open_netconf(self) \u2011> bytes 1 2 3 4 5 6 7 8 9 10 11 12 Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A","title":"open_netconf"},{"location":"api_docs/transport/plugins/system/","text":"window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting()) Module scrapli_netconf.transport.plugins.system.transport \u00b6 scrapli_netconf.transport.plugins.system.transport Expand source code \"\"\"scrapli_netconf.transport.plugins.system.transport\"\"\" from io import BytesIO from scrapli.exceptions import ScrapliConnectionNotOpened from scrapli.transport.base import BaseTransportArgs from scrapli.transport.plugins.system.transport import PluginTransportArgs, SystemTransport # imported from base driver _ = PluginTransportArgs class NetconfSystemTransport(SystemTransport): def __init__( self, base_transport_args: BaseTransportArgs, plugin_transport_args: PluginTransportArgs ): self.write_chunk_size = 65535 super().__init__( base_transport_args=base_transport_args, plugin_transport_args=plugin_transport_args ) def _build_open_cmd(self) -> None: super()._build_open_cmd() # JunOS devices do not allocate pty on port 830 on some (all?) platforms, users can cause # system transport to *not* force the pty (forcing pty is *default behavior*) by setting the # transport arg `netconf_force_pty` to `False`. This defaults to `True` (forcing a pty) as # that has been the default behavior for a while and seems to work in almost all cases, # additionally without this -- in pytest (only in pytest for some reason?) output seems to # come from devices out of order causing all the echo check logic to break... with this pty # being forced that seems to never occur. Worth digging into more at some point... if self._base_transport_args.transport_options.get(\"netconf_force_pty\", True) is True: self.open_cmd.append(\"-tt\") self.open_cmd.extend([\"-s\", \"netconf\"]) self.logger.debug(f\"final open_cmd: {self.open_cmd}\") def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" self.open() def write(self, channel_input: bytes) -> None: if not self.session: raise ScrapliConnectionNotOpened if self.write_chunk_size < = 0: self.session.write(channel_input) else: bytes_to_send_len = len(channel_input) bytes_to_send = BytesIO(channel_input) bytes_sent = 0 while bytes_sent < bytes_to_send_len: self.session.write(bytes_to_send.read(self.write_chunk_size)) bytes_sent += self.write_chunk_size Classes \u00b6 NetconfSystemTransport \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Helper class that provides a standard way to create an ABC using inheritance. System (i.e. /bin/ssh) transport plugin. This transport supports some additional `transport_options` to control behavior -- `ptyprocess` is a dictionary that has the following options: rows: integer number of rows for ptyprocess \"window\" cols: integer number of cols for ptyprocess \"window\" echo: defaults to `True`, passing `False` disables echo in the ptyprocess; should only be used with scrapli-netconf, will break scrapli! `netconf_force_pty` is a scrapli-netconf only argument. This setting defaults to `True` and allows you to *not* force a pty. This setting seems to only be necessary when connecting to juniper devices on port 830 as junos decides to not allocate a pty on that port for some reason! Args: base_transport_args: scrapli base transport plugin arguments plugin_transport_args: system ssh specific transport plugin arguments Returns: N/A Raises: ScrapliUnsupportedPlatform: if system is windows Expand source code class NetconfSystemTransport(SystemTransport): def __init__( self, base_transport_args: BaseTransportArgs, plugin_transport_args: PluginTransportArgs ): self.write_chunk_size = 65535 super().__init__( base_transport_args=base_transport_args, plugin_transport_args=plugin_transport_args ) def _build_open_cmd(self) -> None: super()._build_open_cmd() # JunOS devices do not allocate pty on port 830 on some (all?) platforms, users can cause # system transport to *not* force the pty (forcing pty is *default behavior*) by setting the # transport arg `netconf_force_pty` to `False`. This defaults to `True` (forcing a pty) as # that has been the default behavior for a while and seems to work in almost all cases, # additionally without this -- in pytest (only in pytest for some reason?) output seems to # come from devices out of order causing all the echo check logic to break... with this pty # being forced that seems to never occur. Worth digging into more at some point... if self._base_transport_args.transport_options.get(\"netconf_force_pty\", True) is True: self.open_cmd.append(\"-tt\") self.open_cmd.extend([\"-s\", \"netconf\"]) self.logger.debug(f\"final open_cmd: {self.open_cmd}\") def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" self.open() def write(self, channel_input: bytes) -> None: if not self.session: raise ScrapliConnectionNotOpened if self.write_chunk_size < = 0: self.session.write(channel_input) else: bytes_to_send_len = len(channel_input) bytes_to_send = BytesIO(channel_input) bytes_sent = 0 while bytes_sent < bytes_to_send_len: self.session.write(bytes_to_send.read(self.write_chunk_size)) bytes_sent += self.write_chunk_size Ancestors (in MRO) \u00b6 scrapli.transport.plugins.system.transport.SystemTransport scrapli.transport.base.sync_transport.Transport scrapli.transport.base.base_transport.BaseTransport abc.ABC Methods \u00b6 open_netconf \u00b6 open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A write \u00b6 write(self, channel_input: bytes) \u2011> None 1 2 3 4 5 6 7 8 9 10 Write bytes into the transport session Args: channel_input: bytes to write to transport session Returns: None Raises: N/A","title":"System"},{"location":"api_docs/transport/plugins/system/#module-scrapli_netconftransportpluginssystemtransport","text":"scrapli_netconf.transport.plugins.system.transport Expand source code \"\"\"scrapli_netconf.transport.plugins.system.transport\"\"\" from io import BytesIO from scrapli.exceptions import ScrapliConnectionNotOpened from scrapli.transport.base import BaseTransportArgs from scrapli.transport.plugins.system.transport import PluginTransportArgs, SystemTransport # imported from base driver _ = PluginTransportArgs class NetconfSystemTransport(SystemTransport): def __init__( self, base_transport_args: BaseTransportArgs, plugin_transport_args: PluginTransportArgs ): self.write_chunk_size = 65535 super().__init__( base_transport_args=base_transport_args, plugin_transport_args=plugin_transport_args ) def _build_open_cmd(self) -> None: super()._build_open_cmd() # JunOS devices do not allocate pty on port 830 on some (all?) platforms, users can cause # system transport to *not* force the pty (forcing pty is *default behavior*) by setting the # transport arg `netconf_force_pty` to `False`. This defaults to `True` (forcing a pty) as # that has been the default behavior for a while and seems to work in almost all cases, # additionally without this -- in pytest (only in pytest for some reason?) output seems to # come from devices out of order causing all the echo check logic to break... with this pty # being forced that seems to never occur. Worth digging into more at some point... if self._base_transport_args.transport_options.get(\"netconf_force_pty\", True) is True: self.open_cmd.append(\"-tt\") self.open_cmd.extend([\"-s\", \"netconf\"]) self.logger.debug(f\"final open_cmd: {self.open_cmd}\") def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" self.open() def write(self, channel_input: bytes) -> None: if not self.session: raise ScrapliConnectionNotOpened if self.write_chunk_size < = 0: self.session.write(channel_input) else: bytes_to_send_len = len(channel_input) bytes_to_send = BytesIO(channel_input) bytes_sent = 0 while bytes_sent < bytes_to_send_len: self.session.write(bytes_to_send.read(self.write_chunk_size)) bytes_sent += self.write_chunk_size","title":"Module scrapli_netconf.transport.plugins.system.transport"},{"location":"api_docs/transport/plugins/system/#classes","text":"","title":"Classes"},{"location":"api_docs/transport/plugins/system/#netconfsystemtransport","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Helper class that provides a standard way to create an ABC using inheritance. System (i.e. /bin/ssh) transport plugin. This transport supports some additional `transport_options` to control behavior -- `ptyprocess` is a dictionary that has the following options: rows: integer number of rows for ptyprocess \"window\" cols: integer number of cols for ptyprocess \"window\" echo: defaults to `True`, passing `False` disables echo in the ptyprocess; should only be used with scrapli-netconf, will break scrapli! `netconf_force_pty` is a scrapli-netconf only argument. This setting defaults to `True` and allows you to *not* force a pty. This setting seems to only be necessary when connecting to juniper devices on port 830 as junos decides to not allocate a pty on that port for some reason! Args: base_transport_args: scrapli base transport plugin arguments plugin_transport_args: system ssh specific transport plugin arguments Returns: N/A Raises: ScrapliUnsupportedPlatform: if system is windows Expand source code class NetconfSystemTransport(SystemTransport): def __init__( self, base_transport_args: BaseTransportArgs, plugin_transport_args: PluginTransportArgs ): self.write_chunk_size = 65535 super().__init__( base_transport_args=base_transport_args, plugin_transport_args=plugin_transport_args ) def _build_open_cmd(self) -> None: super()._build_open_cmd() # JunOS devices do not allocate pty on port 830 on some (all?) platforms, users can cause # system transport to *not* force the pty (forcing pty is *default behavior*) by setting the # transport arg `netconf_force_pty` to `False`. This defaults to `True` (forcing a pty) as # that has been the default behavior for a while and seems to work in almost all cases, # additionally without this -- in pytest (only in pytest for some reason?) output seems to # come from devices out of order causing all the echo check logic to break... with this pty # being forced that seems to never occur. Worth digging into more at some point... if self._base_transport_args.transport_options.get(\"netconf_force_pty\", True) is True: self.open_cmd.append(\"-tt\") self.open_cmd.extend([\"-s\", \"netconf\"]) self.logger.debug(f\"final open_cmd: {self.open_cmd}\") def open_netconf(self) -> None: \"\"\" Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A \"\"\" self.open() def write(self, channel_input: bytes) -> None: if not self.session: raise ScrapliConnectionNotOpened if self.write_chunk_size < = 0: self.session.write(channel_input) else: bytes_to_send_len = len(channel_input) bytes_to_send = BytesIO(channel_input) bytes_sent = 0 while bytes_sent < bytes_to_send_len: self.session.write(bytes_to_send.read(self.write_chunk_size)) bytes_sent += self.write_chunk_size","title":"NetconfSystemTransport"},{"location":"api_docs/transport/plugins/system/#ancestors-in-mro","text":"scrapli.transport.plugins.system.transport.SystemTransport scrapli.transport.base.sync_transport.Transport scrapli.transport.base.base_transport.BaseTransport abc.ABC","title":"Ancestors (in MRO)"},{"location":"api_docs/transport/plugins/system/#methods","text":"","title":"Methods"},{"location":"api_docs/transport/plugins/system/#open_netconf","text":"open_netconf(self) \u2011> None 1 2 3 4 5 6 7 8 9 10 11 12 Netconf open method Simply calls the \"normal\" open method, but retaining an explicit \"netconf\" open for sanity Args: N/A Returns: None Raises: N/A","title":"open_netconf"},{"location":"api_docs/transport/plugins/system/#write","text":"write(self, channel_input: bytes) \u2011> None 1 2 3 4 5 6 7 8 9 10 Write bytes into the transport session Args: channel_input: bytes to write to transport session Returns: None Raises: N/A","title":"write"},{"location":"more_scrapli/nornir_scrapli/","text":"Nornir scrapli \u00b6 If you want to use scrapli, but don't want to deal with handling concurrency yourself, there is great news! The nornir_scrapli plugin allows you to use scrapli (and scrapli netconf and scrapli cfg) within the Nornir framework!","title":"Nornir Scrapli"},{"location":"more_scrapli/nornir_scrapli/#nornir-scrapli","text":"If you want to use scrapli, but don't want to deal with handling concurrency yourself, there is great news! The nornir_scrapli plugin allows you to use scrapli (and scrapli netconf and scrapli cfg) within the Nornir framework!","title":"Nornir scrapli"},{"location":"more_scrapli/scrapli/","text":"Scrapli \u00b6 scrapli ( docs ) is the \"parent\" scrapli library. Check it out if you need to connect to devices with telnet or ssh!","title":"Scrapli"},{"location":"more_scrapli/scrapli/#scrapli","text":"scrapli ( docs ) is the \"parent\" scrapli library. Check it out if you need to connect to devices with telnet or ssh!","title":"Scrapli"},{"location":"more_scrapli/scrapli_cfg/","text":"Scrapli Cfg \u00b6 scrapli_cfg ( docs ) is utility that accepts a scrapli Telnet or SSH connection and provides configuration management capabilities. scrapli_cfg allows you to load candidate configurations for merge or replace operations, generate diffs of the current vs candidate, and of course commit or abort the candidate configuration.","title":"Scrapli Cfg"},{"location":"more_scrapli/scrapli_cfg/#scrapli-cfg","text":"scrapli_cfg ( docs ) is utility that accepts a scrapli Telnet or SSH connection and provides configuration management capabilities. scrapli_cfg allows you to load candidate configurations for merge or replace operations, generate diffs of the current vs candidate, and of course commit or abort the candidate configuration.","title":"Scrapli Cfg"},{"location":"more_scrapli/scrapli_community/","text":"Scrapli Community \u00b6 If you would like to use scrapli, but the platform(s) that you work with are not supported in the \"core\" scrapli platforms, you should check out scrapli_community ! This is the place for users to share \"non-core\" scrapli platforms.","title":"Scrapli Community"},{"location":"more_scrapli/scrapli_community/#scrapli-community","text":"If you would like to use scrapli, but the platform(s) that you work with are not supported in the \"core\" scrapli platforms, you should check out scrapli_community ! This is the place for users to share \"non-core\" scrapli platforms.","title":"Scrapli Community"},{"location":"more_scrapli/scrapli_replay/","text":"Scrapli Replay \u00b6 scrapli_replay ( docs ) is a set of tools used to help test scrapli programs. scrapli_replay includes a utility to capture command input/output from real life servers and replay them in a semi-interactive fashion, as well as a pytest plugin that patches and records and replays session data (like vcr.py ) for scrapli connections.","title":"Scrapli Replay"},{"location":"more_scrapli/scrapli_replay/#scrapli-replay","text":"scrapli_replay ( docs ) is a set of tools used to help test scrapli programs. scrapli_replay includes a utility to capture command input/output from real life servers and replay them in a semi-interactive fashion, as well as a pytest plugin that patches and records and replays session data (like vcr.py ) for scrapli connections.","title":"Scrapli Replay"},{"location":"user_guide/advanced_usage/","text":"Advanced Usage \u00b6 Capabilities \u00b6 Netconf capabilities are exchanged when the session is opened. scrapli_netconf stores the server's capabilities in the aptly named server_capabilities attribute of the driver. 1 2 3 4 5 6 7 8 9 10 11 12 13 >>> from scrapli_netconf.driver import NetconfDriver >>> >>> my_device = { ... \"host\" : \"172.18.0.13\" , ... \"auth_username\" : \"vrnetlab\" , ... \"auth_password\" : \"VR-netlab9\" , ... \"auth_strict_key\" : False , ... \"port\" : 830 ... } >>> conn = NetconfDriver ( ** my_device ) >>> conn . open () >>> conn . server_capabilities [ 'urn:ietf:params:netconf:base:1.1' , 'urn:ietf:params:netconf:capability:candidate:1.0' ] Capabilities truncated for readability As for capabilities that scrapli_netconf sends to the server, that depends on the capabilities advertised from the server! If netconf base 1.1 is in the advertised capabilities then scrapli_netconf will advertise netconf 1.1 capabilities, otherwise it will advertise 1.0 capabilities. Datastores \u00b6 scrapli_netconf drives contain an option strict_datastores which defaults to False . If this option is set to True scrapli will raise a ValueError when attempting to perform an operation against a datastore that has not been advertised as a capability by the server. With this option left to the default value of False , scrapli_netconf will simply issue a user warning. Using a Different Transport \u00b6 Just like scrapli \"core\" -- scrapli-netconf supports using different libraries for \"transport\" -- or the actual SSH communication piece. By default, and like scrapli \"core\", scrapli-netconf uses the \"system\" transport. This \"system \" transport means that scrapli-netconf has no external dependencies (other than lxml !) as it just relies on what is available on the machine running the scrapli script. If you wish to swap this out, scrapli-netconf also supports the paramiko , ssh2 , and asyncssh scrapli transport plugins. Like scrapli \"core\", transport selection can be made when instantiating the scrapli connection object by passing in paramiko , ssh2 , asyncssh \" to force scrapli to use the corresponding transport mechanism. If you are using the asyncssh transport you must use the AsyncNetconfScrape driver! While it will be a goal to ensure that these other transport mechanisms are supported and useful, the focus of scrapli development will be on the \"system\" SSH transport. Example using ssh2 as the transport: 1 2 3 4 5 6 7 8 9 10 11 12 from scrapli_netconf import NetconfDriver my_device = { \"host\" : \"172.18.0.11\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"transport\" : \"ssh2\" } with NetconfDriver ( ** my_device ) as conn : print ( conn . get_config ()) A Note about Filters \u00b6 The filter_ string value for the get and get_config methods may contain multiple xml elements at its \"root\" (for subtree filters) -- when cast to an lxml etree object this would normally result in the first filter being the only element in the resulting object. This is because etree.fromstring assumes (rather correctly) that this is the root of the document, and it ignores the remaining filter elements. In example, given the following string data: 1 2 3 4 5 6 7 <interface-configurations xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-ifmgr-cfg\"> <interface-configuration> <active>act</active> </interface-configuration> </interface-configurations> <netconf-yang xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-man-netconf-cfg\"> </netconf-yang> The resulting lxml object (when re-dumped back to string) would look like this: 1 2 3 4 5 <interface-configurations xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-ifmgr-cfg\"> <interface-configuration> <active>act</active> </interface-configuration> </interface-configurations> This is.... shall we say not ideal if we want to pass in a string like the previous section to filter for multiple things in our config. To cope with this scrapli_netconf will wrap the user provided string filter in a \"tmp\" tag, which allows us to load up the filter with all element(s) intact; we then simply ditch the outer temp tag when placing the filter element(s) into the final filter payload, allowing users to simply provide a big string containing as many or few filters as they want. If you preferred to craft your payloads more... \"correctly\" shall we say, then you are welcome to do so, and provide the valid lxml object to the rpc method. The rpc method does nothing but wrap the provided element in the outer-most xml tags needed for a NETCONF payload, so your provided element would need to contain the get/filter/edit/etc. tags as appropriate!","title":"Advanced Usage"},{"location":"user_guide/advanced_usage/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"user_guide/advanced_usage/#capabilities","text":"Netconf capabilities are exchanged when the session is opened. scrapli_netconf stores the server's capabilities in the aptly named server_capabilities attribute of the driver. 1 2 3 4 5 6 7 8 9 10 11 12 13 >>> from scrapli_netconf.driver import NetconfDriver >>> >>> my_device = { ... \"host\" : \"172.18.0.13\" , ... \"auth_username\" : \"vrnetlab\" , ... \"auth_password\" : \"VR-netlab9\" , ... \"auth_strict_key\" : False , ... \"port\" : 830 ... } >>> conn = NetconfDriver ( ** my_device ) >>> conn . open () >>> conn . server_capabilities [ 'urn:ietf:params:netconf:base:1.1' , 'urn:ietf:params:netconf:capability:candidate:1.0' ] Capabilities truncated for readability As for capabilities that scrapli_netconf sends to the server, that depends on the capabilities advertised from the server! If netconf base 1.1 is in the advertised capabilities then scrapli_netconf will advertise netconf 1.1 capabilities, otherwise it will advertise 1.0 capabilities.","title":"Capabilities"},{"location":"user_guide/advanced_usage/#datastores","text":"scrapli_netconf drives contain an option strict_datastores which defaults to False . If this option is set to True scrapli will raise a ValueError when attempting to perform an operation against a datastore that has not been advertised as a capability by the server. With this option left to the default value of False , scrapli_netconf will simply issue a user warning.","title":"Datastores"},{"location":"user_guide/advanced_usage/#using-a-different-transport","text":"Just like scrapli \"core\" -- scrapli-netconf supports using different libraries for \"transport\" -- or the actual SSH communication piece. By default, and like scrapli \"core\", scrapli-netconf uses the \"system\" transport. This \"system \" transport means that scrapli-netconf has no external dependencies (other than lxml !) as it just relies on what is available on the machine running the scrapli script. If you wish to swap this out, scrapli-netconf also supports the paramiko , ssh2 , and asyncssh scrapli transport plugins. Like scrapli \"core\", transport selection can be made when instantiating the scrapli connection object by passing in paramiko , ssh2 , asyncssh \" to force scrapli to use the corresponding transport mechanism. If you are using the asyncssh transport you must use the AsyncNetconfScrape driver! While it will be a goal to ensure that these other transport mechanisms are supported and useful, the focus of scrapli development will be on the \"system\" SSH transport. Example using ssh2 as the transport: 1 2 3 4 5 6 7 8 9 10 11 12 from scrapli_netconf import NetconfDriver my_device = { \"host\" : \"172.18.0.11\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"transport\" : \"ssh2\" } with NetconfDriver ( ** my_device ) as conn : print ( conn . get_config ())","title":"Using a Different Transport"},{"location":"user_guide/advanced_usage/#a-note-about-filters","text":"The filter_ string value for the get and get_config methods may contain multiple xml elements at its \"root\" (for subtree filters) -- when cast to an lxml etree object this would normally result in the first filter being the only element in the resulting object. This is because etree.fromstring assumes (rather correctly) that this is the root of the document, and it ignores the remaining filter elements. In example, given the following string data: 1 2 3 4 5 6 7 <interface-configurations xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-ifmgr-cfg\"> <interface-configuration> <active>act</active> </interface-configuration> </interface-configurations> <netconf-yang xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-man-netconf-cfg\"> </netconf-yang> The resulting lxml object (when re-dumped back to string) would look like this: 1 2 3 4 5 <interface-configurations xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-ifmgr-cfg\"> <interface-configuration> <active>act</active> </interface-configuration> </interface-configurations> This is.... shall we say not ideal if we want to pass in a string like the previous section to filter for multiple things in our config. To cope with this scrapli_netconf will wrap the user provided string filter in a \"tmp\" tag, which allows us to load up the filter with all element(s) intact; we then simply ditch the outer temp tag when placing the filter element(s) into the final filter payload, allowing users to simply provide a big string containing as many or few filters as they want. If you preferred to craft your payloads more... \"correctly\" shall we say, then you are welcome to do so, and provide the valid lxml object to the rpc method. The rpc method does nothing but wrap the provided element in the outer-most xml tags needed for a NETCONF payload, so your provided element would need to contain the get/filter/edit/etc. tags as appropriate!","title":"A Note about Filters"},{"location":"user_guide/basic_usage/","text":"Basic Usage \u00b6 Picking the right Driver \u00b6 Because netconf is a standard we don't need to deal with \"platform\" type drivers for scrapli_netconf! Instead, there are only two options -- NetconfDriver or AsyncNetconfDriver , these can be imported from scrapli_netconf.driver like so: 1 2 from scrapli_netconf.driver import NetconfDriver from scrapli_netconf.driver import AsyncNetconfDriver Note: if you are using async you must set the transport to asyncssh -- this is the only async transport supported at this time! Basic Driver Arguments \u00b6 The drivers of course need some information about the device you are trying to connect to. The most common arguments to provide to the driver are outlined below: Argument Purpose/Value host name/ip of host to connect to port port of host to connect to (defaults to port 830) auth_username username for authentication auth_password password for authentication auth_secondary password for secondary authentication (enable password) auth_private_key private key for authentication auth_strict_key strict key checking -- TRUE by default! ssh_config_file True/False or path to ssh config file to use strip_namespaces True/False strip namespaces from returned XML (default False) These arguments may be passed as keyword arguments to the driver of your choice, or, commonly are passed via dictionary unpacking as show below: 1 2 3 4 5 6 7 8 9 10 11 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.11\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , } conn = NetconfDriver ( ** my_device ) conn . open () NOTE that scrapli enables strict host key checking by default! Opening and Closing a Connection \u00b6 scrapli_netconf does not open the connection for you when creating your scrapli connection object in normal operations , you must manually call the open method prior to sending any commands to the device as shown below. ```python from scrapli_netconf.driver import NetconfDriver my_device = { \"host\": \"172.18.0.11\", \"auth_username\": \"vrnetlab\", \"auth_password\": \"VR-netlab9\", \"auth_strict_key\": False, } conn = NetconfDriver(**my_device) conn.open() response = conn.get_config(source=\"running\") 1 2 3 4 Connections can be closed by calling the `close` method: ```python conn.close() Get Config \u00b6 Configurations can be retrieved from datastores on a netconf device using the get-config netconf method. The get_config method accepts a source argument which must refer to an available datastore on the device -- options for this would be one of: running startup candidate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () response = conn . get_config ( source = \"running\" ) print ( response . result ) Get \u00b6 Much like get-config , the get method can be used to get data from the device, generally \"operational\" or \"show \" type data. The get method requires a \"filter\" to be applied in order to identify the data to get -- this filter can be one of two types -- \"subtree\" (default) or \"xpath\". In the context of network devices it seems that not many devices support \"xpath\" filters (only IOSXE with netconf 1.1 of the tested platforms supports xpath for example). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } filter_ = \"\"\" <components xmlns=\"http://openconfig.net/yang/platform\"> <component> <state> </state> </component> </components>\"\"\" conn = NetconfDriver ( ** my_device ) conn . open () response = conn . get ( filter_ = filter_ , filter_type = \"subtree\" ) print ( response . result ) Lock and Unlock \u00b6 Netconf provides the ability to lock a configuration datastore. Much like get-config a target datastore must be provided, and is dependent on the capabilities of the platform you are interacting with. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () response = conn . lock ( target = \"candidate\" ) print ( response . result ) response = conn . unlock ( target = \"candidate\" ) print ( response . result ) Commit and Discard \u00b6 If your platform supports commit operations (IOS-XR and Junos in the context of scrapli_netconf tested platforms ), any changes created using the edit-config method will need to be committed (or discarded). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () response = conn . commit () print ( response . result ) response = conn . discard () print ( response . result ) Edit Config \u00b6 To edit configs, simply use the edit_config method with an appropriate config payload and target. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } cdp_config = \"\"\" <config> <cdp xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-cdp-cfg\"> <timer>80</timer> <enable>true</enable> <log-adjacency></log-adjacency> <hold-time>200</hold-time> <advertise-v1-only></advertise-v1-only> </cdp> </config> \"\"\" conn = NetconfDriver ( ** my_device ) conn . open () result = conn . edit_config ( config = cdp_config , target = \"candidate\" ) print ( result . result ) Delete Config \u00b6 Some devices may allow you to delete a candidate/startup configuration. You can do so with the delete_config method ; note that this is only currently tested on Junos as the test environment IOSXR version does not support this method . Per the RFC, \"running\" is never a valid target; scrapli_netconf will produce a warning indicating this if \"running\" is set as the target; if strict_datastores is set to True an exception will be raised. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () result = conn . delete_config ( target = \"candidate\" ) print ( result . result ) RPC \u00b6 The rpc method is a \"bare-bones\" rpc call which does not apply any formatting/standardization beyond the outer most rpc tag. Generally this is used for Juniper devices and the \"bare rpc\" type calls supported on junos devices not supporting/using models (YANG/IETF/etc.), but can of course be used to send any kind of custom crafted rpc you'd like! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.15\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 22 } commit_filter = \"\"\" <get-commit-revision-information> <level>detail</level> </get-commit-revision-information> \"\"\" conn = NetconfDriver ( ** my_device ) conn . open () result = conn . rpc ( filter_ = commit_filter ) print ( result . result )","title":"Basic Usage"},{"location":"user_guide/basic_usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"user_guide/basic_usage/#picking-the-right-driver","text":"Because netconf is a standard we don't need to deal with \"platform\" type drivers for scrapli_netconf! Instead, there are only two options -- NetconfDriver or AsyncNetconfDriver , these can be imported from scrapli_netconf.driver like so: 1 2 from scrapli_netconf.driver import NetconfDriver from scrapli_netconf.driver import AsyncNetconfDriver Note: if you are using async you must set the transport to asyncssh -- this is the only async transport supported at this time!","title":"Picking the right Driver"},{"location":"user_guide/basic_usage/#basic-driver-arguments","text":"The drivers of course need some information about the device you are trying to connect to. The most common arguments to provide to the driver are outlined below: Argument Purpose/Value host name/ip of host to connect to port port of host to connect to (defaults to port 830) auth_username username for authentication auth_password password for authentication auth_secondary password for secondary authentication (enable password) auth_private_key private key for authentication auth_strict_key strict key checking -- TRUE by default! ssh_config_file True/False or path to ssh config file to use strip_namespaces True/False strip namespaces from returned XML (default False) These arguments may be passed as keyword arguments to the driver of your choice, or, commonly are passed via dictionary unpacking as show below: 1 2 3 4 5 6 7 8 9 10 11 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.11\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , } conn = NetconfDriver ( ** my_device ) conn . open () NOTE that scrapli enables strict host key checking by default!","title":"Basic Driver Arguments"},{"location":"user_guide/basic_usage/#opening-and-closing-a-connection","text":"scrapli_netconf does not open the connection for you when creating your scrapli connection object in normal operations , you must manually call the open method prior to sending any commands to the device as shown below. ```python from scrapli_netconf.driver import NetconfDriver my_device = { \"host\": \"172.18.0.11\", \"auth_username\": \"vrnetlab\", \"auth_password\": \"VR-netlab9\", \"auth_strict_key\": False, } conn = NetconfDriver(**my_device) conn.open() response = conn.get_config(source=\"running\") 1 2 3 4 Connections can be closed by calling the `close` method: ```python conn.close()","title":"Opening and Closing a Connection"},{"location":"user_guide/basic_usage/#get-config","text":"Configurations can be retrieved from datastores on a netconf device using the get-config netconf method. The get_config method accepts a source argument which must refer to an available datastore on the device -- options for this would be one of: running startup candidate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () response = conn . get_config ( source = \"running\" ) print ( response . result )","title":"Get Config"},{"location":"user_guide/basic_usage/#get","text":"Much like get-config , the get method can be used to get data from the device, generally \"operational\" or \"show \" type data. The get method requires a \"filter\" to be applied in order to identify the data to get -- this filter can be one of two types -- \"subtree\" (default) or \"xpath\". In the context of network devices it seems that not many devices support \"xpath\" filters (only IOSXE with netconf 1.1 of the tested platforms supports xpath for example). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } filter_ = \"\"\" <components xmlns=\"http://openconfig.net/yang/platform\"> <component> <state> </state> </component> </components>\"\"\" conn = NetconfDriver ( ** my_device ) conn . open () response = conn . get ( filter_ = filter_ , filter_type = \"subtree\" ) print ( response . result )","title":"Get"},{"location":"user_guide/basic_usage/#lock-and-unlock","text":"Netconf provides the ability to lock a configuration datastore. Much like get-config a target datastore must be provided, and is dependent on the capabilities of the platform you are interacting with. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () response = conn . lock ( target = \"candidate\" ) print ( response . result ) response = conn . unlock ( target = \"candidate\" ) print ( response . result )","title":"Lock and Unlock"},{"location":"user_guide/basic_usage/#commit-and-discard","text":"If your platform supports commit operations (IOS-XR and Junos in the context of scrapli_netconf tested platforms ), any changes created using the edit-config method will need to be committed (or discarded). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () response = conn . commit () print ( response . result ) response = conn . discard () print ( response . result )","title":"Commit and Discard"},{"location":"user_guide/basic_usage/#edit-config","text":"To edit configs, simply use the edit_config method with an appropriate config payload and target. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } cdp_config = \"\"\" <config> <cdp xmlns=\"http://cisco.com/ns/yang/Cisco-IOS-XR-cdp-cfg\"> <timer>80</timer> <enable>true</enable> <log-adjacency></log-adjacency> <hold-time>200</hold-time> <advertise-v1-only></advertise-v1-only> </cdp> </config> \"\"\" conn = NetconfDriver ( ** my_device ) conn . open () result = conn . edit_config ( config = cdp_config , target = \"candidate\" ) print ( result . result )","title":"Edit Config"},{"location":"user_guide/basic_usage/#delete-config","text":"Some devices may allow you to delete a candidate/startup configuration. You can do so with the delete_config method ; note that this is only currently tested on Junos as the test environment IOSXR version does not support this method . Per the RFC, \"running\" is never a valid target; scrapli_netconf will produce a warning indicating this if \"running\" is set as the target; if strict_datastores is set to True an exception will be raised. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () result = conn . delete_config ( target = \"candidate\" ) print ( result . result )","title":"Delete Config"},{"location":"user_guide/basic_usage/#rpc","text":"The rpc method is a \"bare-bones\" rpc call which does not apply any formatting/standardization beyond the outer most rpc tag. Generally this is used for Juniper devices and the \"bare rpc\" type calls supported on junos devices not supporting/using models (YANG/IETF/etc.), but can of course be used to send any kind of custom crafted rpc you'd like! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.15\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 22 } commit_filter = \"\"\" <get-commit-revision-information> <level>detail</level> </get-commit-revision-information> \"\"\" conn = NetconfDriver ( ** my_device ) conn . open () result = conn . rpc ( filter_ = commit_filter ) print ( result . result )","title":"RPC"},{"location":"user_guide/faq/","text":"FAQ \u00b6 Question: Why build this? ncclient exists? Answer: After building scrapli it was apparent that it could be fairly easily extended to handle netconf connections, at the time dayjob$ had lots of netconf-y things with ncclient happening. I'm not a big fan of ncclient as I find it rather obtuse/hard to understand whats going on, and the dependency on paramiko is not super great. I figured I could support enough netconf things with system transport so... I did. Then it was fairly trivial to add asyncssh to support netconf with asyncio! Question: Is this better than ncclient? Answer: Nope! Supporting asyncio may be a killer use case for some, but otherwise ncclient and scrapli_netconf accomplish much of the same things -- probably with ncclient having a wider/deeper range of netconf rfc support . Net/net though is they are just different! Use whichever you prefer! Question: Is this easy to use? Answer: Biased, but I think so! A big part of the goal of all of this was to have a consistent feel across ssh and netconf both with sync and async support, and (again, biased) I think that has been achieved. Other questions? Ask away!","title":"FAQ"},{"location":"user_guide/faq/#faq","text":"Question: Why build this? ncclient exists? Answer: After building scrapli it was apparent that it could be fairly easily extended to handle netconf connections, at the time dayjob$ had lots of netconf-y things with ncclient happening. I'm not a big fan of ncclient as I find it rather obtuse/hard to understand whats going on, and the dependency on paramiko is not super great. I figured I could support enough netconf things with system transport so... I did. Then it was fairly trivial to add asyncssh to support netconf with asyncio! Question: Is this better than ncclient? Answer: Nope! Supporting asyncio may be a killer use case for some, but otherwise ncclient and scrapli_netconf accomplish much of the same things -- probably with ncclient having a wider/deeper range of netconf rfc support . Net/net though is they are just different! Use whichever you prefer! Question: Is this easy to use? Answer: Biased, but I think so! A big part of the goal of all of this was to have a consistent feel across ssh and netconf both with sync and async support, and (again, biased) I think that has been achieved. Other questions? Ask away!","title":"FAQ"},{"location":"user_guide/installation/","text":"Installation \u00b6 Standard Installation \u00b6 As outlined in the quick start, you should be able to pip install scrapli_netconf \"normally\": 1 pip install scrapli_netconf Installing current master branch \u00b6 To install from the source repositories master branch: 1 pip install git+https://github.com/scrapli/scrapli_netconf Installing current develop branch \u00b6 To install from this repositories develop branch: 1 pip install -e git+https://github.com/scrapli/scrapli_netconf.git@develop#egg=scrapli_netconf Installation from Source \u00b6 To install from source: 1 2 3 git clone https://github.com/scrapli/scrapli_netconf cd scrapli_netconf python setup.py install Optional Extras \u00b6 Just like scrapli \"core\" scrapli_netconf tries to have as few dependencies as possible. scrapli_netconf requires scrapli (of course!) and lxml . If you would like to use any of the transport plugins that are not part of the standard library you can install those as optional extras via pip: 1 pip install scrapli_netconf[paramiko] The available optional installation extras options are: paramiko ssh2 asyncssh Supported Platforms \u00b6 As for platforms to run scrapli on -- it has and will be tested on MacOS and Ubuntu regularly and should work on any POSIX system. Windows at one point was being tested very minimally via GitHub Actions builds, however this is no longer the case as it is just not worth the effort. While scrapli/scrapli_netconf should work on Windows when using the paramiko or ssh2-python transport drivers, it is not \"officially\" supported. It is strongly recommended/preferred for folks to use WSL/Cygwin instead of Windows.","title":"Installation"},{"location":"user_guide/installation/#installation","text":"","title":"Installation"},{"location":"user_guide/installation/#standard-installation","text":"As outlined in the quick start, you should be able to pip install scrapli_netconf \"normally\": 1 pip install scrapli_netconf","title":"Standard Installation"},{"location":"user_guide/installation/#installing-current-master-branch","text":"To install from the source repositories master branch: 1 pip install git+https://github.com/scrapli/scrapli_netconf","title":"Installing current master branch"},{"location":"user_guide/installation/#installing-current-develop-branch","text":"To install from this repositories develop branch: 1 pip install -e git+https://github.com/scrapli/scrapli_netconf.git@develop#egg=scrapli_netconf","title":"Installing current develop branch"},{"location":"user_guide/installation/#installation-from-source","text":"To install from source: 1 2 3 git clone https://github.com/scrapli/scrapli_netconf cd scrapli_netconf python setup.py install","title":"Installation from Source"},{"location":"user_guide/installation/#optional-extras","text":"Just like scrapli \"core\" scrapli_netconf tries to have as few dependencies as possible. scrapli_netconf requires scrapli (of course!) and lxml . If you would like to use any of the transport plugins that are not part of the standard library you can install those as optional extras via pip: 1 pip install scrapli_netconf[paramiko] The available optional installation extras options are: paramiko ssh2 asyncssh","title":"Optional Extras"},{"location":"user_guide/installation/#supported-platforms","text":"As for platforms to run scrapli on -- it has and will be tested on MacOS and Ubuntu regularly and should work on any POSIX system. Windows at one point was being tested very minimally via GitHub Actions builds, however this is no longer the case as it is just not worth the effort. While scrapli/scrapli_netconf should work on Windows when using the paramiko or ssh2-python transport drivers, it is not \"officially\" supported. It is strongly recommended/preferred for folks to use WSL/Cygwin instead of Windows.","title":"Supported Platforms"},{"location":"user_guide/project_details/","text":"Project Details \u00b6 What is scrapli_netconf \u00b6 scrapli_netconf is a library to help send or receive netconf messages to devices, specifically routers (though could be anything speaking netconf in theory). Netconf is an IETF network management protocol that uses XML for message encoding, and SSH (or TLS, which is not supported by scrapli_netconf) for transport of messages. scrapli_netconf is simply an extension of the scrapli \"screen scraping\" library that adds proper message creation, framing, and validation to allow for scrapli to be used as a netconf client. scrapli_netconf adds new drivers ( NetconfScrape and AsyncNetconfScrape ), new transports ( NetconfTransport and AsyncNetconfTransport ), and new channels ( NetconfChannel and AsyncNetconfChannel ) all of which inherit from , and build on, the core scrapli components. scrapli_netconf also includes an extension of the Response object -- aptly named NetconfResponse that adds netconf-specific data to the existing object. A great question to ask right now is: \"why\"! The primary driver is to get ncclient like functionality without needing paramiko for the transport so that we can take full advantage of \"normal\" OpenSSH options, as well as have fewer dependencies (only absolute required dependency is lxml!). Additionally, as scrapli_netconf is just an extension of scrapli, this means that automation of devices over telnet, SSH, and netconf (over SSH) can be done all with an extremely consistent look and feel. Realistically this should cover most modes of present day network automation other than HTTP based APIs (which would likely have a pretty different look and feel anyway). Finally , but still quite important -- with the asyncssh transport plugin, scrapli_netconf provides asyncio support for netconf operations. Supported Platforms \u00b6 At this time scrapli_netconf is a base implementation of netconf 1.0 and netconf 1.1 (note that scrapli is not 100 % RFC compliant in that it currently does not support all methods/options). It should work on anything that runs those versions of netconf, but has only been tested against the following platforms/versions: Cisco IOS-XE (tested on: 16.12.03) with Netconf 1.0 and 1.1 Cisco IOS-XR (tested on: 6.5.3) with Netconf 1.1 Juniper JunOS (tested on: 17.3R2.10) with Netconf 1.0 In addition to the above devices, there has been testing on various versions of Juniper SRX, QFX, and MX platforms on ~18ish+ code, as well as Cisco NCS devices on 6.6.2+ code, and finally there has been limited testing on Nokia devices.","title":"Project Details"},{"location":"user_guide/project_details/#project-details","text":"","title":"Project Details"},{"location":"user_guide/project_details/#what-is-scrapli_netconf","text":"scrapli_netconf is a library to help send or receive netconf messages to devices, specifically routers (though could be anything speaking netconf in theory). Netconf is an IETF network management protocol that uses XML for message encoding, and SSH (or TLS, which is not supported by scrapli_netconf) for transport of messages. scrapli_netconf is simply an extension of the scrapli \"screen scraping\" library that adds proper message creation, framing, and validation to allow for scrapli to be used as a netconf client. scrapli_netconf adds new drivers ( NetconfScrape and AsyncNetconfScrape ), new transports ( NetconfTransport and AsyncNetconfTransport ), and new channels ( NetconfChannel and AsyncNetconfChannel ) all of which inherit from , and build on, the core scrapli components. scrapli_netconf also includes an extension of the Response object -- aptly named NetconfResponse that adds netconf-specific data to the existing object. A great question to ask right now is: \"why\"! The primary driver is to get ncclient like functionality without needing paramiko for the transport so that we can take full advantage of \"normal\" OpenSSH options, as well as have fewer dependencies (only absolute required dependency is lxml!). Additionally, as scrapli_netconf is just an extension of scrapli, this means that automation of devices over telnet, SSH, and netconf (over SSH) can be done all with an extremely consistent look and feel. Realistically this should cover most modes of present day network automation other than HTTP based APIs (which would likely have a pretty different look and feel anyway). Finally , but still quite important -- with the asyncssh transport plugin, scrapli_netconf provides asyncio support for netconf operations.","title":"What is scrapli_netconf"},{"location":"user_guide/project_details/#supported-platforms","text":"At this time scrapli_netconf is a base implementation of netconf 1.0 and netconf 1.1 (note that scrapli is not 100 % RFC compliant in that it currently does not support all methods/options). It should work on anything that runs those versions of netconf, but has only been tested against the following platforms/versions: Cisco IOS-XE (tested on: 16.12.03) with Netconf 1.0 and 1.1 Cisco IOS-XR (tested on: 6.5.3) with Netconf 1.1 Juniper JunOS (tested on: 17.3R2.10) with Netconf 1.0 In addition to the above devices, there has been testing on various versions of Juniper SRX, QFX, and MX platforms on ~18ish+ code, as well as Cisco NCS devices on 6.6.2+ code, and finally there has been limited testing on Nokia devices.","title":"Supported Platforms"},{"location":"user_guide/quickstart/","text":"Quick Start Guide \u00b6 Installation \u00b6 In most cases installation via pip is the simplest and best way to install scrapli_netconf. See here for advanced installation details. 1 pip install scrapli-netconf A Simple Example \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () response = conn . get_config ( source = \"running\" ) print ( response . result ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ python my_scrapli_script.py <rpc-reply message-id=\"101\"> <data> <ssh> <server> <v2/> <netconf>830</netconf> <netconf-vrf-table> <vrf> <vrf-name>default</vrf-name> <enable/> </vrf> </netconf-vrf-table> </server> </ssh> <interface-configurations> <interface-configuration> <active>act</active> <interface-name>MgmtEth0/RP0/CPU0/0</interface-name> <SNIP> </data> </rpc-reply> More Examples \u00b6 Basic Operations IOS-XR Basic Operations Junos Edit Config IOS-XR Asyncio Edit Config IOS-XR","title":"Quick Start Guide"},{"location":"user_guide/quickstart/#quick-start-guide","text":"","title":"Quick Start Guide"},{"location":"user_guide/quickstart/#installation","text":"In most cases installation via pip is the simplest and best way to install scrapli_netconf. See here for advanced installation details. 1 pip install scrapli-netconf","title":"Installation"},{"location":"user_guide/quickstart/#a-simple-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 from scrapli_netconf.driver import NetconfDriver my_device = { \"host\" : \"172.18.0.13\" , \"auth_username\" : \"vrnetlab\" , \"auth_password\" : \"VR-netlab9\" , \"auth_strict_key\" : False , \"port\" : 830 } conn = NetconfDriver ( ** my_device ) conn . open () response = conn . get_config ( source = \"running\" ) print ( response . result ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ python my_scrapli_script.py <rpc-reply message-id=\"101\"> <data> <ssh> <server> <v2/> <netconf>830</netconf> <netconf-vrf-table> <vrf> <vrf-name>default</vrf-name> <enable/> </vrf> </netconf-vrf-table> </server> </ssh> <interface-configurations> <interface-configuration> <active>act</active> <interface-name>MgmtEth0/RP0/CPU0/0</interface-name> <SNIP> </data> </rpc-reply>","title":"A Simple Example"},{"location":"user_guide/quickstart/#more-examples","text":"Basic Operations IOS-XR Basic Operations Junos Edit Config IOS-XR Asyncio Edit Config IOS-XR","title":"More Examples"},{"location":"user_guide/versioning/","text":"Versioning \u00b6 Just like scrapli, scrapli_netconf uses the CalVer versioning standard. All release versions follow the format YYYY.MM.DD , however PyPi will shorten/standardize this to remove leading zeros. The reason for choosing CalVer is simply to make it very clear how old a given release of scrapli is. While there are clearly some potential challenges around indicating when a \"breaking\" change occurs due to there not being the concept of a \"major\" version, this is hopefully not too big a deal for scrapli, and thus far the \"core\" API has been very stable -- there are only so many things you can/need to do over SSH after all! Please also note that the CHANGELOG contains notes about each version (and is updated in develop branch while updates are happening). A final note regarding versioning: scrapli updates are released as often as necessary/there are things to update . This means you should ALWAYS PIN YOUR REQUIREMENTS when using scrapli!! As stated, the \"core\" API has been very stable, but things will change over time -- always pin your requirements, and keep an eye on the changelog/api docs -- you can \"watch\" this repository to ensure you are notified of any releases.","title":"Versioning"},{"location":"user_guide/versioning/#versioning","text":"Just like scrapli, scrapli_netconf uses the CalVer versioning standard. All release versions follow the format YYYY.MM.DD , however PyPi will shorten/standardize this to remove leading zeros. The reason for choosing CalVer is simply to make it very clear how old a given release of scrapli is. While there are clearly some potential challenges around indicating when a \"breaking\" change occurs due to there not being the concept of a \"major\" version, this is hopefully not too big a deal for scrapli, and thus far the \"core\" API has been very stable -- there are only so many things you can/need to do over SSH after all! Please also note that the CHANGELOG contains notes about each version (and is updated in develop branch while updates are happening). A final note regarding versioning: scrapli updates are released as often as necessary/there are things to update . This means you should ALWAYS PIN YOUR REQUIREMENTS when using scrapli!! As stated, the \"core\" API has been very stable, but things will change over time -- always pin your requirements, and keep an eye on the changelog/api docs -- you can \"watch\" this repository to ensure you are notified of any releases.","title":"Versioning"}]}